{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "This notebook implements the method described in the research paper. The approach involves two main stages:\n",
    "\n",
    "Feature Engineering:\n",
    "\n",
    "Filtering features:\n",
    "Selecting the most informative events based on a relevance metric.\n",
    "Extracting sets of features: Mining recurring sets of events using an estimator of the Longest Common Subsequence (LCSS) algorithm.\n",
    "\n",
    "Classification:\n",
    "Building an ensemble classifier based on Naive Bayes, where each classifier corresponds to a time window before the incident."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>incident_id</th>\n",
       "      <th>vehicles_sequence</th>\n",
       "      <th>events_sequence</th>\n",
       "      <th>seconds_to_incident_sequence</th>\n",
       "      <th>approx_lat</th>\n",
       "      <th>approx_lon</th>\n",
       "      <th>train_kph_sequence</th>\n",
       "      <th>dj_ac_state_sequence</th>\n",
       "      <th>dj_dc_state_sequence</th>\n",
       "      <th>incident_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4432881</td>\n",
       "      <td>[609, 609, 609, 609, 609, 609, 609, 609, 609, ...</td>\n",
       "      <td>[2744, 4004, 2852, 4110, 2854, 4396, 1132, 414...</td>\n",
       "      <td>[-5510, -5510, -5507, -5507, -5506, -5506, -55...</td>\n",
       "      <td>50.876601</td>\n",
       "      <td>4.718143</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>4432943</td>\n",
       "      <td>[526, 526, 526, 526, 526, 526, 526, 526, 526, ...</td>\n",
       "      <td>[2744, 4148, 4394, 1566, 1570, 4396, 3634, 412...</td>\n",
       "      <td>[-8573, -8573, -8032, -8032, -8032, -7859, -61...</td>\n",
       "      <td>51.037435</td>\n",
       "      <td>4.431218</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.1,...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>4432955</td>\n",
       "      <td>[592, 592, 592, 592, 592, 592, 592, 592, 592, ...</td>\n",
       "      <td>[4394, 1566, 1570, 4114, 4168, 4168, 4156, 406...</td>\n",
       "      <td>[-12291, -12291, -12291, -10932, -10932, -1091...</td>\n",
       "      <td>50.864083</td>\n",
       "      <td>4.162115</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[True, True, True, True, True, False, True, Tr...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4433021</td>\n",
       "      <td>[576, 576, 576, 576, 576, 576, 576, 576, 576, ...</td>\n",
       "      <td>[4066, 4066, 4066, 4066, 4068, 2742, 4026, 270...</td>\n",
       "      <td>[-14351, -14204, -13890, -13383, -12739, -1243...</td>\n",
       "      <td>51.183220</td>\n",
       "      <td>4.276025</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4433129</td>\n",
       "      <td>[634, 634, 634, 634, 634, 634, 634, 634, 634, ...</td>\n",
       "      <td>[4002, 4032, 4028, 2852, 4026, 4110, 2742, 285...</td>\n",
       "      <td>[-224, -224, -223, -222, -222, -222, -220, -22...</td>\n",
       "      <td>50.818727</td>\n",
       "      <td>3.253601</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>[False, False, False, False, False, False, Fal...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  incident_id                                  vehicles_sequence  \\\n",
       "0           0      4432881  [609, 609, 609, 609, 609, 609, 609, 609, 609, ...   \n",
       "1           1      4432943  [526, 526, 526, 526, 526, 526, 526, 526, 526, ...   \n",
       "2           2      4432955  [592, 592, 592, 592, 592, 592, 592, 592, 592, ...   \n",
       "3           3      4433021  [576, 576, 576, 576, 576, 576, 576, 576, 576, ...   \n",
       "4           4      4433129  [634, 634, 634, 634, 634, 634, 634, 634, 634, ...   \n",
       "\n",
       "                                     events_sequence  \\\n",
       "0  [2744, 4004, 2852, 4110, 2854, 4396, 1132, 414...   \n",
       "1  [2744, 4148, 4394, 1566, 1570, 4396, 3634, 412...   \n",
       "2  [4394, 1566, 1570, 4114, 4168, 4168, 4156, 406...   \n",
       "3  [4066, 4066, 4066, 4066, 4068, 2742, 4026, 270...   \n",
       "4  [4002, 4032, 4028, 2852, 4026, 4110, 2742, 285...   \n",
       "\n",
       "                        seconds_to_incident_sequence  approx_lat  approx_lon  \\\n",
       "0  [-5510, -5510, -5507, -5507, -5506, -5506, -55...   50.876601    4.718143   \n",
       "1  [-8573, -8573, -8032, -8032, -8032, -7859, -61...   51.037435    4.431218   \n",
       "2  [-12291, -12291, -12291, -10932, -10932, -1091...   50.864083    4.162115   \n",
       "3  [-14351, -14204, -13890, -13383, -12739, -1243...   51.183220    4.276025   \n",
       "4  [-224, -224, -223, -222, -222, -222, -220, -22...   50.818727    3.253601   \n",
       "\n",
       "                                  train_kph_sequence  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.1,...   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "3  [0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       "\n",
       "                                dj_ac_state_sequence  \\\n",
       "0  [False, False, False, False, False, False, Fal...   \n",
       "1  [False, False, False, False, False, False, Fal...   \n",
       "2  [False, False, False, False, False, False, Fal...   \n",
       "3  [False, False, False, False, False, False, Fal...   \n",
       "4  [False, False, False, False, False, False, Fal...   \n",
       "\n",
       "                                dj_dc_state_sequence  incident_type  \n",
       "0  [False, False, False, False, False, False, Fal...              4  \n",
       "1  [True, True, True, True, True, True, True, Tru...             13  \n",
       "2  [True, True, True, True, True, False, True, Tr...             14  \n",
       "3  [True, True, True, True, True, True, True, Tru...              2  \n",
       "4  [False, False, False, False, False, False, Fal...             14  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert events_sequence from string to list\n",
    "df['events_sequence'] = df['events_sequence'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total occurrences of each event across all classes\n",
    "total_event_counts = Counter()\n",
    "# Occurrences of each event within each class\n",
    "event_counts_per_class = {}\n",
    "\n",
    "for incident_type in df['incident_type'].unique():\n",
    "    event_counts_per_class[incident_type] = Counter()\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    events = set(row['events_sequence'])  # Use set to avoid duplicate events in the same sequence\n",
    "    incident_type = row['incident_type']\n",
    "    total_event_counts.update(events)\n",
    "    event_counts_per_class[incident_type].update(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute r for each event\n",
    "relevance_metric = {}\n",
    "\n",
    "for event in total_event_counts:\n",
    "    max_h_in_class = max([event_counts_per_class[cls][event] for cls in event_counts_per_class])\n",
    "    h_in_all_classes = total_event_counts[event]\n",
    "    relevance_metric[event] = max_h_in_class / h_in_all_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of events selected with r >= 0.5: 635\n"
     ]
    }
   ],
   "source": [
    "# Set initial threshold\n",
    "tr = 0.5\n",
    "\n",
    "# Select events with r >= tr\n",
    "selected_events = [event for event, r in relevance_metric.items() if r >= tr]\n",
    "\n",
    "print(f\"Number of events selected with r >= {tr}: {len(selected_events)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Events not selected\n",
    "remaining_events = [event for event in total_event_counts if event not in selected_events]\n",
    "\n",
    "# Placeholder for events that pass the OaT procedure\n",
    "additional_events = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(events_subset):\n",
    "    # Prepare the data\n",
    "    df['filtered_events'] = df['events_sequence'].apply(lambda x: [e for e in x if e in events_subset])\n",
    "    \n",
    "    # Convert event lists to strings for vectorization\n",
    "    df['events_str'] = df['filtered_events'].apply(lambda x: ' '.join(map(str, x)))\n",
    "    \n",
    "    # Vectorize the events\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df['events_str'])\n",
    "    y = df['incident_type']\n",
    "    \n",
    "    # Perform stratified 4-fold cross-validation\n",
    "    skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    \n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        clf = MultinomialNB()\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    avg_f1_score = np.mean(f1_scores)\n",
    "    return avg_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial F1-score with selected events: 0.6309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OaT Procedure: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████| 282/282 [08:00<00:00,  1.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final F1-score after OaT: 0.6006\n",
      "Number of additional events added: 241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Current set of events\n",
    "current_events = selected_events.copy()\n",
    "\n",
    "# Evaluate classifier with current events\n",
    "current_f1_score = evaluate_classifier(current_events)\n",
    "print(f\"Initial F1-score with selected events: {current_f1_score:.4f}\")\n",
    "\n",
    "# Iterate over remaining events\n",
    "for event in tqdm(remaining_events, desc='OaT Procedure'):\n",
    "    temp_events = current_events + [event]\n",
    "    f1 = evaluate_classifier(temp_events)\n",
    "    if f1 >= 0.6:\n",
    "        current_events.append(event)\n",
    "        additional_events.append(event)\n",
    "        current_f1_score = f1  # Update the current F1-score\n",
    "\n",
    "print(f\"Final F1-score after OaT: {current_f1_score:.4f}\")\n",
    "print(f\"Number of additional events added: {len(additional_events)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count event frequencies\n",
    "event_counts = Counter([event for seq in df['events_sequence'] for event in seq])\n",
    "\n",
    "# Define a minimum frequency threshold\n",
    "min_event_frequency = 10  # Adjust based on dataset\n",
    "\n",
    "# Filter out rare events\n",
    "frequent_events = set([event for event, count in event_counts.items() if count >= min_event_frequency])\n",
    "\n",
    "# Update events_sequence by keeping only frequent events\n",
    "df['filtered_events_sequence'] = df['events_sequence'].apply(lambda seq: [event for event in seq if event in frequent_events])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for frequent itemset mining\n",
    "transactions = df['filtered_events_sequence'].tolist()\n",
    "\n",
    "# Remove empty transactions\n",
    "transactions = [t for t in transactions if t]\n",
    "\n",
    "# Use TransactionEncoder\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(transactions).transform(transactions)\n",
    "df_te = pd.DataFrame(te_ary, columns=te.columns_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "# Apply the FP-Growth algorithm to find frequent itemsets\n",
    "min_support = 0.05  # Adjust as needed based on dataset size\n",
    "\n",
    "frequent_itemsets = fpgrowth(df_te, min_support=min_support, use_colnames=True)\n",
    "\n",
    "print(f\"Number of frequent itemsets found: {len(frequent_itemsets)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert frequent itemsets into a list of sets\n",
    "frequent_sets = frequent_itemsets['itemsets'].tolist()\n",
    "\n",
    "# Function to check if a frequent set is present in a transaction\n",
    "def check_itemset_presence(transaction, itemset):\n",
    "    return itemset.issubset(set(map(str, transaction)))\n",
    "\n",
    "# Create new features for each frequent itemset\n",
    "for idx, itemset in enumerate(frequent_sets):\n",
    "    feature_name = f'itemset_{idx}'\n",
    "    df[feature_name] = df['events_sequence'].apply(lambda x: int(check_itemset_presence(x, itemset)))\n",
    "\n",
    "print(\"New features added based on frequent itemsets.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define time windows in minutes (for example purposes)\n",
    "time_windows = [5, 10, 15, 20, 30, 60]  # Adjust based on data granularity\n",
    "\n",
    "# Assume each event in events_sequence has an associated timestamp\n",
    "# For this example, we'll simulate event times (in minutes before the incident)\n",
    "import random\n",
    "\n",
    "def assign_event_times(events_sequence):\n",
    "    return [(e, random.randint(0, 120)) for e in events_sequence]  # Events within 2 hours before incident\n",
    "\n",
    "df['events_with_times'] = df['events_sequence'].apply(assign_event_times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a list to hold classifiers and their corresponding data\n",
    "classifiers = []\n",
    "window_features = []\n",
    "\n",
    "for window in time_windows:\n",
    "    # Extract events within the window\n",
    "    def filter_events_by_time(events_with_times):\n",
    "        return [str(e[0]) for e in events_with_times if e[1] <= window]\n",
    "    \n",
    "    df[f'events_window_{window}'] = df['events_with_times'].apply(filter_events_by_time)\n",
    "    \n",
    "    # Convert to string for vectorization\n",
    "    df[f'events_window_{window}_str'] = df[f'events_window_{window}'].apply(lambda x: ' '.join(x))\n",
    "    \n",
    "    # Vectorize the events\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(df[f'events_window_{window}_str'])\n",
    "    y = df['incident_type']\n",
    "    \n",
    "    # Store the vectorizer and data\n",
    "    window_features.append({'window': window, 'vectorizer': vectorizer, 'X': X, 'y': y})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifiers for each window\n",
    "for wf in window_features:\n",
    "    X = wf['X']\n",
    "    y = wf['y']\n",
    "    clf = MultinomialNB(alpha=1e-6)  # Smoothing parameter λ\n",
    "    clf.fit(X, y)\n",
    "    wf['classifier'] = clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Prepare the final predictions\n",
    "y_true = df['incident_type']\n",
    "y_pred = []\n",
    "\n",
    "# Cross-validation setup\n",
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "f1_scores = []\n",
    "\n",
    "for train_index, test_index in skf.split(df, y_true):\n",
    "    y_test_fold = y_true.iloc[test_index]\n",
    "    y_pred_fold = []\n",
    "    \n",
    "    # For each sample in the test fold\n",
    "    for idx in test_index:\n",
    "        prediction_made = False\n",
    "        # Go through classifiers in order of time windows\n",
    "        for wf in window_features:\n",
    "            # Get the features for this sample\n",
    "            events_str = df.iloc[idx][f'events_window_{wf[\"window\"]}_str']\n",
    "            X_sample = wf['vectorizer'].transform([events_str])\n",
    "            # Predict\n",
    "            clf = wf['classifier']\n",
    "            y_pred_sample = clf.predict(X_sample)\n",
    "            # Check if the classifier can make a prediction\n",
    "            if X_sample.nnz != 0:\n",
    "                y_pred_fold.append(y_pred_sample[0])\n",
    "                prediction_made = True\n",
    "                break  # Exit the loop once a prediction is made\n",
    "        if not prediction_made:\n",
    "            # If no classifier could make a prediction, use the most frequent class as a fallback\n",
    "            y_pred_fold.append(y_true.mode()[0])\n",
    "    \n",
    "    # Compute F1-score for this fold\n",
    "    f1 = f1_score(y_test_fold, y_pred_fold, average='weighted', zero_division=0)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "print(f\"Average F1-score of the ensemble classifier: {np.mean(f1_scores):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
