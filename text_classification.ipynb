{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stef4k/train-maintenance-data-mining/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee",
      "metadata": {
        "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee"
      },
      "source": [
        "# Text classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9",
      "metadata": {
        "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58",
      "metadata": {
        "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58"
      },
      "source": [
        "Manually remove the first ';' from the first row in csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
      "metadata": {
        "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
        "outputId": "fc8ae1ff-5768-4d5c-c54a-adfa1ca3e48a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     incident_id                                  vehicles_sequence  \\\n",
              "634      4462103  [516, 516, 516, 516, 516, 516, 516, 516, 516, ...   \n",
              "723      4466677  [568, 568, 568, 568, 568, 568, 568, 568, 568, ...   \n",
              "\n",
              "                                       events_sequence  \\\n",
              "634  [2956, 2956, 2956, 2956, 2956, 2956, 2956, 295...   \n",
              "723  [4002, 4032, 4026, 4028, 2852, 4110, 2854, 249...   \n",
              "\n",
              "                          seconds_to_incident_sequence  approx_lat  \\\n",
              "634  [-14396, -14371, -14358, -14342, -14331, -1430...   50.634509   \n",
              "723  [-11543, -11543, -11541, -11541, -11540, -1154...   50.949979   \n",
              "\n",
              "     approx_lon                                 train_kph_sequence  \\\n",
              "634    4.621612  [17.3, 73.0, 84.8, 91.5, 89.1, 86.2, 86.3, 88....   \n",
              "723    5.072721  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "\n",
              "                                  dj_ac_state_sequence  \\\n",
              "634  [False, False, False, False, False, False, Fal...   \n",
              "723  [False, False, False, False, False, False, Fal...   \n",
              "\n",
              "                                  dj_dc_state_sequence  incident_type  \n",
              "634  [True, True, True, True, True, True, True, Tru...             99  \n",
              "723  [False, False, False, False, False, False, Fal...              9  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-9e56b1d3-dab2-40ed-b78d-8bfae453c9ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incident_id</th>\n",
              "      <th>vehicles_sequence</th>\n",
              "      <th>events_sequence</th>\n",
              "      <th>seconds_to_incident_sequence</th>\n",
              "      <th>approx_lat</th>\n",
              "      <th>approx_lon</th>\n",
              "      <th>train_kph_sequence</th>\n",
              "      <th>dj_ac_state_sequence</th>\n",
              "      <th>dj_dc_state_sequence</th>\n",
              "      <th>incident_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>634</th>\n",
              "      <td>4462103</td>\n",
              "      <td>[516, 516, 516, 516, 516, 516, 516, 516, 516, ...</td>\n",
              "      <td>[2956, 2956, 2956, 2956, 2956, 2956, 2956, 295...</td>\n",
              "      <td>[-14396, -14371, -14358, -14342, -14331, -1430...</td>\n",
              "      <td>50.634509</td>\n",
              "      <td>4.621612</td>\n",
              "      <td>[17.3, 73.0, 84.8, 91.5, 89.1, 86.2, 86.3, 88....</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>723</th>\n",
              "      <td>4466677</td>\n",
              "      <td>[568, 568, 568, 568, 568, 568, 568, 568, 568, ...</td>\n",
              "      <td>[4002, 4032, 4026, 4028, 2852, 4110, 2854, 249...</td>\n",
              "      <td>[-11543, -11543, -11541, -11541, -11540, -1154...</td>\n",
              "      <td>50.949979</td>\n",
              "      <td>5.072721</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9e56b1d3-dab2-40ed-b78d-8bfae453c9ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-9e56b1d3-dab2-40ed-b78d-8bfae453c9ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-9e56b1d3-dab2-40ed-b78d-8bfae453c9ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7ccaf9ad-a60a-4462-87bf-aa40f90f9fa1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7ccaf9ad-a60a-4462-87bf-aa40f90f9fa1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7ccaf9ad-a60a-4462-87bf-aa40f90f9fa1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"incident_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3234,\n        \"min\": 4462103,\n        \"max\": 4466677,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4466677,\n          4462103\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vehicles_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 568, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613, 613]\",\n          \"[516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516, 516]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"events_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[4002, 4032, 4026, 4028, 2852, 4110, 2854, 2492, 2494, 4102, 4106, 2498, 3986, 4004, 4032, 4028, 2852, 4026, 4092, 4094, 4110, 2854, 3260, 2492, 2494, 4102, 4106, 2498, 3986, 4002, 4032, 4028, 2852, 4026, 4110, 2854, 2744, 4026, 4148, 2708, 4140, 4140, 4152, 2554, 4168, 4156, 4406, 4410, 2740, 4408, 4412, 4030, 4020, 4026, 3254, 3254, 3254, 4124, 4394, 152, 2858, 2658, 2688, 3256, 4016, 4026, 4020, 2744, 4026, 4148, 2708, 4020, 2970, 4082, 4092, 4090, 4084, 4090, 4094, 4090, 3236, 2974, 4100, 2852, 2854, 4124, 3634, 2682, 1446, 4396, 4124, 2956, 2682, 2956, 2956, 2956, 2956, 4066, 2708, 4026, 4016, 4026, 4020, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 1578, 4068, 4394, 1738, 3658, 4068, 3658, 4068, 4068, 3720, 3722, 3734, 4396, 3734, 3658, 4068, 3658, 4068, 3658, 4066, 2744, 4148, 2708, 4026, 4020, 4124, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2682, 2684, 4124, 2956, 4002, 4032, 4028, 2852, 4026, 4110, 2854, 2492, 2494, 4102, 4106, 2498, 3986, 4004, 4032, 4028, 2852, 4026, 4110, 2854, 4092, 4094, 3260, 2492, 2494, 4102, 4106, 2498, 3986, 4002, 4032, 4028, 2852, 4026, 4110, 2854, 4026, 4016, 4140, 4140, 4152, 4168, 4156, 4406, 4410, 4408, 4412, 4020, 4394, 152, 3256, 2744, 4026, 2708, 4020, 4026, 3254, 3254, 3254, 4124, 2858, 2658, 2688, 4026, 4016, 4020, 4082, 4092, 4094, 4068, 2744, 4026, 4026, 2708, 4020, 4396, 2852, 2854, 4124, 3636, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2682, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2682, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 4394, 3636, 4066, 3636, 4066, 3636, 3658, 4124, 4066, 4124, 4396, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 4068, 2708, 4026, 4016, 4020, 4068, 4066]\",\n          \"[2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2602, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 4120, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2682, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 2744, 4026, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3980, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3980, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3224, 4396, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2892, 2892, 4068, 3636, 3658, 4124, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seconds_to_incident_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[-11543, -11543, -11541, -11541, -11540, -11540, -11539, -11245, -10945, -10929, -10929, -10645, -10635, -10595, -10595, -10594, -10593, -10593, -10593, -10593, -10593, -10591, -10487, -10298, -9998, -9982, -9982, -9698, -9687, -6102, -6102, -6101, -6100, -6100, -6100, -6098, -6084, -6084, -6084, -6083, -6079, -6042, -6041, -6033, -6033, -6030, -6015, -6014, -6011, -6006, -6004, -5987, -5986, -5986, -5953, -5952, -5924, -5900, -5880, -5880, -5836, -5832, -5832, -5800, -5623, -5623, -5622, -5239, -5239, -5239, -5238, -5237, -5194, -5141, -5141, -5140, -5110, -5110, -5110, -5081, -5008, -4864, -4832, -4821, -4819, -4815, -4813, -4803, -4712, -4711, -4474, -4444, -4438, -4404, -4401, -4387, -4324, -4286, -4279, -4097, -4096, -4095, -4094, -1947, -1368, -1342, -920, -809, -562, -506, -256, -234, -127, 28, 36, 36, 48, 144, 165, 226, 364, 507, 518, 528, 531, 554, 586, 877, 911, 1203, 1252, 1723, 1963, 1963, 1964, 1964, 1965, 2657, 2672, 2672, 2731, 2740, 2792, 2811, 2817, 2832, 2863, 2894, 2899, 2962, 3008, 3017, 3020, 3027, 3038, 3048, 3053, 3056, 3059, 3062, 3063, 3102, 3103, 3139, 3148, 3150, 3213, 3235, 3235, 3252, 3287, 3289, 3342, 3343, 3387, 3392, 3430, 3432, 3443, 3481, 3499, 3503, 3531, 3548, 3548, 3574, 3574, 3574, 3590, -11547, -11547, -11546, -11545, -11545, -11545, -11543, -11250, -10950, -10934, -10934, -10650, -10639, -10598, -10598, -10597, -10596, -10596, -10596, -10594, -10593, -10593, -10495, -10301, -10001, -9985, -9985, -9701, -9690, -6106, -6106, -6105, -6104, -6104, -6104, -6102, -6088, -6087, -6083, -6046, -6041, -6037, -6034, -6022, -6020, -6013, -6012, -5990, -5887, -5887, -5805, -5629, -5629, -5627, -5627, -5627, -5601, -5600, -5578, -5493, -5430, -5428, -5428, -5243, -5242, -5241, -5086, -5086, -5077, -4290, -4102, -4102, -4100, -4099, -4099, -4093, -4056, -4054, -2116, -1951, -1928, -1810, -1798, -1791, -1781, -1755, -1742, -1728, -1718, -1670, -1668, -1653, -1622, -1616, -1615, -1514, -1512, -1478, -1468, -1438, -1433, -1424, -1417, -1372, -1346, -1323, -1280, -1271, -1266, -1225, -1224, -1061, -1013, -1004, -1001, -975, -973, -960, -954, -925, -813, -788, -765, -753, -748, -740, -717, -707, -696, -688, -681, -679, -675, -633, -623, -567, -510, -397, -351, -336, -314, -289, -260, -238, -196, -182, -168, -152, -143, -119, -117, -80, -43, -32, 24, 34, 44, 140, 160, 222, 315, 315, 320, 360, 487, 528, 582, 664, 683, 713, 729, 735, 750, 765, 773, 788, 801, 804, 815, 818, 848, 851, 873, 907, 952, 954, 1006, 1018, 1019, 1057, 1061, 1105, 1107, 1148, 1159, 1161, 1199, 1248, 1350, 1352, 1398, 1400, 1439, 1440, 1443, 1446, 1449, 1454, 1464, 1481, 1491, 1493, 1525, 1538, 1540, 1554, 1580, 1595, 1601, 1626, 1633, 1677, 1683, 1686, 1719, 1724, 1959, 1960, 1961, 3208, 3526]\",\n          \"[-14396, -14371, -14358, -14342, -14331, -14300, -14298, -14287, -14255, -14254, -14225, -14195, -14195, -14160, -14107, -14079, -14079, -14040, -14034, -13979, -13961, -13957, -13956, -13933, -13890, -13890, -13797, -13762, -13711, -13680, -13671, -13639, -13592, -13555, -13549, -13524, -13524, -13472, -13441, -13389, -13356, -13326, -13281, -13258, -13229, -13229, -13165, -13117, -13094, -13062, -13062, -12997, -12985, -12981, -12959, -12947, -12927, -12921, -12905, -12896, -12851, -12839, -12788, -12777, -12666, -12636, -12621, -12575, -12523, -12439, -12402, -12384, -12382, -12366, -12345, -12345, -12290, -12287, -12283, -12279, -12248, -12228, -12194, -12183, -12166, -12154, -12128, -12123, -12113, -12111, -12102, -12092, -12090, -12027, -12025, -12010, -12008, -12003, -11958, -11938, -11911, -11881, -11879, -11875, -11867, -11859, -11823, -11820, -11817, -11801, -11797, -11767, -11753, -11748, -11732, -11705, -11699, -11686, -11676, -11656, -11571, -11569, -11569, -11535, -11523, -11487, -11485, -11472, -11471, -11453, -11452, -11445, -11442, -11441, -11416, -11413, -11399, -11398, -11379, -11373, -11360, -11355, -11350, -11347, -11344, -11336, -11330, -11321, -11305, -11283, -11274, -11262, -11253, -11229, -11226, -11186, -11178, -11169, -11164, -11148, -11139, -11126, -11125, -11123, -11113, -11100, -11077, -11077, -11036, -11025, -11017, -11005, -10979, -10977, -10970, -10967, -10958, -10921, -10917, -10907, -10903, -10872, -10863, -10853, -10839, -10829, -10805, -10800, -10789, -10770, -10740, -10740, -10725, -10690, -10667, -10637, -10608, -10574, -10574, -10491, -10450, -10386, -10360, -10358, -10340, -10311, -10283, -10273, -10202, -10202, -10146, -10104, -10090, -10089, -10066, -10053, -10046, -10035, -10011, -9991, -9989, -9946, -9939, -9901, -9892, -9890, -9851, -9843, -9840, -9795, -9785, -9758, -9755, -9749, -9746, -9736, -9699, -9690, -9661, -9650, -9624, -9615, -9614, -9591, -9573, -9563, -9533, -9532, -9526, -9515, -9496, -9493, -9442, -9423, -9411, -9374, -9301, -9290, -9264, -9248, -9124, -9124, -9073, -9039, -9013, -8982, -8962, -8919, -8917, -8908, -8904, -8876, -8874, -8862, -8856, -8823, -8804, -8766, -8745, -8743, -8725, -8699, -8681, -8643, -8631, -8628, -8614, -8575, -8566, -8554, -8530, -8521, -8500, -8482, -8473, -8450, -8436, -8425, -8414, -8379, -8370, -8345, -8319, -8319, -8281, -8259, -8259, -8229, -8209, -8179, -8164, -8150, -8149, -8135, -8087, -8078, -8040, -8031, -8029, -7990, -7981, -7976, -7921, -7919, -7876, -7872, -7863, -7843, -7839, -7834, -7830, -7802, -7792, -7766, -7764, -7725, -7714, -7712, -7659, -7658, -7642, -7627, -7568, -7559, -7554, -7519, -7510, -7414, -7414, -7342, -7317, -7292, -7228, -7226, -7215, -7209, -7199, -7180, -7175, -7172, -7159, -7139, -7102, -7102, -7044, -7022, -7014, -7009, -6993, -6982, -6967, -6945, -6929, -6922, -6880, -6842, -6822, -6807, -6784, -6759, -6737, -6734, -6732, -6722, -6696, -6683, -6680, -6624, -6589, -6536, -6496, -6476, -6461, -6461, -6408, -6357, -6325, -6310, -6257, -6230, -6230, -6174, -6162, -6147, -6138, -6117, -6102, -6078, -6043, -6029, -6014, -5998, -5977, -5968, -5946, -5922, -5865, -5865, -5758, -5735, -5701, -5675, -5669, -5650, -5632, -5612, -5592, -5586, -5582, -5544, -5541, -5502, -5500, -5486, -5461, -5419, -5388, -5344, -5334, -5317, -5306, -5267, -5257, -5248, -4661, -4661, -2246, -2246, -2217, -2089, -2074, -2055, -2048, -2022, -2002, -1982, -1982, -1968, -1952, -1925, -1919, -1906, -1878, -1839, -1810, -1773, -1772, -1760, -1743, -1717, -1691, -1656, -1614, -1614, -1570, -1553, -1547, -1531, -1517, -1502, -1489, -1460, -1455, -1426, -1404, -1380, -1371, -1357, -1347, -1321, -1285, -1285, -1199, -1184, -1152, -1102, -1074, -1048, -1048, -1002, -971, -957, -955, -943, -910, -901, -868, -858, -850, -813, -803, -765, -751, -745, -707, -697, -660, -658, -639, -603, -574, -560, -538, -526, -497, -464, -464, -404, -400, -392, -367, -356, -350, -339, -338, -274, -249, -221, -220, -210, -202, -179, -33, -33, 479, 515, 549, 566, 597, 613, 633, 636, 701, 703, 753, 756, 796, 806, 817, 827, 830, 839, 856, 865, 869, 907, 917, 919, 968, 978, 983, 1025, 1035, 1037, 1089, 1099, 1143, 1152, 1162, 1167, 1194, 1223, 1281, 1281, 1327, 1331, 1348, 1376, 1388, 1399, 1403, 1413, 1436, 1455, 1465, 1488, 1515, 1524, 1536, 1579, 1591, 1593, 1638, 1658, 1680, 1699, 1703, 1718, 1723, 1752, 1769, 1795, 1801, 1812, 1814, 1839, 1842, 1848, 1850, 1882, 1898, 1927, 1964, 1982, 2000, 2011, 2039, 2039, 2101, 2111, 2156, 2214, 2227, 2244, 2279, 2281, 2297, 2304, 2308, 2315, 2348, 2367, 2377, 2401, 2403, 2430, 2439, 2451, 2480, 2490, 2492, 2534, 2550, 2553, 2563, 2591, 2597, 2601, 2607, 2638, 2646, 2648, 2696, 2698, 2747, 2756, 2794, 2803, 2805, 2816, 2825, 2853, 2871, 2883, 2905, 2917, 2944, 2944, 2944, 2977, 2987, 3019, 3019, 3102, 3124, 3156, 3180, 3206, 3253, 3256, 3258, 3285, 3314, 3314, 3329, 3377, 3434, 3435, 3467, 3494, 3494, 3546, 3559, 3564, 3588]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.223070883308973,\n        \"min\": 50.63450944312057,\n        \"max\": 50.94997931166667,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          50.94997931166667,\n          50.63450944312057\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3189822731991897,\n        \"min\": 4.62161190141844,\n        \"max\": 5.072720958333334,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          5.072720958333334,\n          4.62161190141844\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_kph_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.8, 24.8, 34.1, 34.6, 34.2, 38.1, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 3.9, 0.0, 0.9, 0.0, 136.3, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 20.0, 28.6, 37.3, 69.6, 69.5, 68.7, 43.5, 32.9, 33.4, 22.6, 109.6, 111.8, 112.7, 115.1, 117.0, 115.7, 114.9, 115.1, 115.2, 115.8, 115.5, 118.0, 117.9, 115.0, 114.0, 113.3, 0.0, 0.0, 0.0, 0.0, 80.2, 85.4, 118.4, 117.8, 112.2, 111.5, 92.6, 91.3, 81.6, 64.5, 40.5, 35.7, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 29.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 34.4, 36.6, 36.5, 38.3, 37.3, 36.8, 43.8, 52.4, 65.8, 66.5, 73.1, 105.1, 110.5, 111.4, 120.4, 120.0, 114.1, 113.5, 82.1, 77.1, 64.3, 62.2, 0.0, 0.0, 0.0, 72.7, 79.0, 82.5, 104.2, 105.0, 111.2, 95.6, 86.2, 83.4, 70.1, 67.2, 54.8, 47.7, 0.0, 0.0, 0.0, 53.0, 68.1, 72.4, 89.2, 116.3, 114.6, 113.3, 112.1, 112.5, 113.1, 114.0, 107.5, 97.0, 3.0, 0.0, 79.5, 76.9, 57.5, 39.7, 41.6, 0.3, 0.0, 41.1, 67.0, 85.5, 108.6, 122.0, 138.0, 137.9, 127.5, 104.9, 94.7, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 29.9, 49.7, 66.0, 65.1, 66.4, 68.9, 70.6, 71.8, 72.4, 73.1, 73.5, 75.6, 76.4, 43.8, 41.3, 0.1, 0.0, 40.5, 44.5, 79.9, 87.6, 88.4, 118.9, 118.2, 113.9, 113.8, 100.8, 90.6, 87.0, 0.1, 0.0, 110.1, 110.9, 114.9, 115.0, 117.6, 117.4, 117.5, 117.7, 117.9, 118.0, 118.3, 115.6, 113.3, 112.5, 88.0, 78.2, 77.4, 74.4, 71.8, 58.4, 50.8, 37.0, 35.8, 34.3, 34.0, 33.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2]\",\n          \"[17.3, 73.0, 84.8, 91.5, 89.1, 86.2, 86.3, 88.0, 57.0, 55.0, 2.2, 0.0, 0.0, 19.7, 0.7, 0.0, 0.0, 32.1, 45.6, 71.5, 49.3, 44.7, 45.7, 0.8, 0.0, 0.0, 26.5, 31.4, 31.2, 22.8, 24.2, 27.3, 38.1, 17.2, 2.8, 0.0, 0.0, 30.2, 34.6, 37.8, 35.2, 34.9, 33.8, 4.0, 0.0, 0.0, 30.0, 37.1, 3.1, 0.0, 0.0, 39.9, 47.8, 47.5, 48.3, 45.8, 47.7, 47.7, 41.0, 32.6, 27.7, 16.5, 21.9, 24.3, 23.3, 22.7, 19.1, 9.7, 18.2, 33.3, 26.2, 27.7, 27.6, 3.4, 0.0, 0.0, 29.0, 29.7, 30.4, 30.9, 35.5, 38.9, 40.5, 37.8, 36.7, 37.6, 34.0, 32.8, 34.5, 35.6, 42.2, 46.8, 45.7, 70.3, 69.8, 68.0, 67.6, 67.1, 35.0, 36.1, 46.0, 66.4, 66.0, 65.6, 64.8, 59.6, 52.9, 52.5, 51.9, 56.9, 57.5, 55.3, 56.1, 52.6, 41.0, 38.5, 38.2, 37.3, 30.3, 3.5, 0.0, 0.0, 0.0, 13.7, 28.9, 51.3, 53.7, 75.6, 76.7, 85.5, 85.8, 86.0, 85.6, 85.7, 79.4, 78.1, 76.7, 76.4, 80.8, 78.5, 80.8, 76.4, 72.2, 71.0, 68.0, 65.7, 66.8, 69.6, 58.9, 55.0, 52.3, 47.6, 51.8, 57.7, 59.1, 55.2, 60.5, 77.4, 84.6, 84.8, 82.7, 73.8, 70.6, 68.0, 48.1, 3.4, 0.0, 0.0, 23.7, 53.9, 73.4, 83.1, 82.5, 83.0, 87.5, 85.4, 82.7, 80.5, 78.9, 69.9, 67.6, 55.4, 56.6, 57.5, 56.3, 51.8, 40.1, 40.3, 39.5, 0.9, 0.0, 0.0, 0.0, 33.0, 38.3, 32.6, 2.3, 0.0, 0.0, 0.0, 14.5, 30.2, 51.2, 52.6, 63.7, 63.8, 41.6, 0.2, 0.0, 0.0, 42.3, 61.9, 66.0, 65.6, 73.2, 87.2, 92.3, 97.9, 106.6, 118.5, 119.9, 121.0, 117.7, 115.3, 117.5, 118.3, 119.2, 116.9, 116.0, 111.8, 113.7, 116.3, 116.0, 115.6, 115.2, 115.6, 118.9, 122.3, 122.2, 124.9, 113.2, 109.7, 109.6, 106.2, 102.8, 100.7, 105.9, 105.2, 99.4, 83.2, 49.8, 45.4, 35.6, 34.2, 34.2, 32.6, 29.2, 28.9, 17.2, 0.0, 0.0, 0.0, 12.6, 35.8, 39.1, 32.8, 43.9, 73.4, 76.4, 84.6, 87.5, 98.4, 97.1, 89.5, 83.6, 56.2, 52.7, 44.6, 51.7, 52.9, 54.7, 59.8, 59.0, 86.6, 92.7, 93.2, 95.6, 113.4, 115.6, 115.3, 117.6, 117.0, 117.5, 122.5, 121.2, 109.1, 101.5, 97.6, 94.5, 55.6, 42.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.3, 32.0, 68.5, 75.2, 77.8, 78.3, 81.1, 117.6, 124.3, 113.3, 112.1, 112.0, 113.0, 112.8, 111.5, 117.0, 118.2, 123.9, 125.0, 125.1, 114.5, 113.8, 112.5, 111.7, 103.5, 105.8, 108.0, 107.6, 92.7, 93.1, 92.3, 65.5, 64.1, 62.9, 58.1, 26.4, 25.2, 22.9, 12.2, 0.0, 0.0, 0.0, 22.5, 38.8, 36.6, 53.3, 54.3, 63.4, 66.9, 74.9, 75.7, 75.8, 74.9, 43.4, 0.0, 0.0, 0.0, 50.9, 76.1, 82.6, 85.0, 86.8, 88.4, 88.8, 85.7, 85.4, 85.1, 88.1, 85.3, 87.9, 86.5, 88.1, 84.8, 85.4, 85.8, 86.3, 87.5, 85.8, 79.6, 75.8, 26.9, 24.8, 18.2, 35.1, 0.2, 0.0, 0.0, 46.6, 84.5, 80.9, 79.9, 0.1, 0.0, 0.0, 42.6, 59.8, 75.8, 79.1, 63.5, 59.3, 54.3, 66.0, 75.0, 75.7, 61.3, 38.5, 35.5, 33.5, 0.0, 0.0, 0.0, 33.5, 31.3, 29.9, 42.0, 47.4, 60.8, 73.5, 85.4, 87.9, 87.8, 87.3, 84.3, 83.5, 82.0, 82.1, 81.2, 77.0, 42.8, 30.8, 25.4, 25.0, 24.9, 25.5, 10.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 16.8, 16.9, 30.9, 35.0, 38.0, 46.8, 63.1, 63.4, 73.0, 80.8, 88.2, 88.3, 88.7, 86.8, 81.0, 78.4, 86.2, 86.3, 77.7, 53.7, 38.1, 33.2, 0.2, 0.0, 0.0, 22.3, 47.1, 56.0, 72.5, 78.9, 78.5, 75.7, 46.8, 46.4, 44.1, 43.8, 77.3, 77.3, 70.9, 57.1, 0.2, 0.0, 0.0, 75.7, 82.2, 79.3, 51.4, 0.0, 0.0, 0.0, 23.9, 58.4, 70.6, 70.6, 73.3, 78.2, 78.9, 82.8, 83.7, 84.5, 87.5, 88.0, 84.6, 85.0, 85.9, 86.8, 86.1, 85.8, 85.9, 84.8, 82.8, 79.7, 70.9, 51.7, 41.0, 0.0, 0.0, 0.0, 42.9, 45.6, 51.0, 64.0, 64.8, 62.8, 57.7, 56.9, 32.2, 31.0, 24.3, 24.1, 23.0, 22.5, 0.0, 0.0, 0.0, 0.0, 12.2, 41.3, 46.9, 45.0, 50.5, 55.5, 56.3, 85.7, 87.6, 98.2, 98.2, 99.6, 105.6, 113.5, 117.2, 118.3, 121.6, 119.9, 118.2, 117.5, 107.5, 100.5, 98.9, 103.5, 105.8, 108.0, 100.9, 101.6, 102.2, 109.7, 104.5, 113.5, 116.9, 117.2, 106.3, 50.6, 0.1, 0.0, 0.0, 31.4, 39.1, 64.8, 86.0, 93.0, 101.6, 105.6, 113.6, 117.3, 110.4, 104.8, 104.0, 110.3, 112.1, 116.9, 116.3, 112.5, 111.7, 55.8, 53.2, 57.5, 53.3, 53.1, 53.1, 53.9, 59.4, 68.6, 94.2, 96.7, 100.7, 101.5, 109.8, 109.9, 105.3, 99.3, 58.0, 34.6, 37.0, 29.0, 27.3, 11.9, 0.0, 0.0, 0.0, 32.6, 32.8, 35.8, 30.7, 33.2, 36.8, 75.4, 76.9, 88.2, 92.3, 93.2, 95.0, 98.0, 102.4, 104.3, 104.8, 105.1, 114.6, 117.2, 113.6, 108.2, 103.1, 102.2, 94.4, 107.6, 110.1, 115.5, 106.0, 106.7, 107.7, 108.7, 120.5, 125.3, 124.9, 113.4, 112.5, 108.9, 109.0, 113.7, 112.9, 112.5, 109.5, 107.2, 82.9, 85.5, 84.6, 84.0, 86.9, 43.4, 44.4, 45.3, 16.3, 0.0, 0.0, 0.0, 33.6, 60.5, 57.0, 42.2, 36.5, 29.4, 26.2, 24.1, 0.1, 0.0, 0.0, 0.0, 21.9, 27.1, 27.0, 0.0, 0.0, 0.0, 31.5, 46.5, 45.8, 44.9]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_ac_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_dc_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\",\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63,\n        \"min\": 9,\n        \"max\": 99,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9,\n          99\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';', index_col=0)\n",
        "df.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16737a51-b8eb-4428-b351-51113364e62c",
      "metadata": {
        "id": "16737a51-b8eb-4428-b351-51113364e62c"
      },
      "source": [
        "Now I will analyze the percentage of each event type appearing at least once in an event sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d70f1d49-bf17-4c26-96c9-cea06d26c642",
      "metadata": {
        "id": "d70f1d49-bf17-4c26-96c9-cea06d26c642"
      },
      "outputs": [],
      "source": [
        "events_types_dict = {}\n",
        "for events_sequence in df['events_sequence']:\n",
        "    row_list = ast.literal_eval(events_sequence) #transforming string into actual list\n",
        "    unique_events = set(row_list)\n",
        "    for event in unique_events:\n",
        "        if not events_types_dict.get(event):\n",
        "            events_types_dict[event] = 0\n",
        "        events_types_dict[event] += 1\n",
        "sorted_dict = dict(sorted(events_types_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "# Convert the sorted dictionary to a DataFrame\n",
        "sorted_events_perc_df = pd.DataFrame(list(sorted_dict.items()), columns=['event_type', 'frequency'])\n",
        "sorted_events_perc_df['percentage'] = sorted_events_perc_df['frequency'] / df.shape[0] * 100\n",
        "# Cast the 'event_type' column to string\n",
        "sorted_events_perc_df['event_type'] = sorted_events_perc_df['event_type'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18cbe400-a4a2-4625-92ad-3c527827e5e7",
      "metadata": {
        "id": "18cbe400-a4a2-4625-92ad-3c527827e5e7"
      },
      "source": [
        "We save in a list all event codes that appear in less than 85% of the event sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ba05e687-439c-4f60-a78e-681a6cd75090",
      "metadata": {
        "id": "ba05e687-439c-4f60-a78e-681a6cd75090"
      },
      "outputs": [],
      "source": [
        "events_low_frequency = list(map(int, list(sorted_events_perc_df[sorted_events_perc_df.percentage<=85].event_type)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea15edbc-88df-41fa-b9d1-7099d50a6617",
      "metadata": {
        "id": "ea15edbc-88df-41fa-b9d1-7099d50a6617"
      },
      "source": [
        "## Text preprocessing\n",
        "Before we start with text classification we need to clean the sequences of events. As seen one value of `events_sequence` contains commas and brackets even though it is a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "cb37149f-424d-4374-b560-e330b3942c6e",
      "metadata": {
        "id": "cb37149f-424d-4374-b560-e330b3942c6e",
        "outputId": "985e9a53-25e7-4c00-830c-1e4575659af8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[2744, 4004, 2852, 4110, 2854, 4396, 1132, 4140, 4148, 2708, 4026, 1032, 1082, 4152, 4030, 4018, 4168, 4156, 4394, 152, 2742, 4410, 4406, 4068, 4408, 4412, 4066, 2744, 4026, 4148, 4168, 4140, 3986, 2744, 4002, 2852, 4110, 2854, 4148, 2708, 4026, 4140, 4152, 4030, 4018, 4140, 4168, 4156, 2852, 2854, 4124, 2858, 2658, 2688, 3254, 3254, 3254, 2970, 4082, 4090, 4092, 2982, 3236, 4100, 2702, 4394, 1250, 2970, 2980, 2970, 2980, 2970, 2982, 2970, 2982, 4168, 4140, 3986, 2742, 4004, 2852, 4110, 2854, 2982, 2708, 4026, 4030, 4018, 4148, 4140, 4152, 4168, 4156, 4120, 2858, 2658, 2688, 3254, 3254, 2970, 2982, 2708, 2970, 2982, 4100, 2702, 1250, 4394, 2744, 4026, 4148, 2970, 2980, 4168, 4140, 4168, 3986, 2744, 4002, 2852, 4110, 2854, 2980, 2708, 4026, 4148, 2552, 4168, 4140, 4152, 4030, 4018, 4026, 4140, 4168, 4156, 2970, 2982, 2708, 2970, 4082, 4092, 4090, 4084, 4094, 4090, 3236, 2982, 4100, 2702, 1250, 4394, 4168, 4140, 3986, 2744, 4004, 2852, 4110, 2854, 2982, 2708, 4026, 4140, 4030, 4018, 4140, 4140, 2552, 4168, 4140, 4148, 4140, 4140, 4152, 4168, 4156, 2708, 2970, 4082, 4092, 4090, 4084, 4094, 4090, 3236, 2982, 4066, 2708, 2708, 3082, 4394, 3086, 1286, 1720, 1740, 1760, 1780, 4396, 1286, 2652, 4094, 2742, 4026, 4148, 2708, 3036, 4394, 4168, 4140, 3986, 2744, 4002, 2852, 4110, 2854, 2982, 4148, 2708, 4026, 4140, 4152, 4168, 4030, 4018, 4156, 4406, 4410, 4408, 4412, 2980, 2980, 2970, 3492, 4066, 4068, 4396, 2980, 2708, 2970, 2980, 2970, 2980, 2970, 2980, 2744, 4148, 2970, 2980, 2970, 2980, 4124, 3224, 2690, 3224, 2690, 3224, 2690, 3224, 2690, 4126, 3224, 2690, 2684, 2846, 4124, 3224, 4022, 3032, 4394, 2654, 2708, 4392, 1200, 1202, 2652, 3260, 4092, 2708, 2980, 4396, 1286, 3132, 4394, 4396, 1286, 2652, 2654, 2708, 3082, 4392, 4394, 1200, 1202, 2708, 4394, 1286, 1720, 1740, 1760, 1780, 4396, 1286, 2652, 4094, 2708, 4124, 4072, 2970, 2982, 2708, 2970, 4082, 4090, 4092, 4084, 4094, 4090, 3236, 2974, 4100, 4124, 2708, 2970, 2980, 2970, 4082, 4092, 4090, 4084, 4094, 4090, 3236, 2982, 4100, 2702, 4394, 1250, 2708, 2708, 3082, 1286, 3082, 1720, 1740, 1760, 1780, 1286, 2652, 3260, 4094, 3082, 3086, 1286, 1286, 1720, 1740, 1780, 2652, 3260, 4094, 2708, 2970, 4396, 4082, 4092, 4090, 4084, 4090, 4094, 3236, 2974, 4100, 2708, 2970, 4082, 4090, 4092, 4084, 4090, 4094, 2988, 3236, 4100, 2702, 4394, 1250, 2970, 4396, 2980, 2970, 4082, 4092, 4090, 4084, 4090, 4094, 3236, 2974, 4100, 2708]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "df.events_sequence.iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2183bcea-5226-48ff-a693-7bdd70cb6e21",
      "metadata": {
        "id": "2183bcea-5226-48ff-a693-7bdd70cb6e21"
      },
      "source": [
        "Also, as observed before some event types are so common they do not actually bring a lot of value (as mentioned in the paper as well). We remove those common event types\n",
        "\n",
        "The steps to clean the event sequences are:\n",
        "- keep non-common event types mentioned in list `events_low_frequency`\n",
        "- remove symbols: [] , and store sequences of events as a string without brackets and commas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "b337c2ce-e0fd-49ea-b2e3-a8e71171f7ca",
      "metadata": {
        "id": "b337c2ce-e0fd-49ea-b2e3-a8e71171f7ca"
      },
      "outputs": [],
      "source": [
        "df['clean_events_sequence'] = df.events_sequence.apply(ast.literal_eval).apply(lambda x: [i for i in x if i in events_low_frequency]).astype(str)\\\n",
        "                .replace(r'[\\[\\],]', '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c7dba5-3363-4b8b-95bc-6ea16f64db3b",
      "metadata": {
        "id": "14c7dba5-3363-4b8b-95bc-6ea16f64db3b"
      },
      "source": [
        "## Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18e3f2d-8402-4608-bddc-696b656096be",
      "metadata": {
        "id": "b18e3f2d-8402-4608-bddc-696b656096be"
      },
      "source": [
        "Now we try to experiment using text techniques to transform the list events sequence:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['incident_type'].value_counts()\n",
        "df = df[~df[\"incident_type\"].isin([7, 16, 3, 6, 17])].copy()\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "id": "cyslsE2taYSE"
      },
      "id": "cyslsE2taYSE",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ac4ace38-2df1-45ac-b4ac-ca9179985518",
      "metadata": {
        "id": "ac4ace38-2df1-45ac-b4ac-ca9179985518"
      },
      "outputs": [],
      "source": [
        "target = df['incident_type'].copy() # target column separated\n",
        "#le = LabelEncoder()\n",
        "#target = le.fit_transform(target)\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.clean_events_sequence, target, test_size=0.2,  random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "951515d0-d264-4008-a0b9-862d4de245b9",
      "metadata": {
        "id": "951515d0-d264-4008-a0b9-862d4de245b9"
      },
      "source": [
        "Since the dataset is imbalanced we will use different strategies to battle that. Here we set a new sampling strategy based on a basic script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "1491c584-b6ef-412d-a3bf-f22fb731fa03",
      "metadata": {
        "id": "1491c584-b6ef-412d-a3bf-f22fb731fa03",
        "outputId": "269f7206-cd6c-447e-cb73-dc73e40e22c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{99: 183, 14: 157, 9: 131, 2: 129, 4: 97, 11: 59}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Define custom sampling strategy based on class distribution\n",
        "# Each non-majority class will have equal samples to 15% of the majority class plus their previous samples\n",
        "class_counts = pd.Series(y_train).value_counts()\n",
        "max_class_count = max(class_counts.values)\n",
        "sampling_strategy = {class_counts.index[i]: int(max_class_count * 0.15) + class_counts.values[i]\n",
        "                     for i in range(len(pd.Series(y_train).value_counts().index)) if class_counts.values[i] < max_class_count}\n",
        "sampling_strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89063fd-fac4-49af-bac6-4e7556984ea9",
      "metadata": {
        "id": "e89063fd-fac4-49af-bac6-4e7556984ea9"
      },
      "source": [
        "Starting with CountVectorizer:\n",
        "- Tokenization: Splits text into individual words (tokens).\n",
        "- Builds a Vocabulary: Creates a dictionary of unique words (tokens) from the entire corpus.\n",
        "- Counts the Occurrence: Calculates the frequency (count) of each word in each document.\n",
        "- Transforms Text into a Sparse Matrix: Returns a matrix of shape (n_samples, n_features), where n_samples is the number of documents and n_features is the number of unique words in the vocabulary.\n",
        "\n",
        "  We firstly set the sampling strategy for SMOTE:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd228c63-bcf0-4b11-be54-39aebd1dc739",
      "metadata": {
        "id": "bd228c63-bcf0-4b11-be54-39aebd1dc739"
      },
      "source": [
        "Now we set the pipeline to be used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "430b9fe4-4f4b-40d2-976b-06916ce5688d",
      "metadata": {
        "id": "430b9fe4-4f4b-40d2-976b-06916ce5688d"
      },
      "outputs": [],
      "source": [
        "text_clf = Pipeline([\n",
        "                    ('vect', CountVectorizer()),\n",
        "                     #('decision_tree', DecisionTreeClassifier()),\n",
        "                    ('smote', SMOTE(sampling_strategy=sampling_strategy, random_state=1, k_neighbors=2)),\n",
        "                    ('extra_trees', ExtraTreesClassifier()),\n",
        "                    #('random_forest', RandomForestClassifier())\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "199354cb-7476-434b-92fd-f5ba688b62ce",
      "metadata": {
        "id": "199354cb-7476-434b-92fd-f5ba688b62ce"
      },
      "source": [
        "Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "09d346e4-75de-46e1-aa17-19cd8834e324",
      "metadata": {
        "id": "09d346e4-75de-46e1-aa17-19cd8834e324",
        "outputId": "7339b605-3416-4e0b-fcc7-6b8b15c9f1cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer()),\n",
              "                ('smote',\n",
              "                 SMOTE(k_neighbors=2, random_state=1,\n",
              "                       sampling_strategy={2: 129, 4: 97, 9: 131, 11: 59,\n",
              "                                          14: 157, 99: 183})),\n",
              "                ('extra_trees', ExtraTreesClassifier())])"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()),\n",
              "                (&#x27;smote&#x27;,\n",
              "                 SMOTE(k_neighbors=2, random_state=1,\n",
              "                       sampling_strategy={2: 129, 4: 97, 9: 131, 11: 59,\n",
              "                                          14: 157, 99: 183})),\n",
              "                (&#x27;extra_trees&#x27;, ExtraTreesClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Pipeline<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()),\n",
              "                (&#x27;smote&#x27;,\n",
              "                 SMOTE(k_neighbors=2, random_state=1,\n",
              "                       sampling_strategy={2: 129, 4: 97, 9: 131, 11: 59,\n",
              "                                          14: 157, 99: 183})),\n",
              "                (&#x27;extra_trees&#x27;, ExtraTreesClassifier())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">SMOTE</label><div class=\"sk-toggleable__content fitted\"><pre>SMOTE(k_neighbors=2, random_state=1,\n",
              "      sampling_strategy={2: 129, 4: 97, 9: 131, 11: 59, 14: 157, 99: 183})</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;ExtraTreesClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\">?<span>Documentation for ExtraTreesClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ExtraTreesClassifier()</pre></div> </div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "text_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3273e4d7-fdc3-41ad-b5cf-8af15e8c0d9f",
      "metadata": {
        "id": "3273e4d7-fdc3-41ad-b5cf-8af15e8c0d9f"
      },
      "source": [
        "Print the results for the particular split of test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "63957f6a-e782-4f88-a226-8e41e6ea2c05",
      "metadata": {
        "id": "63957f6a-e782-4f88-a226-8e41e6ea2c05",
        "outputId": "4f669c3b-c939-4fe1-feb7-65982a071507",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           2       0.79      0.82      0.81        28\n",
            "           4       0.68      0.79      0.73        19\n",
            "           9       0.91      0.42      0.57        24\n",
            "          11       1.00      0.00      0.00         5\n",
            "          13       0.64      0.89      0.74        61\n",
            "          14       0.73      0.53      0.62        30\n",
            "          99       0.54      0.50      0.52        30\n",
            "\n",
            "    accuracy                           0.68       197\n",
            "   macro avg       0.75      0.56      0.57       197\n",
            "weighted avg       0.70      0.68      0.66       197\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf_predict = text_clf.predict(X_test)\n",
        "print(classification_report(y_test, clf_predict, zero_division=1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vec = TfidfVectorizer()\n",
        "embeddings = vec.fit_transform(X_train).toarray()\n",
        "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=1, k_neighbors=2)\n",
        "trees = ExtraTreesClassifier()\n",
        "X_train_res, y_train_res = smote.fit_resample(embeddings, y_train)\n",
        "\n",
        "trees.fit(X_train_res, y_train_res)\n",
        "predictions = trees.predict(vec.transform(X_test).toarray())\n",
        "print(classification_report(y_test, predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "4SWVd4Mbsiz5",
        "outputId": "3773c064-a44a-4a0d-8fbb-46c75ce3db7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "4SWVd4Mbsiz5",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           2       0.79      0.79      0.79        28\n",
            "           4       0.92      0.63      0.75        19\n",
            "           9       0.83      0.42      0.56        24\n",
            "          11       1.00      0.00      0.00         5\n",
            "          13       0.61      0.89      0.72        61\n",
            "          14       0.81      0.57      0.67        30\n",
            "          99       0.46      0.53      0.49        30\n",
            "\n",
            "    accuracy                           0.66       197\n",
            "   macro avg       0.77      0.55      0.57       197\n",
            "weighted avg       0.71      0.66      0.65       197\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X_train.values)\n"
      ],
      "metadata": {
        "id": "OKrALanIwbPA"
      },
      "id": "OKrALanIwbPA",
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "embeddings = vectorizer.fit_transform(X_train)\n",
        "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=1, k_neighbors=2)\n",
        "X_train_res, y_train_res = smote.fit_resample(embeddings, y_train)\n",
        "le = LabelEncoder()\n",
        "y_train_res_e = le.fit_transform(y_train_res)\n",
        "y_test_e = le.transform(y_test)\n",
        "dtrain = xgb.DMatrix(data=X_train_res, label=y_train_res_e)\n",
        "dtest = xgb.DMatrix(data=vectorizer.transform(X_test), label=y_test_e)\n",
        "num_classes = len(np.unique(target))\n",
        "params = {\n",
        "    'objective': 'multi:softmax',   # Cambiar a 'multi:softprob' si necesitas probabilidades en lugar de etiquetas\n",
        "    'num_class': num_classes,       # Nmero de clases\n",
        "    'max_depth': 4,                 # Profundidad mxima del rbol\n",
        "    'learning_rate': 0.1,           # Tasa de aprendizaje\n",
        "    'n_estimators': 100,            # Nmero de rboles\n",
        "    'eval_metric': 'mlogloss'       # Mtrica para clasificacin multiclase\n",
        "}\n",
        "bst = xgb.train(params, dtrain)\n",
        "predictions = bst.predict(dtest)\n",
        "accuracy = accuracy_score(y_test_e, predictions)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "7pd9sZ47tJQ6",
        "outputId": "9a6f023b-4e68-4703-b7e0-9db3407e01c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "id": "7pd9sZ47tJQ6",
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'LabelEncoder' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-8a315939ed92>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msmote\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train_res\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmote\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_train_res_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_res\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_test_e\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LabelEncoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U imbalanced-learn"
      ],
      "metadata": {
        "id": "Zt4FhmDO-IPk",
        "outputId": "cdc8868d-5eb7-4042-ff48-4399e7b4cece",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Zt4FhmDO-IPk",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.5.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"incident_type\"].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "id": "0lnEgVMlZ_4P",
        "outputId": "31281faa-dd69-4771-ebba-fd901d29e909"
      },
      "id": "0lnEgVMlZ_4P",
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "incident_type\n",
              "13    318\n",
              "99    175\n",
              "14    149\n",
              "2     119\n",
              "9     117\n",
              "4      78\n",
              "11     26\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incident_type</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.base import TransformerMixin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import differential_evolution\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy"
      ],
      "metadata": {
        "id": "1GTQ_pBgxpIf"
      },
      "id": "1GTQ_pBgxpIf",
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecVectorizer(TransformerMixin):\n",
        "    def __init__(self, size=100, window=5, min_count=1, workers=4):\n",
        "        self.size = size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.w2v_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.w2v_model = Word2Vec(sentences, vector_size=self.size, window=self.window,\n",
        "                                  min_count=self.min_count, workers=self.workers)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        transformed_data = np.array([\n",
        "            np.mean([self.w2v_model.wv[word] for word in sentence.split() if word in self.w2v_model.wv]\n",
        "                    or [np.zeros(self.size)], axis=0)\n",
        "            for sentence in X\n",
        "        ])\n",
        "        return csr_matrix(transformed_data)\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.fit(X, y)\n",
        "        return self.transform(X, y)\n",
        "\n",
        "class Experiment:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=7, stratify=y)\n",
        "        self.trained_models = {}  # To store trained models\n",
        "        self.results = []\n",
        "        self.sampling_strategies = {\n",
        "            \"SMOTE\": SMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            \"Borderline-SMOTE\": BorderlineSMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            \"ADASYN\": ADASYN(sampling_strategy='auto', random_state=1, n_neighbors=3),\n",
        "            \"RandomOversampler\": RandomOverSampler(sampling_strategy='auto', random_state=1),\n",
        "            \"SMOTE-ENN\": SMOTEENN(sampling_strategy='auto', random_state=1),\n",
        "            \"SMOTE-Tomek\": SMOTETomek(sampling_strategy='auto', random_state=1)\n",
        "        }\n",
        "\n",
        "        self.vectorizers = {\n",
        "            \"TFIDF\": TfidfVectorizer(),\n",
        "            \"Count\": CountVectorizer(),\n",
        "            \"Word2Vec\": Word2VecVectorizer(size=100, window=5, min_count=1)\n",
        "        }\n",
        "        self.classifiers = {\n",
        "            'LogisticRegression': LogisticRegression(),\n",
        "            'DecisionTree': DecisionTreeClassifier(),\n",
        "            'RandomForest': RandomForestClassifier(),\n",
        "            'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
        "            'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
        "            'AdaBoostClassifier': AdaBoostClassifier(),\n",
        "            'GaussianNB': GaussianNB(),\n",
        "            'KNN': KNeighborsClassifier(),\n",
        "            'SVM': SVC(probability=True),\n",
        "            'XGBoost': XGBClassifier(),\n",
        "        }\n",
        "\n",
        "    def test(self, model, model_name, vectorizer, vectorizer_name, sampler, sampler_name):\n",
        "        \"\"\"Test a model with Stratified K-Fold and return an array with metric results.\"\"\"\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "        accuracies, recalls, precisions, f1s = [], [], [], []\n",
        "\n",
        "        for train_index, test_index in skf.split(self.X, self.y):\n",
        "            X_train, X_test = self.X[train_index], self.X[test_index]\n",
        "            y_train, y_test = self.y[train_index], self.y[test_index]\n",
        "\n",
        "            X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "            X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "            X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "            model.fit(X_resampled, y_resampled)\n",
        "\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Collect metrics for each fold\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "            recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
        "            precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
        "            f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "        # Return average metrics as a result array\n",
        "        return [\n",
        "            model_name, vectorizer_name, sampler_name,\n",
        "            np.mean(accuracies), np.std(accuracies),\n",
        "            np.mean(recalls), np.std(recalls),\n",
        "            np.mean(precisions), np.std(precisions),\n",
        "            np.mean(f1s), np.std(f1s)\n",
        "        ]\n",
        "\n",
        "    def run(self):\n",
        "        results = []\n",
        "\n",
        "        # Iterate over vectorizers, samplers, and classifiers\n",
        "        for vect_name, ovectorizer in self.vectorizers.items():\n",
        "            vectorizer = deepcopy(ovectorizer)\n",
        "            for samp_name, osampler in self.sampling_strategies.items():\n",
        "                sampler = deepcopy(osampler)\n",
        "                for clf_name, omodel in tqdm(self.classifiers.items()):\n",
        "                    model = deepcopy(omodel)\n",
        "                    # Run the test function for each combination and collect results\n",
        "                    print(f\"\\n=== Vectorizer: {vect_name} ===\")\n",
        "                    print(f\"\\n=== Sampling Strategy: {samp_name} ===\")\n",
        "                    print(f\"\\n=== Classifier: {clf_name} ===\\n\")\n",
        "                    result = self.test(\n",
        "                        model=model,\n",
        "                        model_name=clf_name,\n",
        "                        vectorizer=vectorizer,\n",
        "                        vectorizer_name=vect_name,\n",
        "                        sampler=sampler,\n",
        "                        sampler_name=samp_name\n",
        "                    )\n",
        "                    # Store the trained model and add result to the results list\n",
        "                    results.append(result)\n",
        "\n",
        "                vectorizer = deepcopy(ovectorizer)\n",
        "                sampler = deepcopy(osampler)\n",
        "                model = deepcopy(omodel)\n",
        "                X_train = vectorizer.fit_transform(self.X_train).toarray()\n",
        "                X_resampled, y_resampled = sampler.fit_resample(self.X_train, self.y_train)\n",
        "                model.fit(X_resampled, y_resampled)\n",
        "                self.trained_models[(vect_name, samp_name, clf_name)] = model\n",
        "\n",
        "        # Convert results to a DataFrame for better readability\n",
        "        columns = [\n",
        "            'Model', 'Vectorizer', 'Sampler',\n",
        "            'Accuracy Mean', 'Accuracy Std',\n",
        "            'Recall Mean', 'Recall Std',\n",
        "            'Precision Mean', 'Precision Std',\n",
        "            'F1 Mean', 'F1 Std'\n",
        "        ]\n",
        "        results_df = pd.DataFrame(results, columns=columns)\n",
        "        self.results = results_df\n",
        "        return results_df\n",
        "\n",
        "    def create_ensemble(self):\n",
        "        # Gather predictions from all models\n",
        "        predictions = {}\n",
        "        for (vect_name, samp_name, clf_name), model in self.trained_models.items():\n",
        "            vectorizer = deepcopy(self.vectorizers[vect_name])\n",
        "            X_test_vect = vectorizer.transform(self.X_test)\n",
        "            predictions[(vect_name, samp_name, clf_name)] = model.predict(X_test_vect)\n",
        "\n",
        "        # Convert predictions to a 2D array\n",
        "        pred_matrix = np.array(list(predictions.values())).T\n",
        "\n",
        "        # Define fitness function for genetic algorithm\n",
        "        def fitness(weights):\n",
        "            weighted_pred = np.average(pred_matrix, axis=1, weights=weights)\n",
        "            final_pred = (weighted_pred > 0.5).astype(int)\n",
        "            return -f1_score(self.y_test, final_pred, average='weighted')\n",
        "\n",
        "        # Genetic algorithm for weight optimization\n",
        "        bounds = [(0, 1)] * pred_matrix.shape[1]\n",
        "        result = differential_evolution(fitness, bounds)\n",
        "\n",
        "        # Final ensemble prediction using optimized weights\n",
        "        optimized_weights = result.x\n",
        "        final_weighted_pred = np.average(pred_matrix, axis=1, weights=optimized_weights)\n",
        "        ensemble_pred = (final_weighted_pred > 0.5).astype(int)\n",
        "\n",
        "        # Calculate final F1 score\n",
        "        ensemble_f1 = f1_score(self.y_test, ensemble_pred, average='weighted')\n",
        "        return ensemble_pred, ensemble_f1\n",
        "\n",
        "le = LabelEncoder()\n",
        "target = le.fit_transform(target)\n",
        "exp = Experiment(df.clean_events_sequence, target)\n",
        "results = exp.run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "Z5-kRVEJxLjj",
        "outputId": "773dc520-2e98-4107-a11f-d746402e5577"
      },
      "id": "Z5-kRVEJxLjj",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: LogisticRegression ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|         | 1/10 [00:03<00:28,  3.17s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: DecisionTree ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|        | 2/10 [00:05<00:21,  2.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: RandomForest ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|       | 3/10 [00:11<00:30,  4.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: ExtraTreesClassifier ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|      | 4/10 [00:17<00:28,  4.69s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: GradientBoostingClassifier ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|     | 5/10 [05:17<09:16, 111.40s/it]/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: AdaBoostClassifier ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            " 60%|    | 6/10 [05:28<05:09, 77.33s/it] "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: GaussianNB ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|   | 7/10 [05:29<02:36, 52.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: KNN ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|  | 8/10 [05:30<01:11, 35.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: SVM ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            " 90%| | 9/10 [05:58<00:33, 33.52s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Vectorizer: TFIDF ===\n",
            "\n",
            "=== Sampling Strategy: SMOTE ===\n",
            "\n",
            "=== Classifier: XGBoost ===\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 10/10 [07:02<00:00, 42.22s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "could not convert string to float: '2684 2682 2682 2682 2682 2892 4054 2736 4020 4016 4028 3354 4024 3506 4056 4032 3634 2740 4030 4018 4126 2682 3982 4054 2736 3354 4020 4028 2740 4396 2740 4030 4020 2972 3234 2976 4100 4396 3008 3980 4180 2682 4396 2682 4180 4016 4020 3364 150 3354 4396 4028 3354 3354 4028 3354 4016 3354 1266 3354 4028 4028 3354 4016 4020 148 148 4396 4020 2682 3354 4396 148 3620 2682 3982 4054 2686 2736 4020 4028 3354 3354 4028 4032 4056 3354 4054 4016 3354 1266 3354 4028 4028 3354 4016 4020'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-5e2a9a63530d>\u001b[0m in \u001b[0;36m<cell line: 172>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclean_events_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-50-5e2a9a63530d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvect_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msamp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    206\u001b[0m         \"\"\"\n\u001b[1;32m    207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_more_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0marrays_transformer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mArraysTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m         self.sampling_strategy_ = check_sampling_strategy(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m_check_X_y\u001b[0;34m(self, X, y, accept_sparse)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0maccept_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_target_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindicate_one_vs_all\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinarize_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"y\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    651\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1299\u001b[0m         )\n\u001b[1;32m   1300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1301\u001b[0;31m     X = check_array(\n\u001b[0m\u001b[1;32m   1302\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_sparse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1011\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1012\u001b[0;31m                     \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_asarray_with_order\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1013\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_array_api.py\u001b[0m in \u001b[0;36m_asarray_with_order\u001b[0;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[1;32m    743\u001b[0m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m         \u001b[0;31m# At this point array is a NumPy ndarray. We convert it to an array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self, dtype, copy)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         \"\"\"\n\u001b[1;32m   1030\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0musing_copy_on_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mastype_is_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2684 2682 2682 2682 2682 2892 4054 2736 4020 4016 4028 3354 4024 3506 4056 4032 3634 2740 4030 4018 4126 2682 3982 4054 2736 3354 4020 4028 2740 4396 2740 4030 4020 2972 3234 2976 4100 4396 3008 3980 4180 2682 4396 2682 4180 4016 4020 3364 150 3354 4396 4028 3354 3354 4028 3354 4016 3354 1266 3354 4028 4028 3354 4016 4020 148 148 4396 4020 2682 3354 4396 148 3620 2682 3982 4054 2686 2736 4020 4028 3354 3354 4028 4032 4056 3354 4054 4016 3354 1266 3354 4028 4028 3354 4016 4020'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "exp.trained_models[('TFIDF', 'SMOTE', 'LogisticRegression')].coef_.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96YNRVcGzvD0",
        "outputId": "bb4e48c8-b4e9-4693-c411-75528143155d"
      },
      "id": "96YNRVcGzvD0",
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(7, 100)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.to_csv('results.csv')"
      ],
      "metadata": {
        "id": "Jt2X8YoLxQKk"
      },
      "id": "Jt2X8YoLxQKk",
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "embeddings = vectorizer.fit_transform(df.clean_events_sequence)\n",
        "class_counts = pd.Series(target).value_counts()\n",
        "max_class_count = max(class_counts.values)\n",
        "sampling_strategy = {class_counts.index[i]: int(max_class_count * 0.15) + class_counts.values[i]\n",
        "                     for i in range(len(pd.Series(target).value_counts().index)) if class_counts.values[i] < max_class_count}\n",
        "smote = SMOTE(sampling_strategy=sampling_strategy, random_state=1, k_neighbors=2)\n",
        "X_res, y_res = smote.fit_resample(embeddings, target)\n",
        "le = LabelEncoder()\n",
        "y_res_e = le.fit_transform(y_res)\n"
      ],
      "metadata": {
        "id": "060w7FEq1Mmh"
      },
      "id": "060w7FEq1Mmh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "experiment = SCNB()\n",
        "X_train_res, y_train_res_e = pre_process_data(X_train, y_train)\n",
        "experiment.train_classifiers(X_train_res, y_train_res_e)\n",
        "X_res, y_res = pre_process_data(df.clean_events_sequence, target)\n",
        "experiment.evaluate_models(X_res, y_res)"
      ],
      "metadata": {
        "id": "QnYfReiNzDKm",
        "outputId": "9e849f84-6d84-41bb-a264-e7f087a83883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        }
      },
      "id": "QnYfReiNzDKm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All classifiers trained successfully.\n",
            "Evaluation complete.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                Model  Accuracy Mean  Accuracy Std  Accuracy Median  \\\n",
              "0  LogisticRegression       0.742848      0.046359         0.735294   \n",
              "1        DecisionTree       0.687280      0.096798         0.650327   \n",
              "2        RandomForest       0.812945      0.079332         0.764706   \n",
              "3          GaussianNB       0.680684      0.044453         0.653595   \n",
              "4                 KNN       0.638828      0.068662         0.611111   \n",
              "5                 SVM       0.791280      0.040835         0.781046   \n",
              "6             XGBoost       0.811640      0.085974         0.774510   \n",
              "\n",
              "   Recall Mean  Recall Std  Recall Median  Precision Mean  Precision Std  \\\n",
              "0     0.742848    0.046359       0.735294        0.756782       0.045676   \n",
              "1     0.687280    0.096798       0.650327        0.696068       0.093068   \n",
              "2     0.812945    0.079332       0.764706        0.822176       0.073905   \n",
              "3     0.680684    0.044453       0.653595        0.705803       0.038073   \n",
              "4     0.638828    0.068662       0.611111        0.646472       0.071641   \n",
              "5     0.791280    0.040835       0.781046        0.799608       0.035450   \n",
              "6     0.811640    0.085974       0.774510        0.817087       0.083693   \n",
              "\n",
              "   Precision Median  \n",
              "0          0.753547  \n",
              "1          0.659223  \n",
              "2          0.777173  \n",
              "3          0.696893  \n",
              "4          0.630387  \n",
              "5          0.791265  \n",
              "6          0.783971  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d73e2969-662d-4df6-9e74-3e50bb163a61\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Accuracy Mean</th>\n",
              "      <th>Accuracy Std</th>\n",
              "      <th>Accuracy Median</th>\n",
              "      <th>Recall Mean</th>\n",
              "      <th>Recall Std</th>\n",
              "      <th>Recall Median</th>\n",
              "      <th>Precision Mean</th>\n",
              "      <th>Precision Std</th>\n",
              "      <th>Precision Median</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>LogisticRegression</td>\n",
              "      <td>0.742848</td>\n",
              "      <td>0.046359</td>\n",
              "      <td>0.735294</td>\n",
              "      <td>0.742848</td>\n",
              "      <td>0.046359</td>\n",
              "      <td>0.735294</td>\n",
              "      <td>0.756782</td>\n",
              "      <td>0.045676</td>\n",
              "      <td>0.753547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>DecisionTree</td>\n",
              "      <td>0.687280</td>\n",
              "      <td>0.096798</td>\n",
              "      <td>0.650327</td>\n",
              "      <td>0.687280</td>\n",
              "      <td>0.096798</td>\n",
              "      <td>0.650327</td>\n",
              "      <td>0.696068</td>\n",
              "      <td>0.093068</td>\n",
              "      <td>0.659223</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>RandomForest</td>\n",
              "      <td>0.812945</td>\n",
              "      <td>0.079332</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.812945</td>\n",
              "      <td>0.079332</td>\n",
              "      <td>0.764706</td>\n",
              "      <td>0.822176</td>\n",
              "      <td>0.073905</td>\n",
              "      <td>0.777173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GaussianNB</td>\n",
              "      <td>0.680684</td>\n",
              "      <td>0.044453</td>\n",
              "      <td>0.653595</td>\n",
              "      <td>0.680684</td>\n",
              "      <td>0.044453</td>\n",
              "      <td>0.653595</td>\n",
              "      <td>0.705803</td>\n",
              "      <td>0.038073</td>\n",
              "      <td>0.696893</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KNN</td>\n",
              "      <td>0.638828</td>\n",
              "      <td>0.068662</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.638828</td>\n",
              "      <td>0.068662</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.646472</td>\n",
              "      <td>0.071641</td>\n",
              "      <td>0.630387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SVM</td>\n",
              "      <td>0.791280</td>\n",
              "      <td>0.040835</td>\n",
              "      <td>0.781046</td>\n",
              "      <td>0.791280</td>\n",
              "      <td>0.040835</td>\n",
              "      <td>0.781046</td>\n",
              "      <td>0.799608</td>\n",
              "      <td>0.035450</td>\n",
              "      <td>0.791265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>XGBoost</td>\n",
              "      <td>0.811640</td>\n",
              "      <td>0.085974</td>\n",
              "      <td>0.774510</td>\n",
              "      <td>0.811640</td>\n",
              "      <td>0.085974</td>\n",
              "      <td>0.774510</td>\n",
              "      <td>0.817087</td>\n",
              "      <td>0.083693</td>\n",
              "      <td>0.783971</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d73e2969-662d-4df6-9e74-3e50bb163a61')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d73e2969-662d-4df6-9e74-3e50bb163a61 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d73e2969-662d-4df6-9e74-3e50bb163a61');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7c99de89-e1dd-4cf9-aeb7-3270f55e0658\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7c99de89-e1dd-4cf9-aeb7-3270f55e0658')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7c99de89-e1dd-4cf9-aeb7-3270f55e0658 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"experiment\",\n  \"rows\": 7,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"LogisticRegression\",\n          \"DecisionTree\",\n          \"SVM\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07023159201964846,\n        \"min\": 0.6388278152791171,\n        \"max\": 0.8129454623379406,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.7428479588556735,\n          0.6872795456980606,\n          0.791280402871531\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022418785215148375,\n        \"min\": 0.04083474991104546,\n        \"max\": 0.09679823584884718,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.04635905007765238,\n          0.09679823584884718,\n          0.04083474991104546\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0699558711313239,\n        \"min\": 0.6111111111111112,\n        \"max\": 0.7810457516339869,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.7352941176470589,\n          0.6503267973856209,\n          0.7810457516339869\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07023159201964846,\n        \"min\": 0.6388278152791171,\n        \"max\": 0.8129454623379406,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.7428479588556735,\n          0.6872795456980606,\n          0.791280402871531\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.022418785215148375,\n        \"min\": 0.04083474991104546,\n        \"max\": 0.09679823584884718,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.04635905007765238,\n          0.09679823584884718,\n          0.04083474991104546\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0699558711313239,\n        \"min\": 0.6111111111111112,\n        \"max\": 0.7810457516339869,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.7352941176470589,\n          0.6503267973856209,\n          0.7810457516339869\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06805856362626252,\n        \"min\": 0.646472089165559,\n        \"max\": 0.8221759838984815,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.756782024153418,\n          0.6960679184811273,\n          0.7996076362196127\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.023116936735925443,\n        \"min\": 0.03544955790983071,\n        \"max\": 0.09306769191813657,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.045675522083485735,\n          0.09306769191813657,\n          0.03544955790983071\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Median\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.06510310241366583,\n        \"min\": 0.6303866984704039,\n        \"max\": 0.7912654637403874,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0.7535474181314084,\n          0.659222655196808,\n          0.7912654637403874\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "131b2808-6a94-4c7e-b5e1-952832511e85",
      "metadata": {
        "id": "131b2808-6a94-4c7e-b5e1-952832511e85"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dea2210-71e8-4c09-8a20-347a24c61512",
      "metadata": {
        "id": "7dea2210-71e8-4c09-8a20-347a24c61512"
      },
      "source": [
        "Now we calculate the cross validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb09cd66-9c74-40d7-b566-8789fcac9c48",
      "metadata": {
        "id": "eb09cd66-9c74-40d7-b566-8789fcac9c48"
      },
      "outputs": [],
      "source": [
        "class_counts = target.value_counts()\n",
        "max_class_count = max(class_counts.values)\n",
        "sampling_strategy_cross_val = {class_counts.index[i]: int(max_class_count * 0.15) + class_counts.values[i]\n",
        "                     for i in range(len(y_train.value_counts().index)) if class_counts.values[i] < max_class_count}\n",
        "cross_val_clf = Pipeline([\n",
        "                    ('vect', CountVectorizer()),\n",
        "                    ('smote', SMOTE(sampling_strategy=sampling_strategy_cross_val, random_state=1, k_neighbors=2)),\n",
        "                    ('extra_trees', ExtraTreesClassifier()),\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e64929b-894e-4eab-89a5-8f3511bbf8e2",
      "metadata": {
        "id": "5e64929b-894e-4eab-89a5-8f3511bbf8e2",
        "outputId": "f256a97b-0e15-4102-c8c1-f52e9f7ff6db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6409043854696029"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = cross_val_score(cross_val_clf, df.clean_events_sequence.sample(frac=1, random_state=1), target.sample(frac=1, random_state=1),\n",
        "                        cv=4, scoring='accuracy',n_jobs = -1)\n",
        "scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a8558d-4b91-4f10-83fd-bb11738944f9",
      "metadata": {
        "id": "d9a8558d-4b91-4f10-83fd-bb11738944f9"
      },
      "source": [
        "Create a custom scoring f1 function with zero_division parameter for cross validation to avoid nan values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9e7a78-c871-4a73-b78c-a061d729ea16",
      "metadata": {
        "id": "4b9e7a78-c871-4a73-b78c-a061d729ea16"
      },
      "outputs": [],
      "source": [
        "# Create a custom scoring function with zero_division parameter\n",
        "def custom_f1_score(y_true, y_pred):\n",
        "    return f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "# Wrap the custom scoring function using make_scorer\n",
        "f1_scorer = make_scorer(custom_f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7745109b-673e-4128-9d46-3cdab6b2da93",
      "metadata": {
        "id": "7745109b-673e-4128-9d46-3cdab6b2da93",
        "outputId": "db9c57cf-ad7c-4133-a2e8-a63fc21a047b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.61823676 0.61774323 0.65055584 0.57616849]\n",
            "0.615676079863001\n"
          ]
        }
      ],
      "source": [
        "scores = cross_val_score(cross_val_clf, df.clean_events_sequence.sample(frac=1, random_state=1), target.sample(frac=1, random_state=1),\n",
        "                        cv=4, scoring=f1_scorer,n_jobs = -1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d786ce5a-f7ea-4873-ad20-e37fb230e201",
      "metadata": {
        "id": "d786ce5a-f7ea-4873-ad20-e37fb230e201"
      },
      "source": [
        "F1 is calculated as:\n",
        "$$ F1 Score= 2\\frac{PrecisionRecall}{Precision+Recall}\n",
        "\n",
        "$$\n",
        "There are some minority classes with no correct predictions ($recall=0$) resulting in a null value for the whole f1 score when using a non-custom f1 scorer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091c8078-9125-4d27-9bb7-4324688918bf",
      "metadata": {
        "id": "091c8078-9125-4d27-9bb7-4324688918bf"
      },
      "source": [
        "## GridsearchCV\n",
        "Now we use gridsearchCV to find the optimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e22f86-1790-4f29-badb-3baefd9a084d",
      "metadata": {
        "scrolled": true,
        "id": "77e22f86-1790-4f29-badb-3baefd9a084d",
        "outputId": "45d3ded7-70c6-4033-b7e7-bd8e3d77f144"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{99: 222,\n",
              " 14: 196,\n",
              " 2: 166,\n",
              " 9: 164,\n",
              " 4: 125,\n",
              " 11: 73,\n",
              " 17: 57,\n",
              " 6: 53,\n",
              " 3: 52,\n",
              " 16: 51,\n",
              " 7: 51}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_counts = target.value_counts()\n",
        "max_class_count = max(class_counts.values)\n",
        "sampling_strategy_grid = {class_counts.index[i]: int(max_class_count * 0.15) + class_counts.values[i]\n",
        "                     for i in range(len(y_train.value_counts().index)) if class_counts.values[i] < max_class_count}\n",
        "sampling_strategy_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a495103-d65d-4f38-9dc9-c04afadd49b0",
      "metadata": {
        "id": "6a495103-d65d-4f38-9dc9-c04afadd49b0"
      },
      "outputs": [],
      "source": [
        "grid_clf = Pipeline([\n",
        "                    ('vect', CountVectorizer()),\n",
        "                    ('smote', SMOTE(sampling_strategy=sampling_strategy_grid, random_state=1, k_neighbors=2)),\n",
        "                    ('extra_trees', ExtraTreesClassifier()),\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f0c64e-3389-44a4-8060-68f6b3405a5a",
      "metadata": {
        "id": "55f0c64e-3389-44a4-8060-68f6b3405a5a",
        "outputId": "d37d1752-0dca-4627-9c94-060ffb949066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'extra_trees__max_depth': None, 'extra_trees__n_estimators': 300, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1)}\n",
            "Best Score F1: 0.6406886229518796\n",
            "Accuracy: 0.6548246439550787\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter grid for GridSearchCV 15%\n",
        "param_grid = {\n",
        "    'vect__max_features': [500, 1000],       # Example parameter for CountVectorizer\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],   # Unigrams, bigrams, trigrams\n",
        "    'extra_trees__n_estimators': [100, 200, 300, 400],        # Number of trees in ExtraTrees\n",
        "    'extra_trees__max_depth': [None, 10]        # Depth of each tree\n",
        "}\n",
        "\n",
        "# Cross-validation strategy set here to replicate results\n",
        "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Define GridSearchCV with the pipeline and parameter grid\n",
        "grid_search = GridSearchCV(grid_clf, param_grid, cv=cv, scoring=f1_scorer, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(df.clean_events_sequence, target)\n",
        "\n",
        "# Output the best parameters and the best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score F1:\", grid_search.best_score_)\n",
        "print(\"Accuracy:\", str(np.mean(cross_val_score(grid_search.best_estimator_, df.clean_events_sequence, target, cv=cv, scoring='accuracy'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6c7f72",
      "metadata": {
        "id": "cc6c7f72"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043c4514",
      "metadata": {
        "id": "043c4514"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b46fda7",
      "metadata": {
        "id": "4b46fda7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "137fe5af",
      "metadata": {
        "id": "137fe5af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bf6a4cf-1c8e-4aba-f74f-1c81c7aee5d1",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.7)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.1.2)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.10/dist-packages (0.12.4)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.13.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.8.0)\n",
            "Requirement already satisfied: datashader in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.16.3)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.6.1)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.20.0)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.13.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.24.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.10/dist-packages (from xgboost) (2.23.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from imbalanced-learn) (3.5.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (1.3.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (24.2)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (11.0.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (2024.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2024.2)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2024.10.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (1.0.0)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2.1.1)\n",
            "Requirement already satisfied: pyct in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (0.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2.32.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2024.10.0)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.10/dist-packages (from holoviews->umap-learn[plot]) (1.5.3)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.10/dist-packages (from holoviews->umap-learn[plot]) (3.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (3.2.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->umap-learn[plot]) (3.0.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (6.2.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (2.0.3)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (3.0.0)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->umap-learn[plot]) (1.16.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (2024.10.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (1.4.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (2024.8.30)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->datashader->umap-learn[plot]) (3.20.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask->datashader->umap-learn[plot]) (1.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=1.0->holoviews->umap-learn[plot]) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=1.0->holoviews->umap-learn[plot]) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=1.0->holoviews->umap-learn[plot]) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "! pip install hmmlearn umap-learn umap-learn[plot] xgboost imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a85fd88",
      "metadata": {
        "id": "8a85fd88"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from gensim.models import Word2Vec\n",
        "from hmmlearn import hmm\n",
        "import umap\n",
        "import umap.plot\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import xgboost as xgb\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';')\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "cnXTFBgEyfAS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "f7ebe9cc-b582-429d-b512-cd6148407110"
      },
      "id": "cnXTFBgEyfAS",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0  incident_id  \\\n",
              "895         895      4604847   \n",
              "384         384      4451781   \n",
              "690         690      4465347   \n",
              "970         970      4610465   \n",
              "389         389      4451923   \n",
              "\n",
              "                                     vehicles_sequence  \\\n",
              "895  [537, 537, 537, 537, 537, 537, 537, 537, 537, ...   \n",
              "384  [609, 609, 609, 609, 609, 609, 609, 609, 609, ...   \n",
              "690  [638, 638, 638, 638, 638, 638, 638, 638, 638, ...   \n",
              "970  [529, 529, 529, 529, 529, 529, 529, 529, 529, ...   \n",
              "389  [637, 637, 637, 637, 637, 637, 637, 637, 637, ...   \n",
              "\n",
              "                                       events_sequence  \\\n",
              "895  [3658, 4068, 3658, 4068, 3658, 4068, 3658, 406...   \n",
              "384  [4068, 3658, 4068, 3658, 4066, 3658, 4068, 365...   \n",
              "690  [2742, 4002, 4110, 2708, 4026, 4148, 4140, 412...   \n",
              "970  [4066, 4068, 4068, 3658, 4068, 3658, 4068, 365...   \n",
              "389  [3636, 3658, 2956, 2956, 4066, 3636, 3658, 295...   \n",
              "\n",
              "                          seconds_to_incident_sequence  approx_lat  \\\n",
              "895  [-14388, -14257, -14222, -14127, -14090, -1381...   50.782892   \n",
              "384  [-14137, -14078, -13904, -13892, -13519, -1258...   50.911038   \n",
              "690  [-8280, -8280, -8278, -8275, -8275, -8275, -82...   50.854381   \n",
              "970  [-14271, -14121, -13619, -13600, -13028, -1292...   50.720086   \n",
              "389  [-14397, -14397, -14342, -14284, -14241, -1422...   50.805012   \n",
              "\n",
              "     approx_lon                                 train_kph_sequence  \\\n",
              "895    4.421971  [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, ...   \n",
              "384    4.151967  [0.2, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.2, ...   \n",
              "690    2.737718  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "970    4.397469  [1.6, 2.7, 3.2, 0.0, 0.1, 0.0, 1.8, 0.0, 1.0, ...   \n",
              "389    4.600712  [0.0, 0.0, 31.7, 25.0, 1.9, 0.0, 0.0, 22.0, 27...   \n",
              "\n",
              "                                  dj_ac_state_sequence  \\\n",
              "895  [False, False, False, False, False, False, Fal...   \n",
              "384  [False, False, False, False, False, False, Fal...   \n",
              "690  [False, False, False, False, False, False, Fal...   \n",
              "970  [False, False, False, False, False, False, Fal...   \n",
              "389  [False, False, False, False, False, False, Fal...   \n",
              "\n",
              "                                  dj_dc_state_sequence  incident_type  \n",
              "895  [True, True, True, True, True, True, True, Tru...              2  \n",
              "384  [True, True, True, True, True, True, True, Tru...              9  \n",
              "690  [False, False, False, False, False, False, Fal...             13  \n",
              "970  [True, True, True, True, True, True, True, Tru...              9  \n",
              "389  [True, True, True, True, True, True, True, Tru...             13  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-421907d7-933a-4268-bd91-43ce33602e4a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>incident_id</th>\n",
              "      <th>vehicles_sequence</th>\n",
              "      <th>events_sequence</th>\n",
              "      <th>seconds_to_incident_sequence</th>\n",
              "      <th>approx_lat</th>\n",
              "      <th>approx_lon</th>\n",
              "      <th>train_kph_sequence</th>\n",
              "      <th>dj_ac_state_sequence</th>\n",
              "      <th>dj_dc_state_sequence</th>\n",
              "      <th>incident_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>895</td>\n",
              "      <td>4604847</td>\n",
              "      <td>[537, 537, 537, 537, 537, 537, 537, 537, 537, ...</td>\n",
              "      <td>[3658, 4068, 3658, 4068, 3658, 4068, 3658, 406...</td>\n",
              "      <td>[-14388, -14257, -14222, -14127, -14090, -1381...</td>\n",
              "      <td>50.782892</td>\n",
              "      <td>4.421971</td>\n",
              "      <td>[0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>384</th>\n",
              "      <td>384</td>\n",
              "      <td>4451781</td>\n",
              "      <td>[609, 609, 609, 609, 609, 609, 609, 609, 609, ...</td>\n",
              "      <td>[4068, 3658, 4068, 3658, 4066, 3658, 4068, 365...</td>\n",
              "      <td>[-14137, -14078, -13904, -13892, -13519, -1258...</td>\n",
              "      <td>50.911038</td>\n",
              "      <td>4.151967</td>\n",
              "      <td>[0.2, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.2, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>690</th>\n",
              "      <td>690</td>\n",
              "      <td>4465347</td>\n",
              "      <td>[638, 638, 638, 638, 638, 638, 638, 638, 638, ...</td>\n",
              "      <td>[2742, 4002, 4110, 2708, 4026, 4148, 4140, 412...</td>\n",
              "      <td>[-8280, -8280, -8278, -8275, -8275, -8275, -82...</td>\n",
              "      <td>50.854381</td>\n",
              "      <td>2.737718</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>970</th>\n",
              "      <td>970</td>\n",
              "      <td>4610465</td>\n",
              "      <td>[529, 529, 529, 529, 529, 529, 529, 529, 529, ...</td>\n",
              "      <td>[4066, 4068, 4068, 3658, 4068, 3658, 4068, 365...</td>\n",
              "      <td>[-14271, -14121, -13619, -13600, -13028, -1292...</td>\n",
              "      <td>50.720086</td>\n",
              "      <td>4.397469</td>\n",
              "      <td>[1.6, 2.7, 3.2, 0.0, 0.1, 0.0, 1.8, 0.0, 1.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>389</th>\n",
              "      <td>389</td>\n",
              "      <td>4451923</td>\n",
              "      <td>[637, 637, 637, 637, 637, 637, 637, 637, 637, ...</td>\n",
              "      <td>[3636, 3658, 2956, 2956, 4066, 3636, 3658, 295...</td>\n",
              "      <td>[-14397, -14397, -14342, -14284, -14241, -1422...</td>\n",
              "      <td>50.805012</td>\n",
              "      <td>4.600712</td>\n",
              "      <td>[0.0, 0.0, 31.7, 25.0, 1.9, 0.0, 0.0, 22.0, 27...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-421907d7-933a-4268-bd91-43ce33602e4a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-421907d7-933a-4268-bd91-43ce33602e4a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-421907d7-933a-4268-bd91-43ce33602e4a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ff1541ee-f939-4c1a-ad82-5f92d3b451bd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ff1541ee-f939-4c1a-ad82-5f92d3b451bd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ff1541ee-f939-4c1a-ad82-5f92d3b451bd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 274,\n        \"min\": 384,\n        \"max\": 970,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          384,\n          389,\n          690\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 83080,\n        \"min\": 4451781,\n        \"max\": 4610465,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4451781,\n          4451923,\n          4465347\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vehicles_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 609, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 635, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662, 662]\",\n          \"[637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637, 637]\",\n          \"[638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638, 638]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"events_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[4068, 3658, 4068, 3658, 4066, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 4068, 4068, 4068, 4066, 4068, 4066, 3658, 4066, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4066, 4054, 4016, 4028, 4026, 4026, 4016, 4394, 4020, 4026, 3658, 4066, 3658, 4066, 3658, 4066, 4066, 3658, 4066, 4066, 3980, 4066, 4066, 4066, 4066, 4066, 4068, 4068, 4068, 4066, 4068, 4066, 4066, 4066, 4066, 4066, 4066, 4066, 3658, 4066, 4002, 2852, 4110, 2854, 4026, 4394, 952, 2708, 2744, 4148, 4026, 4030, 4018, 4140, 4152, 4168, 4156, 4130, 2742, 2658, 2684, 2688, 3234, 2942, 2742, 2946, 2742, 4406, 4408, 4412, 4410, 4148, 2852, 2854, 4120, 2858, 2708, 3254, 4180, 3254, 3254, 2970, 4082, 4092, 4090, 4084, 4094, 4090, 3234, 2658, 2684, 2688, 2974, 4100, 2882, 2882, 2886, 2682, 4120, 2686, 4120, 2956, 2956, 2956, 2956, 3982, 4048, 4066, 2736, 2708, 2708, 4020, 4026, 4028, 2708, 2744, 4026, 4148, 2740, 4396, 4030, 4020, 2972, 3236, 2976, 4100, 3636, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2684, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4068, 3636, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 4068, 3636, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 3982, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 4148, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2684, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 3980, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 3980, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 4120, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 4016, 4026, 4028, 4026, 4016, 4020, 4026, 4066, 4066, 3980, 4066, 3658, 4066, 4066, 4066, 3980, 4066, 4066, 4066, 4066, 4066, 4068, 1872, 4068, 3658, 3874, 3822, 3862, 4068, 3658, 4066, 3658, 4068, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 4066]\",\n          \"[3636, 3658, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 4066, 4124, 4124, 4126, 4124, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2684, 2684, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 3254, 4180, 4000, 4080, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 3236, 2742, 4026, 4148, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2682, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3224, 4396, 4066, 3636, 3658, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 4120, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4396, 148, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 2744, 4026, 4124, 4124, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3340, 4394, 3636, 3658, 3498, 3492, 4396, 3492, 4396, 4124, 3340, 4394, 3340, 4394, 2950, 4396, 2964, 4394, 942, 4180, 946, 960, 2950, 2964, 942, 960, 2950, 2708, 2744, 4124, 2950, 2964, 942, 960, 2708, 2744, 4168, 2708, 4140, 3986, 2744, 4004, 2852, 4110, 2854, 2708, 4026, 4140, 4140, 4148, 4140, 4152, 4168, 4156, 4406, 4410, 2740, 4408, 4412, 4124, 4030, 4018, 4026, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4396, 148, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4396, 148, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4124, 2956, 2956, 2956, 2682, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682]\",\n          \"[2742, 4002, 4110, 2708, 4026, 4148, 4140, 4128, 4152, 4026, 4030, 4018, 4168, 4156, 4070, 2402, 2844, 2742, 4148, 4406, 4408, 4410, 4412, 4070, 4066, 4068, 3632, 4120, 3254, 4180, 4120, 2858, 2658, 2688, 4120, 2708, 2708, 2852, 2854, 4120, 2858, 2658, 2688, 3254, 4180, 2708, 4168, 4140, 3234, 3986, 2742, 4004, 2852, 4110, 2854, 2708, 4026, 4148, 4140, 4152, 4168, 4156, 2740, 4068, 2852, 2854, 4120, 2858, 2658, 2688, 3620, 3254, 4180, 4030, 4018, 4026, 2708, 2942, 2742, 3254, 4180, 4168, 4140, 3986, 4002, 2852, 4110, 2854, 4026, 2708, 2742, 4148, 4140, 4152, 2740, 4168, 4156, 3254, 4180, 4068, 2958, 2942, 3254, 4180, 2942, 2958, 3254, 4180, 3254, 3254, 3254, 3254, 2980, 4030, 4018, 4026, 2970, 4082, 4092, 4090, 4084, 4094, 4090, 3234, 2974, 4100, 2744, 4026, 3254, 4180, 2958, 4168, 4140, 3986, 4004, 2852, 4110, 2854, 4026, 2708, 2744, 4026, 4026, 4030, 4018, 2552, 4168, 4140, 4140, 4148, 4140, 4152, 4168, 4156, 4406, 4410, 4408, 4412, 2942, 2958, 3254, 4180, 3254, 3254]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seconds_to_incident_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[-14137, -14078, -13904, -13892, -13519, -12583, -12293, -12282, -12162, -12152, -12022, -12007, -11884, -11622, -11459, -11274, -10882, -10231, -9893, -9639, -9395, -9362, -9144, -9040, -8780, -8743, -8583, -8563, -8278, -8261, -7960, -7755, -7551, -7327, -7120, -6890, -6730, -6485, -4077, -4076, -4032, -4031, -3930, -3929, -3928, -3920, -3920, -3161, -2957, -2939, -2797, -2775, -2600, -2401, -2368, -2166, -1965, -1809, -1743, -1432, -1170, -956, -641, 3, 333, 735, 1088, 1722, 1917, 2103, 2321, 2604, 2768, 2920, 3083, 3105, 3435, -7218, -7215, -7215, -7213, -7212, -7167, -7167, -7026, -7026, -7026, -7017, -7017, -7016, -7002, -7000, -6976, -6974, -6875, -6483, -6452, -6452, -6452, -6451, -6299, -6264, -6219, -6170, -6153, -6151, -6151, -6150, -6138, -6071, -6069, -6066, -6003, -5990, -5907, -5906, -5845, -5843, -5823, -5788, -5788, -5787, -5759, -5759, -5758, -5703, -5662, -5662, -5662, -5596, -5592, -5240, -5236, -5230, -4444, -4444, -4361, -4305, -4235, -4221, -4212, -4139, -4089, -4073, -4073, -4072, -4071, -4052, -4029, -4028, -4028, -3926, -3926, -3926, -3926, -3921, -3920, -3918, -3917, -3855, -3805, -3667, -3665, -3157, -3122, -3058, -3041, -3034, -3032, -3030, -3013, -2992, -2987, -2954, -2935, -2901, -2885, -2871, -2849, -2844, -2837, -2836, -2825, -2824, -2822, -2814, -2794, -2771, -2682, -2675, -2649, -2634, -2630, -2596, -2572, -2496, -2493, -2472, -2467, -2462, -2437, -2435, -2432, -2398, -2364, -2330, -2326, -2323, -2320, -2287, -2275, -2247, -2245, -2243, -2217, -2203, -2199, -2163, -2143, -2143, -2066, -2063, -2030, -2019, -2017, -1961, -1943, -1943, -1885, -1882, -1834, -1832, -1815, -1804, -1802, -1801, -1778, -1777, -1740, -1718, -1718, -1686, -1643, -1640, -1637, -1607, -1605, -1563, -1561, -1527, -1519, -1483, -1468, -1465, -1459, -1428, -1412, -1412, -1359, -1341, -1329, -1326, -1319, -1309, -1302, -1284, -1270, -1248, -1234, -1207, -1203, -1181, -1175, -1167, -1119, -1119, -1080, -1077, -1064, -1056, -1026, -1024, -1014, -1012, -993, -988, -964, -952, -938, -938, -886, -880, -862, -860, -858, -843, -841, -838, -836, -806, -802, -796, -787, -783, -746, -745, -743, -721, -715, -699, -690, -650, -637, -516, -516, -497, -317, -284, -230, -228, -210, -208, -206, -185, -180, -151, -143, -132, -71, -50, 2, 6, 57, 57, 336, 346, 491, 513, 541, 567, 571, 574, 598, 603, 631, 635, 639, 683, 686, 709, 738, 779, 809, 829, 884, 901, 931, 939, 952, 981, 992, 995, 1005, 1015, 1017, 1023, 1026, 1028, 1053, 1057, 1091, 1107, 1141, 1148, 1169, 1171, 1178, 1183, 1188, 1193, 1217, 1222, 1230, 1231, 1233, 1236, 1243, 1255, 1266, 1272, 1289, 1305, 1312, 1325, 1346, 1350, 1366, 1387, 1388, 1408, 1426, 1429, 1469, 1471, 1480, 1481, 1511, 1516, 1518, 1528, 1530, 1537, 1551, 1560, 1567, 1574, 1583, 1587, 1612, 1626, 1641, 1690, 1713, 1721, 1726, 1757, 1811, 1813, 1814, 1865, 1867, 1885, 1920, 1947, 1985, 2002, 2048, 2061, 2063, 2106, 2142, 2232, 2272, 2287, 2288, 2325, 2348, 2442, 2485, 2514, 2529, 2542, 2574, 2579, 2607, 2630, 2667, 2673, 2710, 2722, 2742, 2771, 2788, 2831, 2853, 2924, 2945, 2988, 3006, 3009, 3086, 3109, 3202, 3205, 3257, 3259, 3265, 3271, 3299, 3322, 3365, 3398, 3406, 3438, 3526, -14389, -14370, -14335, -14326, -14284, -14240, -14202, -14193, -14135, -14076, -14011, -13983, -13972, -13946, -13935, -13934, -13902, -13890, -13798, -13786, -13747, -13701, -13658, -13643, -13636, -13629, -13616, -13597, -13555, -13552, -13518, -13516, -12581, -12542, -12526, -12520, -12469, -12442, -12408, -12400, -12394, -12392, -12350, -12341, -12339, -12292, -12280, -12217, -12205, -12203, -12161, -12151, -12079, -12065, -12054, -12020, -12005, -11989, -11961, -11945, -11913, -11910, -11896, -11883, -11858, -11858, -11844, -11814, -11778, -11767, -11753, -11728, -11689, -11655, -11644, -11643, -11620, -11602, -11602, -11583, -11525, -11516, -11457, -11434, -11434, -11415, -11368, -11367, -11323, -11313, -11301, -11273, -11254, -11254, -11240, -11182, -11016, -10979, -10976, -10973, -10925, -10922, -10907, -10905, -10881, -10861, -10861, -10816, -10816, -10790, -10743, -10731, -10729, -10717, -10692, -10690, -10689, -10680, -10673, -10668, -10667, -10658, -10646, -10643, -10634, -10622, -10621, -10611, -10580, -10578, -10570, -10568, -10532, -10530, -10512, -10492, -10490, -10470, -10453, -10448, -10426, -10413, -10406, -10390, -10372, -10366, -10359, -10348, -10337, -10335, -10332, -10331, -10323, -10318, -10293, -10289, -10283, -10278, -10271, -10255, -10252, -10230, -10217, -10217, -10171, -10167, -10141, -10140, -10136, -10130, -10128, -10116, -10104, -10100, -10084, -10049, -10036, -10030, -10010, -10003, -9929, -9891, -9638, -9595, -9560, -9556, -9528, -9526, -9523, -9502, -9500, -9496, -9483, -9478, -9474, -9458, -9454, -9449, -9431, -9427, -9394, -9361, -9337, -9316, -9311, -9289, -9273, -9270, -9254, -9251, -9246, -9234, -9229, -9176, -9169, -9143, -9038, -8988, -8979, -8967, -8952, -8943, -8942, -8904, -8903, -8891, -8886, -8884, -8850, -8848, -8846, -8841, -8829, -8826, -8823, -8810, -8808, -8804, -8779, -8741, -8689, -8683, -8656, -8654, -8644, -8641, -8619, -8612, -8610, -8582, -8562, -8444, -8442, -8415, -8407, -8405, -8384, -8372, -8355, -8349, -8340, -8332, -8329, -8319, -8307, -8276, -8259, -8239, -8211, -8205, -8199, -8144, -8136, -8101, -8099, -8056, -8054, -8033, -8021, -8019, -7989, -7968, -7959, -7940, -7940, -7904, -7900, -7875, -7863, -7848, -7846, -7844, -7816, -7805, -7802, -7753, -7720, -7720, -7650, -7648, -7616, -7605, -7603, -7550, -7509, -7509, -7464, -7459, -7433, -7413, -7411, -7384, -7378, -7375, -7354, -7353, -7347, -7331, -7326, -7301, -7301, -7241, -7240, -7238, -7214, -7203, -7200, -7183, -7181, -7180, -7160, -7149, -7146, -7141, -7119, -7057, -7057, -6963, -6957, -6940, -6935, -6930, -6924, -6922, -6921, -6889, -6875, -6875, -6830, -6823, -6813, -6801, -6794, -6783, -6778, -6766, -6757, -6728, -6705, -6694, -6694, -6640, -6609, -6590, -6587, -6584, -6558, -6523, -6484, -6471, -4075, -4030, -4030, -3928, -3927, -3919, -3919, -2956, -2796, -2650, -2599, -2574, -2400, -2165, -1963, -1806, -1742, -1430, -1169, -955, -640, 4, 67, 334, 344, 427, 621, 659, 736, 776, 1089, 1105, 1724, 1755, 1918, 1944, 2104, 2139, 2323, 2346, 2605, 2628, 2769, 2785, 2922, 2943, 3084, 3436]\",\n          \"[-14397, -14397, -14342, -14284, -14241, -14225, -14225, -14172, -14110, -14062, -14055, -14051, -14036, -14036, -13973, -13936, -13902, -13887, -13871, -13871, -13812, -13772, -13765, -13751, -13744, -13714, -13697, -13655, -13641, -13641, -13599, -13594, -13543, -13530, -13526, -13510, -13409, -13409, -13319, -13296, -13278, -13276, -13275, -13111, -13101, -13101, -13047, -13024, -13010, -13007, -12959, -12942, -12938, -12916, -12888, -12882, -12872, -12820, -12820, -12753, -12747, -12703, -12695, -12692, -12684, -12661, -12652, -12632, -12606, -12442, -12442, -12419, -12408, -12406, -12387, -12344, -12342, -12313, -12281, -12271, -12195, -12144, -12144, -12078, -12074, -12071, -12069, -12069, -12055, -12051, -12031, -11893, -11887, -11864, -11848, -11816, -11799, -11788, -11775, -11775, -11732, -11717, -11693, -11647, -11636, -11606, -11592, -11574, -11523, -11519, -11506, -11484, -11474, -11448, -11367, -11367, -11210, -11208, -11166, -11151, -11146, -11084, -11061, -11034, -10943, -10926, -10903, -10653, -10653, -10653, -10180, -10180, -10174, -10031, -10009, -9958, -9954, -9938, -9892, -9889, -9797, -9786, -9740, -9730, -9730, -9662, -9635, -9630, -9621, -9588, -9573, -9545, -9529, -9506, -9489, -9468, -9400, -9400, -9335, -9309, -9294, -9281, -9278, -9231, -9219, -9216, -9212, -9172, -9137, -9137, -9121, -9069, -9028, -9019, -8981, -8971, -8920, -8899, -8899, -8829, -8818, -8812, -8793, -8786, -8784, -8777, -8741, -8736, -8725, -8711, -8562, -8562, -8513, -8483, -8468, -8451, -8439, -8405, -8404, -8392, -8391, -8359, -8357, -8330, -8247, -8247, -8208, -8115, -8057, -8057, -8011, -8006, -7948, -7927, -7922, -7899, -7869, -7869, -7783, -7763, -7731, -7713, -7710, -7694, -7662, -7630, -7623, -7607, -7607, -7563, -7539, -7503, -7475, -7466, -7448, -7448, -7381, -7334, -7306, -7287, -7287, -7135, -7083, -7054, -7036, -7036, -6985, -6972, -6966, -6953, -6931, -6911, -6890, -6883, -6865, -6856, -6852, -6820, -6814, -6793, -6776, -6776, -6734, -6726, -6681, -6671, -6666, -6654, -6651, -6633, -6617, -6617, -6540, -6524, -6513, -6511, -6495, -6474, -6474, -6441, -6385, -6378, -6373, -6371, -6354, -6343, -6322, -6314, -6299, -6289, -6265, -6260, -6251, -6248, -6239, -6230, -6228, -6144, -6114, -6107, -6091, -6091, -6048, -6018, -6011, -5998, -5958, -5948, -5935, -5918, -5918, -5865, -5848, -5806, -5804, -5784, -5770, -5757, -5751, -5737, -5726, -5709, -5697, -5676, -5666, -5651, -5641, -5624, -5624, -5607, -5594, -5594, -5538, -5490, -5478, -5472, -5448, -5441, -5432, -5400, -5393, -5391, -5308, -5279, -5207, -5190, -5187, -5177, -5154, -5138, -5123, -5107, -5107, -5043, -5034, -5007, -4994, -4983, -4976, -4961, -4957, -4948, -4944, -4939, -4917, -4903, -4900, -4884, -4840, -4834, -4831, -4813, -4645, -4524, -4524, -4462, -4454, -4441, -4391, -4377, -4372, -4358, -4353, -4337, -4326, -4317, -4315, -4304, -4292, -4291, -4284, -4284, -4279, -4276, -4274, -4187, -4185, -4151, -4143, -4121, -4111, -4107, -4095, -4060, -3990, -3990, -3902, -3891, -3882, -3853, -3851, -3835, -3785, -3744, -3735, -3720, -3702, -3675, -3503, -3503, -3422, -3399, -3387, -3364, -3331, -3285, -3263, -3232, -3219, -3192, -3175, -3156, -3129, -3129, -3027, -2994, -2991, -2965, -2951, -2879, -2860, -2860, -2776, -2685, -2652, -2638, -2576, -2490, -2478, -2354, -2354, -1242, -1238, -989, -989, -814, -797, -780, -768, -737, -734, -679, -663, -636, -636, -522, -512, -418, -398, -398, -361, -329, -316, -299, -288, -266, -251, -217, -194, -193, -187, -171, -155, -91, -16, 2, 3, 21, 21, 39, 40, 40, 43, 44, 65, 120, 121, 188, 189, 218, 218, 219, 219, 220, 221, 221, 221, 377, 378, 379, 381, 470, 488, 494, 511, 548, 549, 550, 551, 650, 682, 837, 842, 854, 957, 1353, 1353, 1355, 1355, 1357, 1358, 1358, 1402, 1414, 1418, 1419, 1421, 1429, 1432, 1445, 1447, 1449, 1456, 1458, 1488, 1516, 1517, 1517, 1559, 1655, 1683, 1691, 1748, 1863, 2038, 2048, 2165, 2217, 2252, 2260, 2274, 2298, 2317, 2333, 2333, 2381, 2399, 2414, 2439, 2440, 2467, 2467, 2565, 2568, 2570, 2582, 2594, 2604, 2606, 2615, 2628, 2631, 2631, 2649, 2652, 2654, 2673, 2676, 2778, 2784, 2787, 2813, 3049, 3049, 3098, 3124, 3130, 3142, 3143, 3162, 3205, 3223, 3227, 3243, 3289, 3301, 3308, 3320, 3324, 3345, 3350, 3363, 3365, 3376, 3402, 3406, 3418, 3440, 3454, 3454, 3468, 3485, 3504, 3528, 3529, 3539, 3559, 3566, 3573, 3576]\",\n          \"[-8280, -8280, -8278, -8275, -8275, -8275, -8251, -8250, -8249, -8246, -8246, -8245, -8232, -8230, -8223, -8189, -8170, -2318, -2318, -2302, -2302, -2302, -2300, -2296, -2258, -2258, -2257, -2253, -2180, -2180, -2027, -1957, -1956, -1956, -1871, -1815, -1574, -1556, -1554, -1501, -1436, -1435, -1435, -1396, -1395, -1303, -1106, -1092, -1080, -1063, -855, -855, -853, -853, -851, -850, -850, -850, -806, -805, -792, -789, -776, -775, -743, -742, -734, -671, -670, -670, -670, -626, -626, -372, -371, -371, -140, -116, -48, 25, 25, 139, 152, 204, 383, 385, 385, 387, 389, 414, 414, 464, 481, 483, 511, 521, 523, 578, 578, 700, 800, 801, 1088, 1088, 1139, 1139, 1429, 1429, 1491, 1492, 1521, 1522, 1585, 1657, 1658, 1658, 1666, 1725, 1725, 1726, 1753, 1753, 1754, 1810, 1919, 1925, 2020, 2020, 2196, 2196, 2474, 2785, 2792, 2815, 3019, 3021, 3021, 3023, 3025, 3059, 3059, 3059, 3108, 3108, 3109, 3166, 3166, 3168, 3178, 3186, 3187, 3188, 3197, 3199, 3212, 3213, 3222, 3225, 3320, 3320, 3505, 3505, 3538, 3539]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07230675986104353,\n        \"min\": 50.72008567460733,\n        \"max\": 50.911038159911406,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          50.911038159911406,\n          50.80501206746032,\n          50.85438073536585\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.757322012560567,\n        \"min\": 2.737717975609756,\n        \"max\": 4.600712016349206,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4.1519667506090805,\n          4.600712016349206,\n          2.737717975609756\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_kph_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[0.2, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.2, 0.0, 0.2, 0.0, 0.3, 0.2, 0.2, 0.0, 0.0, 0.7, 0.1, 0.0, 0.2, 0.0, 0.1, 0.0, 0.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.2, 0.2, 0.2, 0.2, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.1, 0.0, 0.1, 0.1, 0.0, 0.0, 0.1, 84.1, 0.2, 0.2, 0.1, 0.2, 0.0, 0.1, 0.0, 0.1, 0.2, 0.1, 0.1, 0.1, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 10.1, 16.1, 17.2, 14.5, 0.0, 0.6, 0.4, 0.3, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 39.1, 37.2, 36.5, 36.4, 36.2, 48.0, 58.4, 50.6, 0.1, 0.0, 24.9, 63.1, 86.4, 110.4, 105.3, 98.3, 97.1, 80.9, 78.5, 74.6, 53.4, 0.0, 0.0, 91.7, 98.6, 84.3, 62.0, 57.2, 0.1, 0.0, 94.7, 96.7, 114.3, 112.3, 110.6, 58.5, 54.8, 50.7, 0.0, 0.0, 11.9, 20.0, 24.5, 30.9, 88.0, 99.6, 114.4, 113.9, 112.7, 93.8, 62.9, 58.5, 0.0, 0.0, 0.0, 79.4, 78.8, 104.1, 97.5, 94.5, 0.0, 0.0, 0.0, 62.4, 66.5, 100.6, 98.7, 88.8, 83.8, 82.6, 81.9, 53.7, 51.8, 0.0, 0.0, 0.0, 16.1, 91.0, 90.1, 89.6, 97.3, 97.0, 99.4, 98.9, 94.2, 93.4, 78.2, 68.1, 64.3, 56.5, 0.1, 0.0, 0.0, 46.3, 85.9, 105.1, 107.3, 109.7, 113.3, 113.8, 85.7, 80.8, 83.0, 73.0, 56.2, 51.7, 23.1, 15.7, 0.1, 0.0, 0.0, 0.0, 0.0, 15.2, 30.1, 77.8, 77.9, 76.6, 76.7, 56.7, 48.5, 18.1, 0.1, 0.0, 0.0, 19.4, 34.9, 53.2, 53.8, 55.0, 57.5, 57.8, 57.4, 57.4, 55.6, 55.7, 55.6, 56.0, 54.7, 47.9, 47.6, 46.8, 37.0, 34.4, 27.9, 27.0, 11.6, 0.0, 0.0, 0.0, 0.0, 0.0, 17.0, 36.0, 36.5, 37.2, 37.3, 37.4, 36.4, 36.2, 19.8, 16.3, 13.0, 3.9, 19.6, 6.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.5, 36.5, 35.5, 34.4, 37.3, 34.6, 34.0, 29.8, 29.3, 28.6, 28.3, 27.5, 18.1, 0.0, 0.0, 0.0, 23.2, 36.2, 40.4, 40.5, 50.5, 55.0, 89.2, 95.0, 98.0, 104.0, 96.9, 96.6, 91.8, 88.3, 86.8, 57.5, 50.0, 0.0, 0.0, 17.6, 34.3, 77.8, 82.3, 93.2, 101.0, 107.0, 112.3, 137.3, 141.9, 148.2, 149.2, 150.7, 152.5, 155.4, 156.7, 157.4, 155.9, 155.9, 155.7, 155.6, 157.1, 156.1, 157.4, 156.8, 155.9, 155.8, 155.9, 150.9, 147.3, 117.9, 118.3, 118.8, 118.8, 118.3, 112.8, 109.7, 93.7, 91.8, 85.6, 82.2, 83.5, 83.6, 84.1, 84.1, 83.4, 85.2, 79.2, 76.6, 56.4, 25.2, 7.0, 0.1, 0.0, 58.8, 58.6, 59.0, 60.0, 60.0, 58.9, 0.1, 0.0, 27.3, 69.4, 84.2, 72.2, 71.5, 0.1, 0.0, 95.8, 79.1, 64.1, 62.7, 0.0, 0.0, 105.5, 104.3, 90.7, 84.5, 82.2, 52.0, 42.3, 0.0, 0.0, 30.1, 47.0, 97.4, 82.4, 60.9, 0.1, 0.0, 21.3, 65.4, 0.0, 0.0, 42.7, 75.8, 79.5, 0.1, 0.0, 81.4, 85.1, 110.7, 107.5, 101.6, 93.6, 59.1, 35.5, 27.5, 25.5, 24.5, 0.0, 0.0, 39.0, 70.8, 118.3, 116.7, 115.4, 117.4, 114.8, 115.0, 0.1, 0.0, 38.9, 83.9, 97.6, 106.3, 84.4, 80.7, 0.0, 0.0, 92.3, 104.6, 113.3, 109.4, 113.3, 105.2, 99.2, 93.3, 78.2, 38.4, 36.0, 32.2, 0.0, 0.0, 0.0, 0.0, 15.2, 24.5, 38.9, 41.7, 82.8, 91.2, 99.3, 102.4, 117.9, 119.4, 119.0, 0.1, 0.0, 76.7, 90.4, 91.0, 0.2, 0.0, 65.6, 86.7, 97.2, 0.2, 0.0, 0.0, 48.3, 79.9, 83.5, 72.1, 35.9, 0.2, 0.0, 0.0, 0.0, 16.6, 86.0, 97.5, 97.6, 117.8, 115.0, 110.0, 76.8, 73.8, 0.1, 0.0, 0.0, 0.0, 106.7, 117.3, 0.1, 0.0, 0.0, 0.0, 61.7, 65.1, 100.6, 86.0, 58.7, 0.0, 0.0, 0.0, 0.0, 29.7, 0.0, 8.5, 15.8, 25.7, 71.6, 70.9, 61.0, 56.6, 0.0, 0.0, 0.0, 0.0, 0.0, 36.1, 82.9, 88.9, 88.8, 87.6, 86.3, 86.5, 86.8, 86.7, 85.6, 85.0, 85.0, 86.0, 85.8, 86.2, 86.9, 86.2, 86.2, 90.2, 115.7, 117.1, 122.5, 125.4, 153.0, 153.2, 155.1, 153.7, 153.5, 152.5, 146.5, 144.4, 152.4, 153.5, 153.7, 154.3, 154.3, 154.5, 154.3, 155.2, 154.0, 153.5, 150.7, 149.3, 142.9, 136.7, 109.1, 105.2, 104.9, 105.2, 95.6, 60.6, 55.1, 0.2, 0.0, 0.0, 36.3, 45.2, 84.5, 84.2, 84.1, 84.5, 84.5, 83.9, 82.0, 75.5, 58.4, 56.8, 57.9, 58.0, 39.9, 36.4, 34.7, 0.1, 0.0, 0.0, 21.7, 27.0, 37.0, 36.9, 37.0, 39.8, 39.9, 39.8, 43.0, 46.4, 46.4, 41.0, 41.2, 41.6, 31.5, 36.8, 0.2, 0.0, 0.0, 31.1, 39.6, 47.8, 48.3, 48.4, 47.9, 46.0, 41.2, 36.9, 35.1, 39.0, 38.7, 0.0, 0.0, 21.5, 38.0, 38.9, 47.1, 51.9, 52.2, 56.8, 56.3, 55.3, 55.0, 55.0, 51.5, 52.4, 51.8, 51.9, 56.9, 58.7, 61.7, 63.0, 61.9, 55.0, 0.1, 0.0, 16.1, 26.3, 72.7, 74.2, 78.3, 77.8, 65.2, 49.8, 44.7, 0.0, 0.0, 43.9, 49.3, 84.9, 83.6, 83.3, 85.9, 99.4, 115.9, 114.9, 113.2, 111.9, 112.4, 107.3, 88.8, 0.0, 0.0, 0.0, 22.7, 32.7, 43.4, 96.4, 94.7, 90.0, 90.2, 95.0, 94.3, 91.7, 89.7, 89.5, 70.5, 15.8, 0.0, 0.0, 0.0, 20.4, 30.1, 84.7, 102.9, 109.5, 109.9, 110.0, 99.8, 88.0, 83.6, 0.0, 0.0, 0.0, 88.9, 90.1, 101.8, 94.6, 91.3, 0.0, 0.0, 0.0, 39.5, 50.5, 93.6, 111.5, 111.4, 116.1, 116.0, 114.6, 84.6, 81.4, 62.1, 15.3, 0.2, 0.0, 0.0, 42.8, 46.4, 51.1, 93.5, 109.9, 112.2, 112.5, 112.0, 111.9, 107.8, 84.8, 72.7, 51.1, 0.2, 0.0, 0.0, 101.4, 107.7, 109.6, 107.7, 98.5, 81.8, 77.2, 72.5, 0.1, 0.0, 0.0, 19.5, 38.8, 60.6, 80.4, 90.2, 96.2, 94.7, 88.7, 76.2, 0.2, 0.0, 0.0, 0.0, 33.0, 36.5, 30.1, 29.9, 30.5, 34.4, 31.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.1, 83.3, 0.1, 0.0, 0.0, 0.1, 0.1, 83.5, 0.1, 0.2, 0.2, 0.2, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 31.0, 26.4, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0]\",\n          \"[0.0, 0.0, 31.7, 25.0, 1.9, 0.0, 0.0, 22.0, 27.0, 17.1, 10.4, 0.8, 0.0, 0.0, 33.4, 54.7, 25.5, 0.5, 0.0, 0.0, 50.0, 51.0, 48.6, 51.2, 53.1, 55.9, 65.8, 0.4, 0.0, 0.0, 40.8, 52.1, 84.4, 67.2, 57.9, 0.9, 0.0, 0.0, 39.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 44.6, 72.0, 80.3, 80.5, 71.6, 57.1, 54.8, 54.1, 57.0, 40.1, 0.2, 0.0, 0.0, 41.9, 51.3, 93.8, 99.1, 100.1, 105.8, 114.8, 109.5, 81.9, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 17.2, 79.5, 81.0, 99.8, 102.3, 101.1, 0.3, 0.0, 0.0, 81.5, 87.6, 91.0, 93.6, 93.8, 9.9, 0.0, 0.0, 37.7, 42.1, 62.5, 54.6, 42.3, 29.6, 1.7, 0.0, 0.0, 25.1, 57.5, 56.3, 88.3, 96.3, 84.0, 65.0, 52.4, 53.7, 57.5, 55.7, 40.1, 29.7, 0.1, 0.0, 0.0, 83.6, 84.1, 63.9, 48.0, 43.1, 34.4, 35.5, 36.7, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.4, 35.2, 52.0, 50.6, 60.0, 77.6, 78.0, 90.3, 87.5, 2.2, 0.0, 0.0, 91.4, 115.9, 116.5, 107.5, 64.8, 70.9, 72.5, 62.4, 50.2, 30.2, 3.0, 0.0, 0.0, 46.6, 62.9, 83.4, 96.2, 98.0, 91.4, 83.7, 81.0, 76.6, 0.1, 0.0, 0.0, 0.0, 67.6, 116.2, 115.0, 115.0, 95.5, 0.0, 0.0, 0.0, 86.1, 96.6, 100.6, 113.0, 115.0, 114.6, 108.8, 76.8, 72.3, 47.5, 0.1, 0.0, 0.0, 12.0, 64.7, 77.4, 87.3, 88.9, 78.0, 78.6, 83.4, 83.2, 62.3, 59.7, 0.5, 0.0, 0.0, 11.7, 0.2, 0.0, 0.0, 30.9, 43.4, 66.0, 43.3, 40.9, 0.9, 0.0, 0.0, 54.0, 53.1, 45.2, 52.7, 53.9, 54.0, 42.5, 16.0, 0.7, 0.0, 0.0, 38.4, 51.1, 53.2, 27.8, 1.8, 0.0, 0.0, 32.6, 30.3, 0.2, 0.0, 0.0, 22.9, 27.3, 0.1, 0.0, 0.0, 37.6, 39.2, 37.9, 33.9, 20.5, 35.4, 44.0, 42.1, 36.8, 32.9, 33.4, 42.3, 42.1, 0.3, 0.0, 0.0, 19.9, 41.0, 75.0, 63.4, 56.0, 48.4, 44.4, 3.6, 0.0, 0.0, 69.8, 53.0, 32.7, 30.8, 0.9, 0.0, 0.0, 0.0, 12.4, 20.9, 34.6, 41.8, 67.6, 69.0, 60.3, 49.2, 40.4, 40.7, 37.4, 35.6, 34.2, 35.3, 42.6, 50.8, 50.2, 44.3, 22.2, 2.7, 0.0, 0.0, 41.8, 90.0, 94.9, 70.7, 36.0, 25.3, 0.2, 0.0, 0.0, 24.1, 37.3, 56.4, 55.2, 61.7, 75.0, 84.2, 89.8, 93.9, 91.0, 86.2, 94.4, 108.6, 109.4, 109.9, 98.2, 41.3, 40.9, 0.9, 0.0, 0.0, 37.7, 111.2, 122.5, 123.4, 145.6, 151.9, 155.3, 146.7, 144.6, 146.0, 24.5, 40.9, 46.8, 67.9, 67.5, 68.8, 69.6, 54.6, 0.7, 0.0, 0.0, 61.2, 73.3, 99.1, 98.7, 89.9, 89.3, 86.2, 85.6, 84.6, 84.7, 84.5, 79.3, 76.1, 73.2, 56.5, 38.8, 38.0, 37.7, 0.7, 0.0, 0.0, 0.0, 13.4, 14.7, 23.5, 80.8, 72.7, 74.6, 80.4, 81.9, 80.1, 87.6, 96.4, 99.7, 112.9, 126.8, 127.8, 130.7, 130.7, 128.8, 127.7, 127.5, 128.4, 128.0, 134.8, 134.8, 126.6, 109.8, 103.2, 78.2, 0.0, 0.0, 0.0, 97.1, 110.4, 119.4, 124.3, 124.0, 122.7, 129.9, 121.7, 105.4, 81.8, 54.9, 0.1, 0.0, 0.0, 79.2, 84.3, 83.4, 85.6, 85.3, 88.9, 85.9, 83.6, 82.9, 76.4, 48.2, 0.0, 0.0, 0.0, 77.9, 78.7, 78.5, 77.1, 73.8, 0.0, 0.0, 0.0, 76.1, 35.1, 29.5, 28.3, 29.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 51.0, 72.8, 89.3, 96.9, 73.5, 70.3, 28.6, 0.0, 0.0, 0.0, 111.9, 110.7, 0.0, 0.0, 0.0, 13.4, 51.6, 66.2, 94.8, 107.3, 126.9, 123.3, 115.9, 106.5, 104.7, 99.6, 79.0, 62.3, 31.9, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.7, 19.0, 20.1, 21.0, 17.8, 17.4, 19.8, 18.8, 75.0, 124.7, 135.0, 124.4, 87.6, 43.2, 0.0, 0.0, 0.0, 55.9, 86.0, 108.2, 128.5, 129.5, 135.3, 135.3, 133.1, 133.4, 133.5, 135.3, 125.6, 103.8, 101.6, 91.1, 83.2, 79.8, 80.2, 62.9, 59.3, 55.7, 47.2, 47.4, 36.6, 32.9, 30.4, 0.1, 0.0, 0.0, 0.0, 16.6, 17.0, 17.4, 17.5, 27.2, 38.7, 67.6, 67.6, 52.6, 36.6, 38.8, 49.4, 70.7, 75.1, 82.2, 86.5, 83.4, 83.0, 95.1, 90.8, 85.3, 61.8, 0.1, 0.0, 0.0, 0.0, 25.3, 80.2, 103.5, 103.8, 116.5, 139.7, 142.9, 140.1, 138.9]\",\n          \"[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_ac_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_dc_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\",\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 2,\n        \"max\": 13,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2,\n          9,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events_list = []\n",
        "events_pre_incident = []\n",
        "events_post_incident = []\n",
        "\n",
        "for i, (events, seconds_to_incident_sequence, vehicles_sequence, train_kph_sequence, dj_ac_state_sequence, dj_dc_state_sequence) in tqdm(enumerate(zip(df[\"events_sequence\"],\n",
        "                                                                                                                                                      df[\"seconds_to_incident_sequence\"],\n",
        "                                                                                                                                                      df[\"vehicles_sequence\"],\n",
        "                                                                                                                                                      df[\"train_kph_sequence\"],\n",
        "                                                                                                                                                      df[\"dj_ac_state_sequence\"],\n",
        "                                                                                                                                                      df[\"dj_dc_state_sequence\"])), total=len(df)):\n",
        "    events = ast.literal_eval(events)\n",
        "    seconds_to_incident_sequence = ast.literal_eval(seconds_to_incident_sequence)\n",
        "    vehicles_sequence = ast.literal_eval(vehicles_sequence)\n",
        "    train_kph_sequence = ast.literal_eval(train_kph_sequence)\n",
        "    dj_ac_state_sequence = ast.literal_eval(dj_ac_state_sequence)\n",
        "    dj_dc_state_sequence = ast.literal_eval(dj_dc_state_sequence)\n",
        "\n",
        "\n",
        "    pre_incidents = []\n",
        "    pre_incidents_vehicles = []\n",
        "    pre_incidents_kph = []\n",
        "    pre_incidents_ac = []\n",
        "    pre_incidents_dc = []\n",
        "\n",
        "\n",
        "    post_incidents = []\n",
        "    post_incidents_vehicles = []\n",
        "    post_incidents_kph = []\n",
        "    post_incidents_ac = []\n",
        "    post_incidents_dc = []\n",
        "\n",
        "    event_seq = []\n",
        "    prev_event = 0\n",
        "\n",
        "    for event, time_to_incident, vehicle, kph, ac, dc in zip(events, seconds_to_incident_sequence, vehicles_sequence, train_kph_sequence, dj_ac_state_sequence, dj_dc_state_sequence):\n",
        "      #if event != prev_event:\n",
        "      event_seq.append(str(event))\n",
        "      if time_to_incident <= 0:\n",
        "          pre_incidents.append(str(event))\n",
        "          pre_incidents_vehicles.append(str(vehicle))\n",
        "          pre_incidents_kph.append(str(kph))\n",
        "          pre_incidents_ac.append(str(ac))\n",
        "          pre_incidents_dc.append(str(dc))\n",
        "      else:\n",
        "          post_incidents.append(str(event))\n",
        "          post_incidents_vehicles.append(str(vehicle))\n",
        "          post_incidents_kph.append(str(kph))\n",
        "          post_incidents_ac.append(str(ac))\n",
        "          post_incidents_dc.append(str(dc))\n",
        "      #prev_event = event\n",
        "\n",
        "    # Append the pre and post incident lists to the main lists\n",
        "    events_pre_incident.append(pre_incidents)\n",
        "    events_post_incident.append(post_incidents)\n",
        "    events_list.append(event_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs6FuHq84zef",
        "outputId": "4a43108b-c44f-41a1-d63a-b57faf758154"
      },
      "id": "Rs6FuHq84zef",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1011/1011 [00:15<00:00, 64.88it/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from itertools import combinations\n",
        "tuples_post = []\n",
        "tuples_pre = []\n",
        "\n",
        "for post_events in tqdm(events_post_incident):\n",
        "  prev_post_event = 0\n",
        "  for post_event in post_events:\n",
        "    if prev_post_event != 0:\n",
        "      tup = [prev_post_event, post_event]\n",
        "      prev_post_event = post_event\n",
        "      if tup not in tuples_post:\n",
        "        tuples_post.append(tup)\n",
        "    else:\n",
        "      prev_post_event = post_event\n",
        "\n",
        "for pre_events in tqdm(events_pre_incident):\n",
        "  prev_pre_event = 0\n",
        "  for pre_event in pre_events:\n",
        "    if prev_pre_event != 0:\n",
        "      tup = [prev_pre_event, pre_event]\n",
        "      prev_pre_event = pre_event\n",
        "      if tup not in tuples_pre:\n",
        "        tuples_pre.append(tup)\n",
        "    else:\n",
        "      prev_pre_event = pre_event"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOwIIe9Yi4VZ",
        "outputId": "bc7a36bf-6b46-4e3b-8d01-eac3b180621a"
      },
      "id": "oOwIIe9Yi4VZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1011/1011 [00:08<00:00, 123.91it/s]\n",
            "100%|| 1011/1011 [00:10<00:00, 97.33it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for post in tuples_post:\n",
        "  if post in tuples_pre:\n",
        "    c += 1\n",
        "c/len(tuples_post)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkg-WzIgoMcg",
        "outputId": "48063f99-b2cf-420d-983e-22676db0471b"
      },
      "id": "vkg-WzIgoMcg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4953556864521976"
            ]
          },
          "metadata": {},
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "c = 0\n",
        "for pre in tuples_pre:\n",
        "  if pre in tuples_post:\n",
        "    c += 1\n",
        "c/len(tuples_pre)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7kfduUHpOgW",
        "outputId": "d915bf94-6cdd-47e6-d272-86b4e472d0d9"
      },
      "id": "t7kfduUHpOgW",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4728078711212023"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_events_pre_incident = []\n",
        "tuples_removed = 0\n",
        "\n",
        "for prev_events in tqdm(events_pre_incident):\n",
        "  cleaned_events = []\n",
        "  prev_pre_event = 0\n",
        "  for pre_event in prev_events:\n",
        "    if prev_pre_event != 0:\n",
        "      tup = [prev_pre_event, pre_event]\n",
        "      if tup not in tuples_post:\n",
        "        cleaned_events.append(prev_pre_event)\n",
        "      else:\n",
        "        tuples_removed += 1\n",
        "    else:\n",
        "      prev_pre_event = pre_event\n",
        "  cleaned_events.append(pre_event)\n",
        "  cleaned_events_pre_incident.append(cleaned_events)\n",
        "cleaned_events_pre_incident[0]"
      ],
      "metadata": {
        "id": "Q_tE3kjAqKQj",
        "outputId": "aa5a0981-ffc9-4c4f-d11e-26e768e6c7a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Q_tE3kjAqKQj",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1011/1011 [01:07<00:00, 14.95it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "cleaned_events_pre_incident = []\n",
        "tuples_removed = 0\n",
        "\n",
        "for prev_events in tqdm(events_pre_incident):\n",
        "    cleaned_events = []\n",
        "    prev_pre_event = None  # Usa None para identificar el primer elemento\n",
        "\n",
        "    for pre_event in prev_events:\n",
        "        if prev_pre_event is not None:\n",
        "            tup = [prev_pre_event, pre_event]\n",
        "\n",
        "            if tup not in tuples_post:\n",
        "                cleaned_events.append(prev_pre_event)\n",
        "            else:\n",
        "                tuples_removed += 1\n",
        "\n",
        "        prev_pre_event = pre_event\n",
        "\n",
        "    if prev_pre_event is not None:\n",
        "        cleaned_events.append(prev_pre_event)\n",
        "    cleaned_events_pre_incident.append(cleaned_events)\n",
        "\n",
        "cleaned_events_pre_incident[0]\n"
      ],
      "metadata": {
        "id": "k5rmXT4TzJmN",
        "outputId": "18052d69-92fb-4ad3-845c-66237f4b36c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "k5rmXT4TzJmN",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 1011/1011 [00:16<00:00, 60.09it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1132',\n",
              " '4026',\n",
              " '1082',\n",
              " '2742',\n",
              " '4092',\n",
              " '2982',\n",
              " '1250',\n",
              " '1250',\n",
              " '2982',\n",
              " '4394',\n",
              " '2708',\n",
              " '3036',\n",
              " '3986']"
            ]
          },
          "metadata": {},
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tuples_removed"
      ],
      "metadata": {
        "id": "uWmUMWstu2eT",
        "outputId": "54145744-b8a4-4736-cbcf-41c4b02cdcb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "uWmUMWstu2eT",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "191311"
            ]
          },
          "metadata": {},
          "execution_count": 250
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupby(\"incident_type\").count()[\"incident_id\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "SLY3DamcRZf5",
        "outputId": "f2b2edae-be83-4b96-f667-ead715f7fa60"
      },
      "id": "SLY3DamcRZf5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "incident_type\n",
              "2     119\n",
              "3       5\n",
              "4      78\n",
              "6       6\n",
              "7       4\n",
              "9     117\n",
              "11     26\n",
              "13    318\n",
              "14    149\n",
              "16      4\n",
              "17     10\n",
              "99    175\n",
              "Name: incident_id, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incident_id</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>incident_type</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>78</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>318</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>175</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lens = []\n",
        "for events in cleaned_events_pre_incident:\n",
        "  lens.append(len(events))\n",
        "print(np.mean(lens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oSmmOtV7iiu1",
        "outputId": "07185d16-be85-4c8b-b5fc-1de533382930"
      },
      "id": "oSmmOtV7iiu1",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9.22650840751731\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_size = 4\n",
        "word2vec = Word2Vec(sentences=cleaned_events_pre_incident, vector_size=vector_size, window=9, sg=1, min_count=4, workers=-1)\n",
        "embeddings = []\n",
        "labels = df[\"incident_type\"]\n",
        "actual_events = []\n",
        "\n",
        "for events in tqdm(cleaned_events_pre_incident):\n",
        "  embedding = np.zeros(vector_size)\n",
        "  denominator = 0\n",
        "  for event in events:\n",
        "    if event in word2vec.wv:\n",
        "      embedding += word2vec.wv[event]\n",
        "      denominator += 1\n",
        "\n",
        "  embeddings.append(embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgIbXBmK8c3M",
        "outputId": "fde2a02b-9c13-4281-fcef-8073a22b2a88"
      },
      "id": "kgIbXBmK8c3M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:EPOCH 0: supplied example count (0) did not equal expected count (1011)\n",
            "WARNING:gensim.models.word2vec:EPOCH 0: supplied raw word count (0) did not equal expected count (9328)\n",
            "WARNING:gensim.models.word2vec:EPOCH 1: supplied example count (0) did not equal expected count (1011)\n",
            "WARNING:gensim.models.word2vec:EPOCH 1: supplied raw word count (0) did not equal expected count (9328)\n",
            "WARNING:gensim.models.word2vec:EPOCH 2: supplied example count (0) did not equal expected count (1011)\n",
            "WARNING:gensim.models.word2vec:EPOCH 2: supplied raw word count (0) did not equal expected count (9328)\n",
            "WARNING:gensim.models.word2vec:EPOCH 3: supplied example count (0) did not equal expected count (1011)\n",
            "WARNING:gensim.models.word2vec:EPOCH 3: supplied raw word count (0) did not equal expected count (9328)\n",
            "WARNING:gensim.models.word2vec:EPOCH 4: supplied example count (0) did not equal expected count (1011)\n",
            "WARNING:gensim.models.word2vec:EPOCH 4: supplied raw word count (0) did not equal expected count (9328)\n",
            "100%|| 1011/1011 [00:00<00:00, 24517.17it/s]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}