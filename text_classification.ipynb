{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stef4k/train-maintenance-data-mining/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee",
      "metadata": {
        "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee"
      },
      "source": [
        "# Text classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9",
      "metadata": {
        "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import ast\n",
        "import pickle\n",
        "from joblib import Parallel, delayed\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.base import TransformerMixin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import differential_evolution\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58",
      "metadata": {
        "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58"
      },
      "source": [
        "Manually remove the first ';' from the first row in csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
      "metadata": {
        "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
        "outputId": "1e451854-1f10-4f1d-b892-c0fa64e97a88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     incident_id                                  vehicles_sequence  \\\n",
              "373      4451319  [541, 541, 541, 541, 541, 541, 541, 541, 541, ...   \n",
              "991      4611315  [547, 547, 547, 547, 547, 547, 547, 547, 547, ...   \n",
              "\n",
              "                                       events_sequence  \\\n",
              "373  [3636, 2956, 2956, 2956, 2956, 2956, 4066, 363...   \n",
              "991  [4068, 4068, 4068, 4068, 4068, 3658, 4068, 365...   \n",
              "\n",
              "                          seconds_to_incident_sequence  approx_lat  \\\n",
              "373  [-14390, -14306, -14294, -14273, -14269, -1424...   50.948527   \n",
              "991  [-14273, -14006, -13798, -13647, -13502, -1347...   50.558296   \n",
              "\n",
              "     approx_lon                                 train_kph_sequence  \\\n",
              "373    4.417702  [0.0, 88.7, 90.2, 80.4, 78.6, 57.2, 0.0, 0.0, ...   \n",
              "991    3.919084  [0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, ...   \n",
              "\n",
              "                                  dj_ac_state_sequence  \\\n",
              "373  [False, False, False, False, False, False, Fal...   \n",
              "991  [False, False, False, False, False, False, Fal...   \n",
              "\n",
              "                                  dj_dc_state_sequence  incident_type  \n",
              "373  [True, True, True, True, True, True, True, Tru...             99  \n",
              "991  [True, True, True, True, True, True, True, Tru...              9  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-22de0631-67c5-4256-a24a-e6dc0f4d42d8\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incident_id</th>\n",
              "      <th>vehicles_sequence</th>\n",
              "      <th>events_sequence</th>\n",
              "      <th>seconds_to_incident_sequence</th>\n",
              "      <th>approx_lat</th>\n",
              "      <th>approx_lon</th>\n",
              "      <th>train_kph_sequence</th>\n",
              "      <th>dj_ac_state_sequence</th>\n",
              "      <th>dj_dc_state_sequence</th>\n",
              "      <th>incident_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>373</th>\n",
              "      <td>4451319</td>\n",
              "      <td>[541, 541, 541, 541, 541, 541, 541, 541, 541, ...</td>\n",
              "      <td>[3636, 2956, 2956, 2956, 2956, 2956, 4066, 363...</td>\n",
              "      <td>[-14390, -14306, -14294, -14273, -14269, -1424...</td>\n",
              "      <td>50.948527</td>\n",
              "      <td>4.417702</td>\n",
              "      <td>[0.0, 88.7, 90.2, 80.4, 78.6, 57.2, 0.0, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>991</th>\n",
              "      <td>4611315</td>\n",
              "      <td>[547, 547, 547, 547, 547, 547, 547, 547, 547, ...</td>\n",
              "      <td>[4068, 4068, 4068, 4068, 4068, 3658, 4068, 365...</td>\n",
              "      <td>[-14273, -14006, -13798, -13647, -13502, -1347...</td>\n",
              "      <td>50.558296</td>\n",
              "      <td>3.919084</td>\n",
              "      <td>[0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-22de0631-67c5-4256-a24a-e6dc0f4d42d8')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-22de0631-67c5-4256-a24a-e6dc0f4d42d8 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-22de0631-67c5-4256-a24a-e6dc0f4d42d8');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cfe1760b-7d20-400f-ae35-129bdb9691f3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cfe1760b-7d20-400f-ae35-129bdb9691f3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cfe1760b-7d20-400f-ae35-129bdb9691f3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"incident_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 113134,\n        \"min\": 4451319,\n        \"max\": 4611315,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4611315,\n          4451319\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vehicles_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 547, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680, 680]\",\n          \"[541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 541, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"events_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[4068, 4068, 4068, 4068, 4068, 3658, 4068, 3658, 4068, 4068, 3658, 4068, 3658, 4068, 4068, 4068, 4068, 2744, 4148, 2708, 4026, 4020, 4124, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 4394, 2708, 4124, 3636, 3658, 4078, 2956, 2956, 2956, 2956, 4066, 3636, 4078, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4078, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 4072, 2708, 4016, 4026, 4020, 4068, 4068, 4068, 4068, 4068, 4066, 2744, 4148, 4026, 2708, 4026, 4020, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 4026, 4016, 4026, 4020, 4068, 4068, 4068, 4068, 4068, 4068, 4066, 4068, 4068, 4068, 4068, 4068, 4066, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 3224, 4396, 4068, 3636, 3658, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 4026, 4016, 4020, 4066, 4066, 3658, 4066, 3658, 4066, 1758, 4394, 1746, 1750, 4066, 3658, 4066, 3782, 3658, 4066, 4066, 3658, 4066, 4066, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 2742, 4148, 2708, 4026, 4020, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4070, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2858, 4120, 2708, 4026, 4016, 4020, 4066, 4066, 4066, 4066, 4066, 4066, 3792, 3792, 3782, 3792, 3792, 3794, 2742, 4026, 2708, 4020, 4026, 2684, 3620, 1758, 4394, 3782, 3792, 3782, 1746, 1750, 3636, 3658, 3782, 3792, 3794, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3734, 3722, 3772, 3762, 3636, 3658, 3722, 3762, 3722, 3620, 4068, 3734, 3636, 3658, 2708, 4068, 3636, 3658, 3734, 4068, 3636, 3658, 3734, 3722, 4068, 3774, 3772, 3774, 3772, 3636, 3658, 3772, 3772, 3774, 3762, 4120, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708]\",\n          \"[3636, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2684, 4124, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 4396, 2956, 2956, 4394, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 4026, 4016, 4026, 4020, 4068, 4068, 4068, 4068, 4068, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4066, 3658, 4066, 3658, 4068, 3658, 4068, 3658, 4066, 4068, 4068, 4068, 4068, 3354, 3364, 4068, 4068, 4068, 2940, 4068, 4068, 4068, 4066, 4068, 2744, 4026, 4148, 2708, 4020, 3788, 3788, 3352, 3352, 3352, 3352, 3780, 3782, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 3352, 3352, 3352, 3352, 3352, 4124, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4066, 3658, 4068, 3658, 4068, 3658, 4068, 4066, 4396, 154, 4394, 152, 4066, 4066, 4068, 4068, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4066, 3658, 4068, 3658, 4068, 2744, 4026, 2708, 4020, 4026, 3636, 3658, 4124, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 4124, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 3354, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4180, 4066, 2708, 4026, 4016, 4020, 3352, 3352, 3352, 3352, 3352, 3352, 3352, 3352, 3352, 4066, 4068, 4068, 4068, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 4068, 4068, 4066]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seconds_to_incident_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[-14273, -14006, -13798, -13647, -13502, -13479, -13237, -13221, -12974, -12621, -12595, -12380, -12342, -12152, -12002, -11837, -11503, -11307, -11307, -11306, -11306, -11305, -11009, -10947, -10947, -10834, -10800, -10741, -10719, -10686, -10676, -10661, -10609, -10577, -10577, -10521, -10507, -10487, -10477, -10472, -10459, -10398, -10364, -10303, -10274, -10265, -10239, -10226, -10225, -10193, -10169, -10129, -10113, -10109, -10102, -10099, -10091, -10053, -10012, -10004, -9871, -9784, -9697, -9697, -9686, -9662, -9591, -9541, -9528, -9479, -9418, -9410, -9339, -9338, -9278, -9262, -9190, -9165, -9161, -9158, -9115, -9097, -9090, -9082, -8978, -8964, -8927, -8874, -8871, -8865, -8836, -8830, -8809, -8803, -8798, -8749, -8713, -8713, -8680, -8677, -8666, -8662, -8623, -8567, -8538, -8516, -8484, -8443, -8404, -8368, -8335, -8329, -8321, -8284, -8254, -8254, -8181, -8169, -8167, -8162, -8147, -8144, -8106, -8086, -8086, -7980, -7979, -7976, -7911, -7902, -7888, -7886, -7879, -7847, -7810, -7780, -7760, -7759, -7744, -7742, -7737, -7735, -7728, -7721, -7705, -7687, -7667, -7647, -7645, -7627, -7609, -7579, -7533, -7454, -7400, -7375, -7332, -7318, -7314, -7280, -7268, -7190, -7181, -7175, -7135, -7113, -7099, -7086, -7035, -7023, -6991, -6979, -6942, -6937, -6921, -6913, -6898, -6815, -6783, -6778, -6767, -6756, -6665, -6665, -6664, -5925, -5652, -5404, -5148, -4934, -4582, -4407, -4407, -4406, -4405, -4405, -4404, -3314, -3314, -3257, -3164, -3144, -3128, -3106, -3083, -3070, -3053, -3051, -3037, -3015, -3003, -2997, -2977, -2972, -2960, -2932, -2930, -2920, -2917, -2904, -2866, -2826, -2826, -2762, -2751, -2746, -2745, -2731, -2691, -2675, -2644, -2624, -2624, -2592, -2563, -2561, -2542, -2522, -2508, -2501, -2488, -2457, -2433, -2407, -2407, -2318, -2301, -2274, -2263, -2249, -2236, -2230, -2214, -2190, -2155, -2123, -2123, -2066, -2012, -2000, -1953, -1941, -1939, -1869, -1836, -1836, -1765, -1750, -1737, -1710, -1628, -1581, -1386, -1227, -1226, -1226, -1225, -179, 56, 359, 495, 792, 1233, 2149, 2470, 2681, 2843, 3009, 3262, 3546, -14390, -14320, -14276, -14251, -14251, -14217, -14188, -14164, -14161, -14141, -14123, -14106, -14088, -14083, -14075, -14074, -14068, -14067, -14052, -14036, -14009, -13996, -13996, -13955, -13947, -13945, -13933, -13926, -13894, -13885, -13883, -13801, -13787, -13787, -13749, -13744, -13724, -13712, -13650, -13638, -13638, -13593, -13585, -13580, -13578, -13572, -13547, -13506, -13482, -13440, -13410, -13385, -13335, -13308, -13283, -13280, -13272, -13269, -13241, -13223, -13149, -13139, -13112, -13085, -13080, -13078, -13037, -13020, -13003, -12978, -12951, -12951, -12917, -12868, -12865, -12861, -12856, -12835, -12831, -12814, -12796, -12794, -12787, -12784, -12768, -12766, -12705, -12693, -12624, -12597, -12526, -12472, -12460, -12384, -12345, -12269, -12215, -12200, -12185, -12184, -12156, -12128, -12128, -12075, -12061, -12038, -12005, -11984, -11984, -11950, -11916, -11906, -11892, -11882, -11840, -11819, -11819, -11764, -11748, -11707, -11679, -11614, -11584, -11543, -11506, -11495, -11310, -11309, -11308, -10612, -10367, -10307, -10196, -10173, -10016, -10009, -10008, -9848, -9848, -9482, -9422, -9085, -9008, -8982, -8753, -8446, -8407, -8288, -8109, -7850, -7814, -7582, -7537, -7283, -7272, -7103, -7090, -6781, -6669, -6669, -6668, -6668, -6667, -6197, -6197, -6168, -6007, -5988, -5981, -5971, -5928, -5895, -5895, -5801, -5799, -5755, -5745, -5734, -5691, -5677, -5667, -5655, -5633, -5633, -5569, -5553, -5547, -5535, -5521, -5511, -5486, -5472, -5450, -5436, -5407, -5353, -5353, -5276, -5273, -5251, -5235, -5225, -5215, -5199, -5198, -5185, -5173, -5151, -5100, -5100, -5053, -5011, -4998, -4994, -4985, -4937, -4910, -4910, -4860, -4846, -4843, -4835, -4833, -4811, -4801, -4797, -4786, -4777, -4767, -4749, -4738, -4736, -4724, -4721, -4697, -4686, -4678, -4665, -4628, -4610, -4602, -4585, -4579, -4578, -4574, -4410, -4409, -4407, -2870, -2648, -2436, -2159, -1872, -1584, -1272, -1269, -1264, -1244, -1241, -1240, -1231, -1231, -1229, -1229, -1229, -1130, -1130, -1126, -1125, -1101, -1054, -1046, -879, -879, -580, -580, -570, -568, -567, -548, -410, -380, -361, -329, -327, -285, -272, -235, -222, -199, -183, -119, -119, -77, -42, -31, -22, 53, 84, 87, 105, 107, 139, 139, 210, 256, 270, 331, 355, 404, 439, 439, 474, 492, 559, 559, 590, 789, 898, 898, 940, 943, 1230, 1345, 1348, 1356, 1367, 1406, 1406, 1413, 1415, 1415, 1460, 1799, 1847, 1855, 1997, 2027, 2104, 2146, 2206, 2206, 2241, 2279, 2304, 2306, 2331, 2348, 2366, 2385, 2392, 2402, 2403, 2409, 2411, 2427, 2442, 2467, 2486, 2486, 2521, 2529, 2530, 2542, 2550, 2583, 2592, 2594, 2678, 2708, 2708, 2746, 2752, 2773, 2786, 2840, 2876, 2876, 2928, 2936, 2941, 2943, 2948, 2972, 3005, 3023, 3023, 3056, 3088, 3110, 3166, 3195, 3217, 3220, 3227, 3230, 3259, 3277, 3277, 3342, 3352, 3383, 3416, 3422, 3424, 3482, 3507, 3514, 3542, 3552]\",\n          \"[-14390, -14306, -14294, -14273, -14269, -14244, -14218, -14200, -14158, -14147, -14137, -14124, -14083, -14054, -13990, -13834, -13822, -13796, -13779, -13775, -13771, -13757, -13733, -13707, -13674, -13669, -13645, -13639, -13637, -13612, -13562, -13550, -13520, -13512, -13488, -13444, -13433, -13416, -13406, -13375, -13373, -13349, -13342, -13322, -13318, -13312, -13306, -13304, -13272, -13263, -13246, -13219, -13191, -13163, -13124, -13084, -13077, -13075, -13067, -13024, -13001, -12981, -12932, -12898, -12882, -12851, -12843, -12833, -12795, -12781, -12765, -12739, -12694, -12691, -12683, -12637, -12633, -12624, -12623, -12572, -12556, -12501, -12488, -12477, -12464, -12462, -12430, -12401, -12391, -12347, -12343, -12319, -12313, -12278, -12219, -12178, -12175, -12173, -12170, -12162, -12155, -12119, -12083, -12000, -11997, -11986, -11940, -11934, -11904, -11855, -11855, -11822, -11803, -11794, -11745, -11742, -11740, -11727, -11724, -11723, -11700, -11680, -11680, -11629, -11624, -11602, -11600, -11598, -11595, -11590, -11581, -11577, -11547, -11523, -11523, -11487, -11481, -11461, -11457, -11447, -11443, -11424, -11414, -11400, -11400, -11370, -11365, -11348, -11344, -11319, -11316, -11313, -11277, -11275, -11240, -11123, -11123, -10937, -10905, -10879, -10863, -10857, -10843, -10838, -10818, -10805, -10788, -10774, -10765, -10731, -10710, -10710, -10656, -10643, -10610, -10603, -10597, -10563, -10535, -10481, -10468, -10451, -10442, -10429, -10401, -10394, -10378, -10325, -10251, -10247, -10211, -10204, -10202, -10194, -10170, -10161, -10140, -10114, -10084, -10043, -10007, -10005, -9978, -9947, -9938, -9862, -9839, -9777, -9773, -9771, -9708, -9705, -9692, -9679, -9651, -9638, -9622, -9592, -9537, -9518, -9501, -9469, -9457, -9428, -9420, -9410, -9390, -9387, -9379, -9363, -9356, -9339, -9310, -9174, -9171, -9101, -9069, -9061, -8904, -8902, -8839, -8817, -8083, -8082, -8081, -8080, -6904, -6630, -6406, -6209, -6079, -5901, -5874, -5766, -5742, -5603, -5584, -5471, -5452, -5367, -5346, -5092, -5029, -4827, -4795, -4690, -4664, -4490, -4455, -4244, -4214, -3832, -3561, -3366, -3028, -2793, -2748, -2748, -2453, -2130, -1807, -1656, -1597, -1359, -1039, -802, -516, -276, -276, -276, -275, -274, -62, -59, 8, 26, 163, 169, 237, 342, 376, 376, 395, 487, 491, 515, 518, 546, 549, 991, 1000, 1004, 1006, 1017, 1021, 1036, 1044, 1082, 1086, 1088, 1114, 1145, 1145, 1182, 1184, 1187, 1201, 1204, 1219, 1222, 1244, 1246, 1250, 1253, 1260, 1310, 1326, 1326, 1366, 1370, 1392, 1408, 1432, 1441, 1451, 1459, 1478, 1486, 1494, 1519, 1540, 1540, 1576, 1596, 1610, 1636, 1693, 1721, 1721, 1775, 1798, 1813, 1823, 1832, 1841, 1846, 1858, 1865, 1886, 1914, 1944, 1950, 1954, 1960, 1973, 1979, 1985, 1990, 1994, 2003, 2012, 2037, 2050, 2077, 2086, 2099, 2137, 2164, 2237, 2247, 2262, 2265, 2285, 2312, 2334, 2369, 2380, 2392, 2404, 2445, 2468, 2495, 2505, 2509, 2526, 2542, 2546, 2550, 2564, 2592, 2647, 2681, 2688, 2715, 2723, 2726, 2755, 2800, 2809, 2831, 2843, 2876, 2876, 2911, 2921, 2936, 2945, 2975, 2977, 3001, 3007, 3023, 3026, 3031, 3036, 3037, 3068, 3077, 3094, 3117, 3147, 3174, 3174, 3232, 3271, 3278, 3279, 3287, 3323, 3347, 3366, 3366, 3424, 3458, 3474, 3502, 3509, 3519, 3552, 3565, 3583, -14393, -14221, -14202, -13735, -13710, -13515, -13490, -13193, -13165, -13004, -12983, -12768, -12742, -12575, -12559, -12316, -12280, -11906, -11703, -11604, -11595, -11594, -11593, -11549, -11417, -11243, -10733, -10566, -10538, -10380, -10327, -10116, -10086, -9864, -9842, -9625, -9594, -9341, -9312, -8841, -8085, -8085, -8083, -8083, -8083, -7838, -7838, -7818, -7262, -7226, -7088, -7085, -7069, -7025, -7023, -6956, -6947, -6906, -6881, -6881, -6818, -6790, -6783, -6769, -6724, -6709, -6688, -6679, -6665, -6658, -6632, -6601, -6601, -6558, -6527, -6518, -6507, -6505, -6461, -6451, -6448, -6445, -6408, -6384, -6384, -6326, -6291, -6281, -6211, -6185, -6185, -6151, -6135, -6081, -6052, -6052, -5987, -5976, -5970, -5945, -5933, -5928, -5903, -5876, -5848, -5800, -5795, -5780, -5768, -5744, -5706, -5677, -5671, -5668, -5662, -5647, -5620, -5605, -5586, -5534, -5531, -5500, -5473, -5454, -5423, -5416, -5369, -5347, -5287, -5285, -5254, -5241, -5220, -5206, -5202, -5198, -5184, -5134, -5094, -5031, -4988, -4984, -4959, -4957, -4954, -4936, -4932, -4930, -4926, -4904, -4897, -4893, -4877, -4874, -4871, -4858, -4855, -4829, -4797, -4757, -4753, -4733, -4730, -4717, -4714, -4693, -4666, -4629, -4622, -4618, -4586, -4575, -4522, -4519, -4514, -4493, -4456, -4398, -4393, -4354, -4350, -4338, -4320, -4315, -4310, -4308, -4303, -4275, -4271, -4246, -4216, -4197, -4175, -4168, -4116, -4113, -4108, -4106, -4102, -4095, -4067, -4028, -3996, -3985, -3946, -3927, -3878, -3860, -3854, -3834, -3800, -3800, -3747, -3708, -3698, -3691, -3663, -3656, -3646, -3605, -3564, -3540, -3540, -3494, -3447, -3438, -3428, -3387, -3368, -3351, -3351, -3294, -3265, -3246, -3198, -3196, -3189, -3184, -3179, -3151, -3126, -3125, -3098, -3087, -3071, -3058, -3047, -3030, -2992, -2977, -2977, -2922, -2910, -2869, -2861, -2844, -2842, -2836, -2818, -2815, -2795, -2770, -2770, -2750, -2677, -2662, -2660, -2658, -2656, -2644, -2628, -2624, -2613, -2591, -2579, -2563, -2556, -2543, -2514, -2501, -2475, -2455, -2310, -2310, -2274, -2233, -2221, -2201, -2132, -2074, -2074, -2018, -2003, -1958, -1941, -1922, -1907, -1895, -1883, -1874, -1871, -1862, -1855, -1848, -1844, -1826, -1824, -1809, -1767, -1767, -1720, -1710, -1700, -1683, -1681, -1666, -1662, -1648, -1629, -1599, -1517, -1517, -1457, -1442, -1428, -1400, -1384, -1361, -1342, -1342, -1287, -1274, -1262, -1235, -1222, -1203, -1170, -1131, -1110, -1098, -1077, -1074, -1041, -1013, -1013, -943, -934, -928, -888, -886, -878, -863, -857, -840, -837, -834, -804, -780, -780, -736, -731, -727, -700, -698, -680, -678, -652, -648, -622, -618, -591, -542, -518, -506, -278, -277, -276, 6, 24, 161, 167, 990, 998, 1002, 1004, 1015, 1111, 1308, 1517, 1691, 1884, 1911, 2135, 2161, 2309, 2331, 2590, 2645, 2840, 3145, 3345, 3580]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.27593528385272,\n        \"min\": 50.55829607228572,\n        \"max\": 50.94852749304751,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          50.55829607228572,\n          50.94852749304751\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.35257582964986556,\n        \"min\": 3.9190841944285713,\n        \"max\": 4.417701714484357,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3.9190841944285713,\n          4.417701714484357\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_kph_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[0.1, 0.1, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.2, 0.0, 0.2, 0.0, 0.1, 0.1, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.1, 33.6, 99.2, 115.9, 106.5, 103.1, 91.3, 0.0, 0.0, 0.0, 69.1, 78.1, 76.6, 70.1, 65.8, 53.9, 26.2, 0.0, 0.0, 14.8, 35.0, 78.5, 72.2, 70.1, 0.1, 0.0, 32.1, 61.9, 67.1, 75.2, 79.4, 86.7, 63.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.5, 84.7, 84.9, 82.8, 0.2, 0.0, 0.0, 74.5, 75.8, 82.8, 85.1, 51.9, 38.0, 38.5, 38.5, 35.1, 24.9, 14.8, 0.0, 0.0, 0.0, 32.5, 35.2, 34.8, 34.4, 39.2, 44.7, 53.7, 54.9, 58.8, 0.2, 0.0, 0.0, 43.6, 49.5, 49.5, 48.0, 36.9, 35.7, 52.5, 74.0, 68.8, 0.1, 0.0, 32.9, 74.0, 75.0, 73.3, 0.1, 0.0, 0.0, 76.9, 72.5, 72.1, 71.5, 70.1, 69.2, 0.0, 0.0, 0.0, 74.7, 74.4, 73.7, 75.2, 75.0, 75.2, 75.3, 66.4, 0.1, 0.0, 36.5, 80.3, 82.5, 102.0, 103.9, 109.8, 111.2, 109.5, 100.5, 74.1, 54.1, 51.0, 53.5, 53.3, 54.2, 49.5, 0.1, 0.0, 27.1, 36.3, 60.4, 82.9, 65.1, 62.1, 0.1, 0.0, 116.1, 125.4, 127.7, 69.7, 28.2, 0.2, 0.0, 80.4, 99.2, 123.2, 122.2, 119.9, 118.0, 94.5, 81.7, 58.6, 27.9, 11.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2, 0.2, 0.3, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.1, 30.3, 26.3, 28.6, 56.5, 57.9, 60.7, 61.6, 72.0, 81.1, 83.9, 85.2, 89.6, 87.2, 80.8, 71.8, 71.5, 70.4, 67.5, 56.1, 0.0, 0.0, 0.0, 55.0, 67.8, 68.9, 69.5, 72.7, 73.0, 47.2, 0.0, 0.0, 0.0, 7.9, 63.6, 66.2, 90.8, 97.8, 96.1, 86.7, 76.3, 37.2, 0.1, 0.0, 0.0, 85.3, 97.0, 95.8, 95.9, 85.1, 75.6, 70.8, 60.5, 35.9, 0.0, 0.0, 0.0, 36.0, 88.1, 87.2, 84.4, 76.8, 75.2, 0.1, 0.0, 0.0, 45.4, 46.1, 46.7, 31.3, 33.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.3, 0.0, 0.0, 0.0, 0.0, 0.2, 1.0, 0.1, 0.2, 0.4, 0.3, 0.1, 37.9, 37.6, 0.0, 0.0, 0.0, 28.9, 36.4, 39.2, 39.4, 57.9, 59.9, 65.9, 99.0, 107.2, 117.1, 118.7, 117.7, 117.6, 85.9, 63.3, 0.0, 0.0, 0.0, 55.3, 72.8, 75.3, 93.4, 102.6, 117.8, 117.3, 116.6, 0.0, 0.0, 0.0, 34.7, 50.2, 83.7, 84.2, 0.1, 0.0, 0.0, 55.1, 69.2, 77.5, 80.4, 88.0, 66.2, 0.0, 0.0, 51.7, 75.0, 46.2, 38.2, 46.7, 64.0, 63.9, 60.8, 59.0, 0.0, 0.0, 57.6, 57.6, 38.8, 37.7, 38.0, 38.0, 36.1, 25.0, 22.1, 0.0, 0.0, 0.0, 19.3, 36.2, 35.2, 33.9, 32.2, 42.0, 45.5, 62.6, 82.6, 83.6, 87.9, 87.2, 87.0, 87.2, 89.0, 89.1, 0.1, 0.0, 84.1, 86.4, 87.5, 0.1, 0.0, 62.0, 69.6, 69.1, 56.4, 56.2, 0.0, 0.0, 0.0, 66.8, 76.6, 53.5, 0.0, 0.0, 0.0, 36.6, 102.3, 111.6, 111.3, 93.4, 0.0, 0.0, 0.0, 82.2, 104.8, 111.8, 64.7, 40.6, 34.0, 28.7, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.1, 0.0, 0.1, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.1, 0.0, 0.0, 0.2, 0.2, 0.0, 0.2, 0.2, 0.2, 0.0, 0.2, 0.0, 0.3, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 41.8, 60.0, 64.3, 66.7, 0.0, 0.0, 0.0, 76.4, 77.3, 98.9, 96.4, 94.3, 67.8, 37.2, 21.8, 0.0, 0.0, 0.0, 56.4, 72.4, 77.0, 86.1, 94.4, 99.7, 111.5, 109.1, 90.9, 65.3, 0.2, 0.0, 0.0, 52.5, 57.1, 78.9, 89.4, 94.7, 97.4, 92.9, 92.0, 72.1, 52.7, 0.1, 0.0, 0.0, 39.6, 91.3, 85.3, 83.2, 63.2, 0.0, 0.0, 0.0, 47.6, 70.5, 73.6, 83.3, 85.2, 103.1, 108.2, 110.3, 109.8, 105.2, 98.0, 88.1, 82.3, 81.6, 73.9, 71.6, 54.4, 53.8, 54.7, 40.2, 29.7, 21.9, 18.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.3, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.4, 36.1, 40.3, 85.9, 88.5, 111.5, 114.8, 78.2, 74.3, 42.9, 0.3, 0.0, 0.0, 64.2, 96.3, 96.1, 103.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.1, 27.6, 37.2, 35.1, 35.3, 0.1, 0.0, 0.0, 20.5, 37.3, 36.8, 37.0, 55.6, 60.1, 59.8, 87.6, 88.8, 97.2, 99.1, 98.1, 98.1, 89.6, 66.5, 0.3, 0.0, 0.0, 61.4, 76.3, 78.2, 94.2, 102.7, 115.0, 113.9, 113.2, 0.0, 0.0, 0.0, 28.4, 44.8, 84.0, 85.6, 0.1, 0.0, 0.0, 57.4, 72.0, 81.5, 84.1, 89.5, 85.0, 0.2, 0.0, 0.0, 48.2, 72.3, 60.2, 32.4, 44.8, 75.2, 74.8, 69.2, 65.8, 0.2, 0.0, 0.0, 56.3, 56.5, 34.4, 30.7, 33.9, 33.8, 25.3, 17.1, 15.2, 0.1, 0.0]\",\n          \"[0.0, 88.7, 90.2, 80.4, 78.6, 57.2, 0.0, 0.0, 45.5, 76.3, 88.9, 84.9, 74.6, 60.8, 35.6, 15.9, 52.7, 55.3, 48.7, 48.0, 47.6, 45.5, 0.4, 0.0, 26.8, 40.4, 41.5, 41.7, 41.9, 38.3, 27.1, 28.5, 20.3, 0.3, 0.0, 30.2, 34.0, 36.5, 36.4, 37.7, 37.8, 48.9, 59.4, 81.4, 79.8, 85.9, 94.4, 96.5, 113.3, 112.7, 108.8, 60.8, 0.3, 0.0, 20.1, 117.0, 116.6, 116.0, 106.2, 52.1, 0.3, 0.0, 83.2, 111.8, 107.7, 100.6, 99.0, 96.6, 62.5, 28.5, 0.2, 0.0, 32.8, 41.7, 59.2, 72.5, 82.3, 84.0, 83.6, 0.3, 0.0, 76.8, 99.0, 117.2, 116.1, 115.2, 105.5, 99.6, 98.3, 60.8, 54.8, 16.0, 0.0, 0.0, 24.7, 61.3, 59.6, 58.8, 57.4, 53.6, 52.6, 26.0, 17.9, 0.0, 0.0, 19.4, 32.4, 32.0, 0.1, 0.0, 0.0, 0.0, 18.2, 24.9, 39.0, 40.5, 40.6, 44.4, 44.4, 44.4, 0.0, 0.0, 0.0, 21.8, 35.0, 40.7, 40.9, 43.4, 45.2, 43.5, 42.4, 41.9, 0.4, 0.0, 0.0, 17.9, 32.3, 47.2, 46.7, 45.6, 45.2, 21.4, 0.5, 0.0, 0.0, 15.5, 28.0, 34.7, 35.5, 34.7, 34.1, 34.0, 29.5, 29.7, 0.2, 0.0, 0.0, 26.3, 35.0, 46.5, 50.0, 50.7, 60.0, 64.8, 82.0, 85.7, 78.5, 71.8, 56.1, 0.4, 0.0, 0.0, 78.3, 90.5, 92.2, 75.8, 69.1, 0.3, 0.0, 80.8, 85.7, 81.3, 75.4, 78.4, 51.4, 33.0, 0.4, 0.0, 52.9, 64.7, 111.5, 115.6, 113.4, 109.7, 109.6, 106.2, 73.4, 0.3, 0.0, 20.4, 88.4, 89.9, 99.5, 116.1, 114.8, 0.2, 0.0, 84.2, 90.3, 90.8, 94.2, 94.2, 95.0, 55.0, 51.9, 46.4, 0.1, 0.0, 11.0, 55.0, 90.9, 93.6, 88.7, 113.4, 116.1, 116.9, 101.5, 96.2, 80.4, 55.3, 43.5, 0.2, 0.0, 82.7, 78.6, 31.6, 25.0, 26.0, 27.5, 26.2, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.4, 0.4, 0.7, 0.8, 0.8, 0.0, 1.6, 0.0, 0.6, 0.0, 0.1, 0.0, 0.4, 0.0, 0.6, 0.0, 0.6, 0.0, 0.6, 0.0, 0.2, 0.0, 0.4, 0.0, 0.3, 0.5, 0.4, 0.3, 0.2, 0.0, 0.0, 0.6, 0.2, 0.5, 76.2, 0.2, 0.7, 0.8, 1.1, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.3, 36.5, 38.3, 38.0, 31.2, 31.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.2, 15.5, 43.1, 41.0, 40.0, 0.0, 0.0, 0.0, 37.2, 44.0, 54.3, 54.8, 59.0, 58.5, 59.8, 103.2, 102.7, 102.6, 102.4, 97.1, 0.0, 0.0, 0.0, 39.1, 53.8, 81.6, 57.0, 56.9, 55.5, 57.0, 57.1, 57.8, 58.8, 56.2, 0.0, 0.0, 0.0, 47.2, 77.9, 75.8, 65.7, 0.0, 0.0, 0.0, 51.2, 103.3, 112.7, 112.2, 97.6, 76.0, 71.4, 51.1, 41.1, 0.0, 0.0, 30.5, 52.9, 66.3, 80.5, 86.1, 85.6, 94.7, 103.6, 110.2, 115.6, 114.3, 116.3, 114.5, 115.3, 114.4, 81.9, 0.0, 0.0, 102.5, 114.4, 117.0, 113.7, 66.6, 0.2, 0.0, 42.2, 75.0, 86.7, 83.8, 85.7, 92.5, 110.1, 104.7, 95.3, 71.4, 46.2, 45.8, 46.3, 45.0, 0.0, 0.0, 22.5, 35.1, 32.4, 29.4, 29.1, 36.3, 36.2, 36.5, 24.3, 0.1, 0.0, 0.0, 28.5, 36.7, 38.2, 38.3, 38.5, 38.2, 55.0, 70.8, 101.9, 106.6, 112.4, 117.6, 116.7, 112.5, 111.7, 114.6, 67.2, 0.0, 0.0, 0.0, 22.8, 117.4, 117.0, 116.4, 115.4, 61.2, 0.0, 0.0, 0.0, 79.7, 116.0, 114.5, 108.7, 113.5, 110.6, 57.1, 34.9, 0.0, 0.0, 0.0, 0.0, 0.5, 0.0, 0.4, 0.0, 1.3, 0.0, 0.8, 0.0, 0.2, 0.0, 0.4, 0.0, 0.0, 0.0, 0.2, 0.0, 40.7, 44.2, 44.1, 43.7, 0.8, 0.8, 0.8, 1.4, 1.0, 0.0, 1.8, 0.0, 0.9, 0.0, 0.4, 0.0, 0.3, 0.0, 0.5, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.2, 55.3, 57.8, 54.0, 97.6, 98.4, 116.2, 110.1, 0.2, 0.0, 0.0, 101.4, 91.9, 88.6, 59.0, 60.9, 85.1, 117.5, 112.7, 69.2, 59.1, 0.3, 0.0, 0.0, 21.5, 95.6, 113.8, 118.1, 117.4, 101.1, 98.0, 96.0, 93.1, 0.2, 0.0, 0.0, 90.1, 109.8, 102.9, 0.4, 0.0, 0.0, 49.2, 73.1, 0.4, 0.0, 0.0, 96.5, 92.9, 91.2, 67.5, 62.5, 58.8, 0.2, 0.0, 14.5, 70.1, 67.0, 37.6, 0.3, 0.0, 22.7, 110.4, 120.4, 122.0, 119.8, 112.8, 41.1, 0.4, 0.0, 69.8, 77.7, 73.8, 0.0, 0.0, 19.9, 41.0, 0.2, 0.0, 79.6, 83.9, 86.2, 80.7, 72.8, 71.8, 68.4, 64.3, 44.5, 33.4, 0.3, 0.0, 26.6, 33.9, 38.5, 38.4, 38.3, 37.7, 37.9, 37.7, 38.0, 37.9, 47.2, 46.8, 48.2, 48.3, 48.4, 48.5, 48.1, 0.3, 0.0, 42.0, 42.1, 46.9, 47.3, 48.5, 48.4, 0.4, 0.0, 19.3, 22.3, 22.3, 15.1, 15.1, 36.4, 35.9, 36.2, 0.2, 0.0, 31.9, 32.2, 52.0, 58.9, 74.1, 77.0, 80.0, 83.6, 86.0, 90.7, 57.7, 47.5, 0.3, 0.0, 0.0, 25.6, 43.8, 75.7, 77.3, 80.8, 81.7, 81.6, 82.2, 107.6, 110.6, 104.2, 91.0, 61.4, 54.8, 48.8, 43.7, 39.4, 0.2, 0.0, 0.0, 39.4, 102.1, 112.4, 116.6, 106.5, 99.8, 99.1, 73.6, 0.4, 0.0, 0.0, 23.9, 91.4, 99.1, 103.5, 49.4, 0.2, 0.0, 0.0, 51.6, 96.0, 96.9, 90.5, 90.1, 86.4, 81.0, 76.2, 49.3, 37.6, 37.9, 34.4, 38.1, 35.0, 35.2, 36.3, 0.2, 0.0, 0.0, 0.0, 25.9, 35.2, 41.2, 41.2, 40.6, 40.4, 41.6, 61.1, 59.1, 0.1, 0.0, 0.0, 0.0, 21.8, 58.6, 62.3, 64.4, 68.8, 84.2, 92.4, 91.9, 89.8, 86.1, 85.5, 86.6, 86.3, 82.5, 80.4, 80.3, 44.5, 0.3, 0.0, 0.0, 0.0, 82.2, 94.9, 86.6, 0.2, 0.0, 0.0, 66.1, 89.5, 88.3, 85.5, 85.2, 87.2, 88.7, 90.2, 91.5, 92.1, 91.1, 83.4, 74.3, 66.2, 46.7, 42.5, 0.3, 0.0, 0.0, 35.4, 62.7, 81.0, 81.5, 81.0, 77.4, 76.6, 80.2, 64.6, 0.2, 0.0, 0.0, 69.2, 69.6, 69.4, 69.1, 63.0, 0.2, 0.0, 0.0, 25.6, 41.2, 42.2, 35.6, 32.3, 25.2, 10.8, 50.4, 52.4, 42.2, 76.9, 77.0, 0.3, 0.0, 0.0, 67.1, 70.5, 70.4, 37.8, 36.5, 42.9, 50.8, 49.8, 37.5, 35.4, 33.9, 0.3, 0.0, 0.0, 14.2, 23.4, 36.6, 52.7, 53.6, 46.8, 45.0, 30.0, 29.2, 35.7, 35.0, 25.1, 13.4, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.2, 0.1, 0.2, 0.1]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_ac_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_dc_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\",\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 63,\n        \"min\": 9,\n        \"max\": 99,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9,\n          99\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';', index_col=0)\n",
        "df.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecVectorizer(TransformerMixin):\n",
        "    def __init__(self, size=100, window=5, min_count=1, workers=4):\n",
        "        self.size = size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.w2v_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.w2v_model = Word2Vec(sentences, vector_size=self.size, window=self.window,\n",
        "                                  min_count=self.min_count, workers=self.workers)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        transformed_data = np.array([\n",
        "            np.mean([self.w2v_model.wv[word] for word in sentence.split() if word in self.w2v_model.wv]\n",
        "                    or [np.zeros(self.\n",
        "                                 size)], axis=0)\n",
        "            for sentence in X\n",
        "        ])\n",
        "        return csr_matrix(transformed_data)\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.fit(X, y)\n",
        "        return self.transform(X, y)"
      ],
      "metadata": {
        "id": "fOHHiU-pDaet"
      },
      "id": "fOHHiU-pDaet",
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleModel:\n",
        "    def __init__(self, trained_models, trained_vectorizers):\n",
        "\n",
        "        self.trained_models = trained_models\n",
        "        self.trained_vectorizers = trained_vectorizers\n",
        "        self.optimized_weights = None\n",
        "\n",
        "    def _generate_prediction_matrix(self, X):\n",
        "        predictions = {}\n",
        "        for (vect_name, samp_name, clf_name), model in self.trained_models.items():\n",
        "            vectorizer = deepcopy(self.trained_vectorizers[vect_name])\n",
        "            X_vect = vectorizer.transform(X).toarray()\n",
        "\n",
        "            predictions[(vect_name, samp_name, clf_name)] = model.predict_proba(X_vect)\n",
        "\n",
        "        return np.stack(list(predictions.values()), axis=2)\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        pred_matrix = self._generate_prediction_matrix(X_train)\n",
        "\n",
        "        def fitness(weights):\n",
        "            weighted_pred = np.tensordot(pred_matrix, weights, axes=([2], [0]))\n",
        "            final_pred = np.argmax(weighted_pred, axis=1)\n",
        "            return -f1_score(y_train, final_pred, average='weighted')\n",
        "\n",
        "        num_models = pred_matrix.shape[2]\n",
        "        bounds = [(0, 1)] * num_models\n",
        "        result = differential_evolution(fitness, bounds)\n",
        "\n",
        "        self.optimized_weights = result.x\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.optimized_weights is None:\n",
        "            raise ValueError(\"The ensemble model must be trained using `train_ensemble` before prediction.\")\n",
        "\n",
        "        pred_matrix = self._generate_prediction_matrix(X)\n",
        "        weighted_pred = np.tensordot(pred_matrix, self.optimized_weights, axes=([2], [0]))\n",
        "        ensemble_pred = np.argmax(weighted_pred, axis=1)\n",
        "        return ensemble_pred\n"
      ],
      "metadata": {
        "id": "6CDFeUgXDg3F"
      },
      "id": "6CDFeUgXDg3F",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Representations:\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def representation_a(self, df):\n",
        "            events_types_dict = {}\n",
        "            for events_sequence in df['events_sequence']:\n",
        "                row_list = ast.literal_eval(events_sequence)\n",
        "                unique_events = set(row_list)\n",
        "                for event in unique_events:\n",
        "                    if not events_types_dict.get(event):\n",
        "                        events_types_dict[event] = 0\n",
        "                    events_types_dict[event] += 1\n",
        "\n",
        "            sorted_dict = dict(sorted(events_types_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "            sorted_events_perc_df = pd.DataFrame(list(sorted_dict.items()), columns=['event_type', 'frequency'])\n",
        "            sorted_events_perc_df['percentage'] = sorted_events_perc_df['frequency'] / df.shape[0] * 100\n",
        "            sorted_events_perc_df['event_type'] = sorted_events_perc_df['event_type'].astype(str)\n",
        "\n",
        "            events_low_frequency = list(map(int, list(sorted_events_perc_df[sorted_events_perc_df.percentage <= 85].event_type)))\n",
        "\n",
        "            df['clean_events_sequence'] = (\n",
        "                df['events_sequence']\n",
        "                .apply(ast.literal_eval)\n",
        "                .apply(lambda x: [i for i in x if i in events_low_frequency])\n",
        "                .astype(str)\n",
        "                .replace(r'[\\[\\],]', '', regex=True)\n",
        "            )\n",
        "\n",
        "            self.y = df['incident_type'].copy()\n",
        "            self.X = df['clean_events_sequence'].copy()\n",
        "            return self.X, self.y\n",
        "\n",
        "  def representation_b(self, df):\n",
        "\n",
        "    before_incident = []\n",
        "    after_incident = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        events = ast.literal_eval(row['events_sequence'])\n",
        "        seconds = ast.literal_eval(row['seconds_to_incident_sequence'])\n",
        "        incident_type = row['incident_type']\n",
        "\n",
        "        before_events = \" \".join([str(event) for event, time in zip(events, seconds) if time <= 0])\n",
        "        if before_events:\n",
        "            before_incident.append({\n",
        "                \"events_sequence\": before_events,\n",
        "                \"class\": incident_type\n",
        "            })\n",
        "\n",
        "        after_events = \" \".join([str(event) for event, time in zip(events, seconds) if time > 0])\n",
        "        if after_events:\n",
        "            after_incident.append({\n",
        "                \"events_sequence\": after_events,\n",
        "                \"class\": 100\n",
        "            })\n",
        "\n",
        "    before_df = pd.DataFrame(before_incident)\n",
        "    after_df = pd.DataFrame(after_incident)\n",
        "\n",
        "    return before_df, after_df\n",
        "\n",
        "\n",
        "  def representation_c(self, df, sequence_length=30):\n",
        "\n",
        "    overlapping_sequences = []\n",
        "    step = sequence_length // 2\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        events = ast.literal_eval(row['events_sequence'])\n",
        "        seconds = ast.literal_eval(row['seconds_to_incident_sequence'])\n",
        "        incident_type = row['incident_type']\n",
        "\n",
        "        for i in range(0, len(events) - sequence_length + 1, step):\n",
        "            sequence = [str(i) for i in events[i:i + sequence_length]]\n",
        "            seconds_slice = seconds[i:i + sequence_length]\n",
        "            sequence_class = incident_type\n",
        "\n",
        "            overlapping_sequences.append({\"sequence\": \" \".join(sequence), \"class\": sequence_class})\n",
        "\n",
        "    sequences_df = pd.DataFrame(overlapping_sequences)\n",
        "    return sequences_df.sequence, sequences_df['class']\n"
      ],
      "metadata": {
        "id": "3zsHqb8oDiyJ"
      },
      "id": "3zsHqb8oDiyJ",
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Experiment:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.le = LabelEncoder()\n",
        "        self.y = self.le.fit_transform(y)\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=7, stratify=y)\n",
        "        self.y_train = self.le.transform(self.y_train)\n",
        "        self.y_test = self.le.transform(self.y_test)\n",
        "        self.trained_models = {}\n",
        "        self.trained_vectorizers = {}\n",
        "        self.trainer_samplers = {}\n",
        "        self.results = []\n",
        "        self.sampling_strategies = {\n",
        "            \"SMOTE\": SMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            \"Borderline-SMOTE\": BorderlineSMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            \"ADASYN\": ADASYN(sampling_strategy='auto', random_state=1, n_neighbors=3),\n",
        "            \"RandomOversampler\": RandomOverSampler(sampling_strategy='auto', random_state=1),\n",
        "            \"SMOTE-ENN\": SMOTEENN(sampling_strategy='auto', random_state=1),\n",
        "            \"SMOTE-Tomek\": SMOTETomek(sampling_strategy='auto', random_state=1)\n",
        "        }\n",
        "\n",
        "        self.vectorizers = {\n",
        "            \"TFIDF\": TfidfVectorizer(),\n",
        "            \"Count\": CountVectorizer(),\n",
        "            \"Word2Vec\": Word2VecVectorizer(size=100, window=5, min_count=1)\n",
        "        }\n",
        "        self.classifiers = {\n",
        "            'LogisticRegression': LogisticRegression(),\n",
        "            'DecisionTree': DecisionTreeClassifier(),\n",
        "            'RandomForest': RandomForestClassifier(),\n",
        "            'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
        "            'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
        "            'AdaBoostClassifier': AdaBoostClassifier(),\n",
        "            'GaussianNB': GaussianNB(),\n",
        "            'KNN': KNeighborsClassifier(),\n",
        "            'SVM': SVC(probability=True),\n",
        "            'XGBoost': XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), eval_metric=\"mlogloss\", use_label_encoder=False, n_jobs = 2),\n",
        "        }\n",
        "\n",
        "\n",
        "    def duplicate_minor_classes(self, X, y, min_instances=5):\n",
        "        X = X.reset_index(drop=True)\n",
        "        y  = y.reset_index(drop=True)\n",
        "        class_counts = y.value_counts()\n",
        "\n",
        "        minor_classes = class_counts[class_counts < min_instances].index\n",
        "\n",
        "        minor_class_rows = X.loc[y.isin(minor_classes)]\n",
        "        minor_class_labels = y.loc[y.isin(minor_classes)]\n",
        "\n",
        "        duplicated_X = pd.concat([minor_class_rows] * 2, ignore_index=True)\n",
        "        duplicated_y = pd.concat([minor_class_labels] * 2, ignore_index=True)\n",
        "\n",
        "        X_balanced = pd.concat([X, duplicated_X], ignore_index=True)\n",
        "        y_balanced = pd.concat([y, duplicated_y], ignore_index=True)\n",
        "\n",
        "        X_balanced.reset_index(drop=True, inplace=True)\n",
        "        y_balanced.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "\n",
        "    def test(self, model, model_name, vectorizer, vectorizer_name, sampler, sampler_name):\n",
        "        \"\"\"Test a model with Stratified K-Fold and return an array with metric results.\"\"\"\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "        accuracies, recalls, precisions, f1s = [], [], [], []\n",
        "        X, y = self.X_train, self.y_train\n",
        "\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "            X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "            X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "            model.fit(X_resampled, y_resampled)\n",
        "\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "            recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
        "            precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
        "            f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "        return [\n",
        "            model_name, vectorizer_name, sampler_name,\n",
        "            np.mean(accuracies), np.std(accuracies),\n",
        "            np.mean(recalls), np.std(recalls),\n",
        "            np.mean(precisions), np.std(precisions),\n",
        "            np.mean(f1s), np.std(f1s)\n",
        "        ]\n",
        "\n",
        "    def training(self):\n",
        "        results = []\n",
        "        progress_bar = tqdm(total = len(self.vectorizers.keys()) * len(self.sampling_strategies.keys()) * len(self.classifiers.keys()))\n",
        "        self.X_train, self.y_train = self.duplicate_minor_classes(self.X_train, pd.Series(self.y_train))\n",
        "        for vect_name, ovectorizer in self.vectorizers.items():\n",
        "            vectorizer = deepcopy(ovectorizer)\n",
        "            for samp_name, osampler in self.sampling_strategies.items():\n",
        "                sampler = deepcopy(osampler)\n",
        "                for clf_name, omodel in self.classifiers.items():\n",
        "                    model = deepcopy(omodel)\n",
        "\n",
        "                    result = self.test(\n",
        "                        model=model,\n",
        "                        model_name=clf_name,\n",
        "                        vectorizer=vectorizer,\n",
        "                        vectorizer_name=vect_name,\n",
        "                        sampler=sampler,\n",
        "                        sampler_name=samp_name\n",
        "                    )\n",
        "                    results.append(result)\n",
        "\n",
        "                    vectorizer = deepcopy(ovectorizer)\n",
        "                    sampler = deepcopy(osampler)\n",
        "                    model = deepcopy(omodel)\n",
        "                    vectorizer.fit(self.X_train)\n",
        "                    X_train = vectorizer.transform(self.X_train).toarray()\n",
        "                    X_resampled, y_resampled = sampler.fit_resample(X_train, self.y_train)\n",
        "                    model.fit(X_resampled, y_resampled)\n",
        "                    self.trained_models[(vect_name, samp_name, clf_name)] = model\n",
        "                    self.trained_vectorizers[vect_name] = vectorizer\n",
        "                    self.trainer_samplers[(vect_name, samp_name)] = sampler\n",
        "                    progress_bar.update(1)\n",
        "\n",
        "        columns = [\n",
        "            'Model', 'Vectorizer', 'Sampler',\n",
        "            'Accuracy Mean', 'Accuracy Std',\n",
        "            'Recall Mean', 'Recall Std',\n",
        "            'Precision Mean', 'Precision Std',\n",
        "            'F1 Mean', 'F1 Std'\n",
        "        ]\n",
        "        results_df = pd.DataFrame(results, columns=columns)\n",
        "        self.results = results_df\n",
        "        return results_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z5-kRVEJxLjj"
      },
      "id": "Z5-kRVEJxLjj",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evolution:\n",
        "    def __init__(self, X, y):\n",
        "\n",
        "        self.original_X = X\n",
        "        self.original_y = y\n",
        "        self.sequences = [x.split() for x in X]\n",
        "        self.labels = y\n",
        "        self.keep = self.get_unique_events()\n",
        "        self.remove = []\n",
        "        self.candidates = self.get_candidates()\n",
        "        self.next = self.candidates[0] if self.candidates else None\n",
        "        self.best_score = None\n",
        "\n",
        "    def get_unique_events(self):\n",
        "        return list(set(event for seq in self.sequences for event in seq))\n",
        "\n",
        "    def get_candidates(self):\n",
        "        sequences_flat = [(i, event) for i, seq in enumerate(self.sequences) for event in seq]\n",
        "        df = pd.DataFrame(sequences_flat, columns=[\"seq_idx\", \"event\"])\n",
        "        total_occurrences = df.shape[0]\n",
        "\n",
        "        class_counts = df.groupby(\"event\").apply(\n",
        "            lambda x: len(set(self.labels[x[\"seq_idx\"].values]))\n",
        "        )\n",
        "        event_counts = df[\"event\"].value_counts()\n",
        "        gamma_scores = (class_counts / len(set(self.labels))) * (event_counts / total_occurrences)\n",
        "\n",
        "        return gamma_scores.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "    def calculate_fitness(self):\n",
        "        mutated_X = [' '.join(seq) for seq in self.sequences]\n",
        "        mutated_y = self.labels\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=5)\n",
        "        f1_scores = []\n",
        "\n",
        "        def fit_model(train_idx, test_idx):\n",
        "            X_train = [mutated_X[i] for i in train_idx]\n",
        "            X_test = [mutated_X[i] for i in test_idx]\n",
        "            y_train, y_test = mutated_y[train_idx], mutated_y[test_idx]\n",
        "\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            X_train_transformed = vectorizer.fit_transform(X_train).toarray()\n",
        "            X_test_transformed = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "            smote = SMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3)\n",
        "            X_resampled, y_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
        "\n",
        "            model = XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), eval_metric=\"mlogloss\", use_label_encoder=False, n_jobs = 2)\n",
        "            model.fit(X_resampled, y_resampled)\n",
        "\n",
        "            y_pred = model.predict(X_test_transformed)\n",
        "            return f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "        f1_scores = Parallel(n_jobs=-1)(\n",
        "            delayed(fit_model)(train_idx, test_idx) for train_idx, test_idx in skf.split(mutated_X, mutated_y)\n",
        "        )\n",
        "        mean_f1 = np.mean(f1_scores)\n",
        "        std_f1 = np.std(f1_scores)\n",
        "        return mean_f1 - std_f1\n",
        "\n",
        "    def mutate(self):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = self.calculate_fitness()\n",
        "            print(\"Starting score:\", self.best_score)\n",
        "            return\n",
        "\n",
        "        modified_sequences = [[event for event in seq if event != self.next] for seq in self.sequences]\n",
        "\n",
        "        original_sequences = self.sequences[:]\n",
        "\n",
        "        self.sequences = modified_sequences\n",
        "        fitness_score = self.calculate_fitness()\n",
        "\n",
        "        if fitness_score >= self.best_score:\n",
        "            self.remove.append(self.next)\n",
        "            self.keep.remove(self.next)\n",
        "            self.best_score = fitness_score\n",
        "            print(\"New best score:\", self.best_score)\n",
        "        elif self.best_score - fitness_score <= 0.03:\n",
        "            self.sequences = [seq for seq in modified_sequences if seq]\n",
        "            self.remove.append(self.next)\n",
        "            self.keep.remove(self.next)\n",
        "            print(\"Noise removed\", fitness_score)\n",
        "        else:\n",
        "            self.sequences = original_sequences\n",
        "            print(\"No changes\", fitness_score)\n",
        "\n",
        "        self.candidates = [e for e in self.candidates if e not in self.remove]\n",
        "        self.next = self.candidates[0] if self.candidates else None\n",
        "\n",
        "    def save_state(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "        print(f\"State saved to {filename}.\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_state(cls, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            obj = pickle.load(f)\n",
        "        print(f\"State loaded from {filename}.\")\n",
        "        return obj\n",
        "\n",
        "    def run(self, generations):\n",
        "        c = 0\n",
        "        for _ in tqdm(range(generations)):\n",
        "            if not self.next:\n",
        "                break\n",
        "            self.mutate()\n",
        "            if c == 30:\n",
        "              self.save_state(\"evolution_state.pkl\")\n",
        "              files.download('evolution_state.pkl')\n",
        "              c = 0\n",
        "            c += 1\n"
      ],
      "metadata": {
        "id": "pPZNbecHzfTz"
      },
      "id": "pPZNbecHzfTz",
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def duplicate_minor_classes(X, y, min_instances=10):\n",
        "    X = X.reset_index(drop=True)\n",
        "    y  = y.reset_index(drop=True)\n",
        "    class_counts = y.value_counts()\n",
        "\n",
        "    minor_classes = class_counts[class_counts <= min_instances].index\n",
        "\n",
        "    minor_class_rows = X.loc[y.isin(minor_classes)]\n",
        "    minor_class_labels = y.loc[y.isin(minor_classes)]\n",
        "\n",
        "    duplicated_X = pd.concat([minor_class_rows] * 2, ignore_index=True)\n",
        "    duplicated_y = pd.concat([minor_class_labels] * 2, ignore_index=True)\n",
        "\n",
        "    X_balanced = pd.concat([X, duplicated_X], ignore_index=True)\n",
        "    y_balanced = pd.concat([y, duplicated_y], ignore_index=True)\n",
        "\n",
        "    X_balanced.reset_index(drop=True, inplace=True)\n",
        "    y_balanced.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return X_balanced, y_balanced"
      ],
      "metadata": {
        "id": "FEn5DGXm_hyE"
      },
      "id": "FEn5DGXm_hyE",
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the pipeline with no sequence modification:"
      ],
      "metadata": {
        "id": "5JiAkAiE3qUo"
      },
      "id": "5JiAkAiE3qUo"
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = duplicate_minor_classes(df, df.incident_type)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWMj2hzMHL9L",
        "outputId": "585fae31-c1c8-416b-cf29-33b942fd6a77"
      },
      "id": "XWMj2hzMHL9L",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 180/180 [4:34:36<00:00, 91.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.83      0.78        24\n",
            "           1       1.00      1.00      1.00         3\n",
            "           2       0.92      0.69      0.79        16\n",
            "           3       0.50      0.25      0.33         4\n",
            "           4       0.67      1.00      0.80         2\n",
            "           5       0.62      0.78      0.69        23\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       0.88      0.80      0.84        64\n",
            "           8       0.70      0.53      0.60        30\n",
            "           9       0.67      1.00      0.80         2\n",
            "          10       0.75      1.00      0.86         6\n",
            "          11       0.51      0.66      0.57        35\n",
            "\n",
            "    accuracy                           0.71       214\n",
            "   macro avg       0.66      0.71      0.67       214\n",
            "weighted avg       0.72      0.71      0.71       214\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.sort_values(by=['F1 Mean', \"F1 Std\"], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "sHHmhAIcqDQg",
        "outputId": "43aa0547-0e8f-4e60-9ff5-86c0ece50481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "id": "sHHmhAIcqDQg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Model Vectorizer            Sampler  Accuracy Mean  \\\n",
              "64   GradientBoostingClassifier      Count              SMOTE       0.718129   \n",
              "74   GradientBoostingClassifier      Count   Borderline-SMOTE       0.711111   \n",
              "84   GradientBoostingClassifier      Count             ADASYN       0.712281   \n",
              "114  GradientBoostingClassifier      Count        SMOTE-Tomek       0.702924   \n",
              "94   GradientBoostingClassifier      Count  RandomOversampler       0.695906   \n",
              "54   GradientBoostingClassifier      TFIDF        SMOTE-Tomek       0.683041   \n",
              "34   GradientBoostingClassifier      TFIDF  RandomOversampler       0.684211   \n",
              "4    GradientBoostingClassifier      TFIDF              SMOTE       0.676023   \n",
              "24   GradientBoostingClassifier      TFIDF             ADASYN       0.677193   \n",
              "14   GradientBoostingClassifier      TFIDF   Borderline-SMOTE       0.678363   \n",
              "\n",
              "     Accuracy Std  Recall Mean  Recall Std  Precision Mean  Precision Std  \\\n",
              "64       0.050224     0.718129    0.050224        0.722637       0.054595   \n",
              "74       0.039247     0.711111    0.039247        0.720117       0.038706   \n",
              "84       0.050360     0.712281    0.050360        0.717498       0.053334   \n",
              "114      0.046549     0.702924    0.046549        0.707647       0.050819   \n",
              "94       0.034696     0.695906    0.034696        0.716885       0.038906   \n",
              "54       0.044598     0.683041    0.044598        0.727463       0.046621   \n",
              "34       0.024253     0.684211    0.024253        0.702683       0.025199   \n",
              "4        0.026824     0.676023    0.026824        0.711289       0.030520   \n",
              "24       0.027529     0.677193    0.027529        0.707741       0.027595   \n",
              "14       0.035859     0.678363    0.035859        0.708378       0.030483   \n",
              "\n",
              "      F1 Mean    F1 Std  \n",
              "64   0.712874  0.053408  \n",
              "74   0.707874  0.040774  \n",
              "84   0.707082  0.052112  \n",
              "114  0.697597  0.048697  \n",
              "94   0.695975  0.038387  \n",
              "54   0.687537  0.046580  \n",
              "34   0.684024  0.024986  \n",
              "4    0.678414  0.028567  \n",
              "24   0.678114  0.029618  \n",
              "14   0.677870  0.038310  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96659097-8a99-473e-9ea0-2948a69d87eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>Sampler</th>\n",
              "      <th>Accuracy Mean</th>\n",
              "      <th>Accuracy Std</th>\n",
              "      <th>Recall Mean</th>\n",
              "      <th>Recall Std</th>\n",
              "      <th>Precision Mean</th>\n",
              "      <th>Precision Std</th>\n",
              "      <th>F1 Mean</th>\n",
              "      <th>F1 Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>SMOTE</td>\n",
              "      <td>0.718129</td>\n",
              "      <td>0.050224</td>\n",
              "      <td>0.718129</td>\n",
              "      <td>0.050224</td>\n",
              "      <td>0.722637</td>\n",
              "      <td>0.054595</td>\n",
              "      <td>0.712874</td>\n",
              "      <td>0.053408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>Borderline-SMOTE</td>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.039247</td>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.039247</td>\n",
              "      <td>0.720117</td>\n",
              "      <td>0.038706</td>\n",
              "      <td>0.707874</td>\n",
              "      <td>0.040774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>ADASYN</td>\n",
              "      <td>0.712281</td>\n",
              "      <td>0.050360</td>\n",
              "      <td>0.712281</td>\n",
              "      <td>0.050360</td>\n",
              "      <td>0.717498</td>\n",
              "      <td>0.053334</td>\n",
              "      <td>0.707082</td>\n",
              "      <td>0.052112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>SMOTE-Tomek</td>\n",
              "      <td>0.702924</td>\n",
              "      <td>0.046549</td>\n",
              "      <td>0.702924</td>\n",
              "      <td>0.046549</td>\n",
              "      <td>0.707647</td>\n",
              "      <td>0.050819</td>\n",
              "      <td>0.697597</td>\n",
              "      <td>0.048697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>RandomOversampler</td>\n",
              "      <td>0.695906</td>\n",
              "      <td>0.034696</td>\n",
              "      <td>0.695906</td>\n",
              "      <td>0.034696</td>\n",
              "      <td>0.716885</td>\n",
              "      <td>0.038906</td>\n",
              "      <td>0.695975</td>\n",
              "      <td>0.038387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>SMOTE-Tomek</td>\n",
              "      <td>0.683041</td>\n",
              "      <td>0.044598</td>\n",
              "      <td>0.683041</td>\n",
              "      <td>0.044598</td>\n",
              "      <td>0.727463</td>\n",
              "      <td>0.046621</td>\n",
              "      <td>0.687537</td>\n",
              "      <td>0.046580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>RandomOversampler</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.024253</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.024253</td>\n",
              "      <td>0.702683</td>\n",
              "      <td>0.025199</td>\n",
              "      <td>0.684024</td>\n",
              "      <td>0.024986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>SMOTE</td>\n",
              "      <td>0.676023</td>\n",
              "      <td>0.026824</td>\n",
              "      <td>0.676023</td>\n",
              "      <td>0.026824</td>\n",
              "      <td>0.711289</td>\n",
              "      <td>0.030520</td>\n",
              "      <td>0.678414</td>\n",
              "      <td>0.028567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>ADASYN</td>\n",
              "      <td>0.677193</td>\n",
              "      <td>0.027529</td>\n",
              "      <td>0.677193</td>\n",
              "      <td>0.027529</td>\n",
              "      <td>0.707741</td>\n",
              "      <td>0.027595</td>\n",
              "      <td>0.678114</td>\n",
              "      <td>0.029618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>Borderline-SMOTE</td>\n",
              "      <td>0.678363</td>\n",
              "      <td>0.035859</td>\n",
              "      <td>0.678363</td>\n",
              "      <td>0.035859</td>\n",
              "      <td>0.708378</td>\n",
              "      <td>0.030483</td>\n",
              "      <td>0.677870</td>\n",
              "      <td>0.038310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96659097-8a99-473e-9ea0-2948a69d87eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96659097-8a99-473e-9ea0-2948a69d87eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96659097-8a99-473e-9ea0-2948a69d87eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1031d6e5-9248-416c-b59a-3b98a24bf272\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1031d6e5-9248-416c-b59a-3b98a24bf272')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1031d6e5-9248-416c-b59a-3b98a24bf272 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"GradientBoostingClassifier\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vectorizer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"TFIDF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sampler\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Borderline-SMOTE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016188024629324994,\n        \"min\": 0.6760233918128655,\n        \"max\": 0.7181286549707602,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6771929824561403\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009771890250463664,\n        \"min\": 0.024253147781669863,\n        \"max\": 0.050360351208446764,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.027528894259508344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016188024629324994,\n        \"min\": 0.6760233918128655,\n        \"max\": 0.7181286549707602,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6771929824561403\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009771890250463664,\n        \"min\": 0.024253147781669863,\n        \"max\": 0.050360351208446764,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.027528894259508344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007886957089541772,\n        \"min\": 0.7026829465350568,\n        \"max\": 0.7274628477686632,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.707741132885196\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.011090186557508956,\n        \"min\": 0.025198879888343875,\n        \"max\": 0.05459460053374582,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.02759457695202868\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013405785894073952,\n        \"min\": 0.6778704028329686,\n        \"max\": 0.7128737036885326,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.678114167259271\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010084079889067583,\n        \"min\": 0.02498554943047707,\n        \"max\": 0.05340847869831244,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.029617763558057127\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = Representations(df).representation_a(df)\n",
        "temp = pd.concat([X, y], axis=1)\n",
        "X, y = duplicate_minor_classes(temp.clean_events_sequence, temp.incident_type)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "_aqzrdn9ukm_",
        "outputId": "c2f6621d-a227-4b88-aa5e-35a3f073fef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "_aqzrdn9ukm_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 31%|       | 56/180 [1:40:43<7:23:11, 214.45s/it] "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "3_juRB6oN_xb"
      },
      "id": "3_juRB6oN_xb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_before, df_after = Representations(df).representation_b(df)\n",
        "X, y = pd.concat([df_before.events_sequence, df_after.events_sequence]), pd.concat([df_before['class'], df_after['class']])\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "razuBz2VOucS"
      },
      "id": "razuBz2VOucS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "eVEC5TGedBmj"
      },
      "id": "eVEC5TGedBmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = Representations(df).representation_c(df, sequence_length=100)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "A7frHCFPeHhB"
      },
      "id": "A7frHCFPeHhB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "ljaZ-7XPdDtn"
      },
      "id": "ljaZ-7XPdDtn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiQypjdvdf2N"
      },
      "id": "wiQypjdvdf2N",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}