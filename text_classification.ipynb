{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stef4k/train-maintenance-data-mining/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee",
      "metadata": {
        "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee"
      },
      "source": [
        "# Text classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9",
      "metadata": {
        "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "\n",
        "#from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score, LeaveOneOut\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline\n",
        "from sklearn.metrics import make_scorer, f1_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58",
      "metadata": {
        "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58"
      },
      "source": [
        "Manually remove the first ';' from the first row in csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
      "metadata": {
        "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
        "outputId": "610172fa-b252-4a11-da0b-8b0fdb42eb92"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incident_id</th>\n",
              "      <th>vehicles_sequence</th>\n",
              "      <th>events_sequence</th>\n",
              "      <th>seconds_to_incident_sequence</th>\n",
              "      <th>approx_lat</th>\n",
              "      <th>approx_lon</th>\n",
              "      <th>train_kph_sequence</th>\n",
              "      <th>dj_ac_state_sequence</th>\n",
              "      <th>dj_dc_state_sequence</th>\n",
              "      <th>incident_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>525</th>\n",
              "      <td>4457555</td>\n",
              "      <td>[604, 604, 604, 604, 604, 604, 604, 604, 604, ...</td>\n",
              "      <td>[2434, 4002, 4032, 2852, 4110, 2854, 4028, 402...</td>\n",
              "      <td>[-13421, -13421, -13421, -13418, -13418, -1341...</td>\n",
              "      <td>50.936962</td>\n",
              "      <td>5.311587</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473</th>\n",
              "      <td>4455349</td>\n",
              "      <td>[702, 702, 702, 702, 702, 702, 702, 702, 702, ...</td>\n",
              "      <td>[4066, 4068, 4124, 3634, 2682, 3620, 4148, 412...</td>\n",
              "      <td>[-12509, -12509, -11526, -11525, -11522, -1152...</td>\n",
              "      <td>50.903678</td>\n",
              "      <td>4.388016</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     incident_id                                  vehicles_sequence  \\\n",
              "525      4457555  [604, 604, 604, 604, 604, 604, 604, 604, 604, ...   \n",
              "473      4455349  [702, 702, 702, 702, 702, 702, 702, 702, 702, ...   \n",
              "\n",
              "                                       events_sequence  \\\n",
              "525  [2434, 4002, 4032, 2852, 4110, 2854, 4028, 402...   \n",
              "473  [4066, 4068, 4124, 3634, 2682, 3620, 4148, 412...   \n",
              "\n",
              "                          seconds_to_incident_sequence  approx_lat  \\\n",
              "525  [-13421, -13421, -13421, -13418, -13418, -1341...   50.936962   \n",
              "473  [-12509, -12509, -11526, -11525, -11522, -1152...   50.903678   \n",
              "\n",
              "     approx_lon                                 train_kph_sequence  \\\n",
              "525    5.311587  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "473    4.388016  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "\n",
              "                                  dj_ac_state_sequence  \\\n",
              "525  [False, False, False, False, False, False, Fal...   \n",
              "473  [False, False, False, False, False, False, Fal...   \n",
              "\n",
              "                                  dj_dc_state_sequence  incident_type  \n",
              "525  [False, False, False, False, False, False, Fal...             99  \n",
              "473  [True, True, True, True, True, True, True, Tru...             99  "
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';')\n",
        "df.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16737a51-b8eb-4428-b351-51113364e62c",
      "metadata": {
        "id": "16737a51-b8eb-4428-b351-51113364e62c"
      },
      "source": [
        "Now I will analyze the percentage of each event type appearing at least once in an event sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d70f1d49-bf17-4c26-96c9-cea06d26c642",
      "metadata": {
        "id": "d70f1d49-bf17-4c26-96c9-cea06d26c642"
      },
      "outputs": [],
      "source": [
        "events_types_dict = {}\n",
        "for events_sequence in df['events_sequence']:\n",
        "    row_list = ast.literal_eval(events_sequence) #transforming string into actual list\n",
        "    unique_events = set(row_list)\n",
        "    for event in unique_events:\n",
        "        if not events_types_dict.get(event):\n",
        "            events_types_dict[event] = 0\n",
        "        events_types_dict[event] += 1\n",
        "sorted_dict = dict(sorted(events_types_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "# Convert the sorted dictionary to a DataFrame\n",
        "sorted_events_perc_df = pd.DataFrame(list(sorted_dict.items()), columns=['event_type', 'frequency'])\n",
        "sorted_events_perc_df['percentage'] = sorted_events_perc_df['frequency'] / df.shape[0] * 100\n",
        "# Cast the 'event_type' column to string\n",
        "sorted_events_perc_df['event_type'] = sorted_events_perc_df['event_type'].astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18cbe400-a4a2-4625-92ad-3c527827e5e7",
      "metadata": {
        "id": "18cbe400-a4a2-4625-92ad-3c527827e5e7"
      },
      "source": [
        "We save in a list all event codes that appear in less than 85% of the event sequences:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba05e687-439c-4f60-a78e-681a6cd75090",
      "metadata": {
        "id": "ba05e687-439c-4f60-a78e-681a6cd75090"
      },
      "outputs": [],
      "source": [
        "events_low_frequency = list(map(int, list(sorted_events_perc_df[sorted_events_perc_df.percentage<=85].event_type)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea15edbc-88df-41fa-b9d1-7099d50a6617",
      "metadata": {
        "id": "ea15edbc-88df-41fa-b9d1-7099d50a6617"
      },
      "source": [
        "## Text preprocessing\n",
        "Before we start with text classification we need to clean the sequences of events. As seen one value of `events_sequence` contains commas and brackets even though it is a string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb37149f-424d-4374-b560-e330b3942c6e",
      "metadata": {
        "id": "cb37149f-424d-4374-b560-e330b3942c6e",
        "outputId": "3377afe2-eec9-454a-8a0f-04f6182a8f63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'[2744, 4004, 2852, 4110, 2854, 4396, 1132, 4140, 4148, 2708, 4026, 1032, 1082, 4152, 4030, 4018, 4168, 4156, 4394, 152, 2742, 4410, 4406, 4068, 4408, 4412, 4066, 2744, 4026, 4148, 4168, 4140, 3986, 2744, 4002, 2852, 4110, 2854, 4148, 2708, 4026, 4140, 4152, 4030, 4018, 4140, 4168, 4156, 2852, 2854, 4124, 2858, 2658, 2688, 3254, 3254, 3254, 2970, 4082, 4090, 4092, 2982, 3236, 4100, 2702, 4394, 1250, 2970, 2980, 2970, 2980, 2970, 2982, 2970, 2982, 4168, 4140, 3986, 2742, 4004, 2852, 4110, 2854, 2982, 2708, 4026, 4030, 4018, 4148, 4140, 4152, 4168, 4156, 4120, 2858, 2658, 2688, 3254, 3254, 2970, 2982, 2708, 2970, 2982, 4100, 2702, 1250, 4394, 2744, 4026, 4148, 2970, 2980, 4168, 4140, 4168, 3986, 2744, 4002, 2852, 4110, 2854, 2980, 2708, 4026, 4148, 2552, 4168, 4140, 4152, 4030, 4018, 4026, 4140, 4168, 4156, 2970, 2982, 2708, 2970, 4082, 4092, 4090, 4084, 4094, 4090, 3236, 2982, 4100, 2702, 1250, 4394, 4168, 4140, 3986, 2744, 4004, 2852, 4110, 2854, 2982, 2708, 4026, 4140, 4030, 4018, 4140, 4140, 2552, 4168, 4140, 4148, 4140, 4140, 4152, 4168, 4156, 2708, 2970, 4082, 4092, 4090, 4084, 4094, 4090, 3236, 2982, 4066, 2708, 2708, 3082, 4394, 3086, 1286, 1720, 1740, 1760, 1780, 4396, 1286, 2652, 4094, 2742, 4026, 4148, 2708, 3036, 4394, 4168, 4140, 3986, 2744, 4002, 2852, 4110, 2854, 2982, 4148, 2708, 4026, 4140, 4152, 4168, 4030, 4018, 4156, 4406, 4410, 4408, 4412, 2980, 2980, 2970, 3492, 4066, 4068, 4396, 2980, 2708, 2970, 2980, 2970, 2980, 2970, 2980, 2744, 4148, 2970, 2980, 2970, 2980, 4124, 3224, 2690, 3224, 2690, 3224, 2690, 3224, 2690, 4126, 3224, 2690, 2684, 2846, 4124, 3224, 4022, 3032, 4394, 2654, 2708, 4392, 1200, 1202, 2652, 3260, 4092, 2708, 2980, 4396, 1286, 3132, 4394, 4396, 1286, 2652, 2654, 2708, 3082, 4392, 4394, 1200, 1202, 2708, 4394, 1286, 1720, 1740, 1760, 1780, 4396, 1286, 2652, 4094, 2708, 4124, 4072, 2970, 2982, 2708, 2970, 4082, 4090, 4092, 4084, 4094, 4090, 3236, 2974, 4100, 4124, 2708, 2970, 2980, 2970, 4082, 4092, 4090, 4084, 4094, 4090, 3236, 2982, 4100, 2702, 4394, 1250, 2708, 2708, 3082, 1286, 3082, 1720, 1740, 1760, 1780, 1286, 2652, 3260, 4094, 3082, 3086, 1286, 1286, 1720, 1740, 1780, 2652, 3260, 4094, 2708, 2970, 4396, 4082, 4092, 4090, 4084, 4090, 4094, 3236, 2974, 4100, 2708, 2970, 4082, 4090, 4092, 4084, 4090, 4094, 2988, 3236, 4100, 2702, 4394, 1250, 2970, 4396, 2980, 2970, 4082, 4092, 4090, 4084, 4090, 4094, 3236, 2974, 4100, 2708]'"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.events_sequence.iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2183bcea-5226-48ff-a693-7bdd70cb6e21",
      "metadata": {
        "id": "2183bcea-5226-48ff-a693-7bdd70cb6e21"
      },
      "source": [
        "Also, as observed before some event types are so common they do not actually bring a lot of value (as mentioned in the paper as well). We remove those common event types\n",
        "\n",
        "The steps to clean the event sequences are:\n",
        "- keep non-common event types mentioned in list `events_low_frequency`\n",
        "- remove symbols: [] , and store sequences of events as a string without brackets and commas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b337c2ce-e0fd-49ea-b2e3-a8e71171f7ca",
      "metadata": {
        "id": "b337c2ce-e0fd-49ea-b2e3-a8e71171f7ca"
      },
      "outputs": [],
      "source": [
        "df['clean_events_sequence'] = df.events_sequence.apply(ast.literal_eval).apply(lambda x: [i for i in x if i in events_low_frequency]).astype(str)\\\n",
        "                .replace(r'[\\[\\],]', '', regex=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14c7dba5-3363-4b8b-95bc-6ea16f64db3b",
      "metadata": {
        "id": "14c7dba5-3363-4b8b-95bc-6ea16f64db3b"
      },
      "source": [
        "## Text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b18e3f2d-8402-4608-bddc-696b656096be",
      "metadata": {
        "id": "b18e3f2d-8402-4608-bddc-696b656096be"
      },
      "source": [
        "Now we try to experiment using text techniques to transform the list events sequence:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac4ace38-2df1-45ac-b4ac-ca9179985518",
      "metadata": {
        "id": "ac4ace38-2df1-45ac-b4ac-ca9179985518"
      },
      "outputs": [],
      "source": [
        "target = df['incident_type'].copy() # target column separated\n",
        "X_train, X_test, y_train, y_test = train_test_split(df.clean_events_sequence, target, test_size=0.2,  random_state=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "951515d0-d264-4008-a0b9-862d4de245b9",
      "metadata": {
        "id": "951515d0-d264-4008-a0b9-862d4de245b9"
      },
      "source": [
        "Since the dataset is imbalanced we will use different strategies to battle that. Here we set a new sampling strategy based on a basic script:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1491c584-b6ef-412d-a3bf-f22fb731fa03",
      "metadata": {
        "id": "1491c584-b6ef-412d-a3bf-f22fb731fa03",
        "outputId": "eb303c48-0c0e-4222-90c4-393b38844fd5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{99: 167,\n",
              " 14: 155,\n",
              " 2: 135,\n",
              " 9: 135,\n",
              " 4: 105,\n",
              " 11: 58,\n",
              " 17: 47,\n",
              " 6: 45,\n",
              " 3: 43,\n",
              " 7: 43,\n",
              " 16: 43}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define custom sampling strategy based on class distribution\n",
        "# Each non-majority class will have equal samples to 15% of the majority class plus their previous samples\n",
        "class_counts = y_train.value_counts()\n",
        "max_class_count = max(class_counts.values)\n",
        "sampling_strategy = {class_counts.index[i]: int(max_class_count * 0.15) + class_counts.values[i]\n",
        "                     for i in range(len(y_train.value_counts().index)) if class_counts.values[i] < max_class_count}\n",
        "sampling_strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e89063fd-fac4-49af-bac6-4e7556984ea9",
      "metadata": {
        "id": "e89063fd-fac4-49af-bac6-4e7556984ea9"
      },
      "source": [
        "Starting with CountVectorizer:\n",
        "- Tokenization: Splits text into individual words (tokens).\n",
        "- Builds a Vocabulary: Creates a dictionary of unique words (tokens) from the entire corpus.\n",
        "- Counts the Occurrence: Calculates the frequency (count) of each word in each document.\n",
        "- Transforms Text into a Sparse Matrix: Returns a matrix of shape (n_samples, n_features), where n_samples is the number of documents and n_features is the number of unique words in the vocabulary.\n",
        "\n",
        "  We firstly set the sampling strategy for SMOTE:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd228c63-bcf0-4b11-be54-39aebd1dc739",
      "metadata": {
        "id": "bd228c63-bcf0-4b11-be54-39aebd1dc739"
      },
      "source": [
        "Now we set the pipeline to be used:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "430b9fe4-4f4b-40d2-976b-06916ce5688d",
      "metadata": {
        "id": "430b9fe4-4f4b-40d2-976b-06916ce5688d"
      },
      "outputs": [],
      "source": [
        "text_clf = Pipeline([\n",
        "                    ('vect', CountVectorizer()),\n",
        "                     #('decision_tree', DecisionTreeClassifier()),\n",
        "                    ('smote', SMOTE(sampling_strategy=sampling_strategy, random_state=1, k_neighbors=2)),\n",
        "                    ('extra_trees', ExtraTreesClassifier()),\n",
        "                    #('random_forest', RandomForestClassifier())\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "199354cb-7476-434b-92fd-f5ba688b62ce",
      "metadata": {
        "id": "199354cb-7476-434b-92fd-f5ba688b62ce"
      },
      "source": [
        "Training the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09d346e4-75de-46e1-aa17-19cd8834e324",
      "metadata": {
        "id": "09d346e4-75de-46e1-aa17-19cd8834e324",
        "outputId": "ad1ccee0-3e2d-4a60-b8dd-9d850c876ec6"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {\n",
              "  /* Definition of color scheme common for light and dark mode */\n",
              "  --sklearn-color-text: black;\n",
              "  --sklearn-color-line: gray;\n",
              "  /* Definition of color scheme for unfitted estimators */\n",
              "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
              "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
              "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
              "  --sklearn-color-unfitted-level-3: chocolate;\n",
              "  /* Definition of color scheme for fitted estimators */\n",
              "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
              "  --sklearn-color-fitted-level-1: #d4ebff;\n",
              "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
              "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
              "\n",
              "  /* Specific color for light theme */\n",
              "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
              "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
              "  --sklearn-color-icon: #696969;\n",
              "\n",
              "  @media (prefers-color-scheme: dark) {\n",
              "    /* Redefinition of color scheme for dark theme */\n",
              "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
              "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
              "    --sklearn-color-icon: #878787;\n",
              "  }\n",
              "}\n",
              "\n",
              "#sk-container-id-1 {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 pre {\n",
              "  padding: 0;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-hidden--visually {\n",
              "  border: 0;\n",
              "  clip: rect(1px 1px 1px 1px);\n",
              "  clip: rect(1px, 1px, 1px, 1px);\n",
              "  height: 1px;\n",
              "  margin: -1px;\n",
              "  overflow: hidden;\n",
              "  padding: 0;\n",
              "  position: absolute;\n",
              "  width: 1px;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-dashed-wrapped {\n",
              "  border: 1px dashed var(--sklearn-color-line);\n",
              "  margin: 0 0.4em 0.5em 0.4em;\n",
              "  box-sizing: border-box;\n",
              "  padding-bottom: 0.4em;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-container {\n",
              "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
              "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
              "     so we also need the `!important` here to be able to override the\n",
              "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
              "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
              "  display: inline-block !important;\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-text-repr-fallback {\n",
              "  display: none;\n",
              "}\n",
              "\n",
              "div.sk-parallel-item,\n",
              "div.sk-serial,\n",
              "div.sk-item {\n",
              "  /* draw centered vertical line to link estimators */\n",
              "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
              "  background-size: 2px 100%;\n",
              "  background-repeat: no-repeat;\n",
              "  background-position: center center;\n",
              "}\n",
              "\n",
              "/* Parallel-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item::after {\n",
              "  content: \"\";\n",
              "  width: 100%;\n",
              "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
              "  flex-grow: 1;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel {\n",
              "  display: flex;\n",
              "  align-items: stretch;\n",
              "  justify-content: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  position: relative;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
              "  align-self: flex-end;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
              "  align-self: flex-start;\n",
              "  width: 50%;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
              "  width: 0;\n",
              "}\n",
              "\n",
              "/* Serial-specific style estimator block */\n",
              "\n",
              "#sk-container-id-1 div.sk-serial {\n",
              "  display: flex;\n",
              "  flex-direction: column;\n",
              "  align-items: center;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  padding-right: 1em;\n",
              "  padding-left: 1em;\n",
              "}\n",
              "\n",
              "\n",
              "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
              "clickable and can be expanded/collapsed.\n",
              "- Pipeline and ColumnTransformer use this feature and define the default style\n",
              "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
              "*/\n",
              "\n",
              "/* Pipeline and ColumnTransformer style (default) */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable {\n",
              "  /* Default theme specific background. It is overwritten whether we have a\n",
              "  specific estimator or a Pipeline/ColumnTransformer */\n",
              "  background-color: var(--sklearn-color-background);\n",
              "}\n",
              "\n",
              "/* Toggleable label */\n",
              "#sk-container-id-1 label.sk-toggleable__label {\n",
              "  cursor: pointer;\n",
              "  display: block;\n",
              "  width: 100%;\n",
              "  margin-bottom: 0;\n",
              "  padding: 0.5em;\n",
              "  box-sizing: border-box;\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
              "  /* Arrow on the left of the label */\n",
              "  content: \"▸\";\n",
              "  float: left;\n",
              "  margin-right: 0.25em;\n",
              "  color: var(--sklearn-color-icon);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
              "  color: var(--sklearn-color-text);\n",
              "}\n",
              "\n",
              "/* Toggleable content - dropdown */\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content {\n",
              "  max-height: 0;\n",
              "  max-width: 0;\n",
              "  overflow: hidden;\n",
              "  text-align: left;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content pre {\n",
              "  margin: 0.2em;\n",
              "  border-radius: 0.25em;\n",
              "  color: var(--sklearn-color-text);\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
              "  /* Expand drop-down */\n",
              "  max-height: 200px;\n",
              "  max-width: 100%;\n",
              "  overflow: auto;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
              "  content: \"▾\";\n",
              "}\n",
              "\n",
              "/* Pipeline/ColumnTransformer-specific style */\n",
              "\n",
              "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator-specific style */\n",
              "\n",
              "/* Colorize estimator box */\n",
              "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  /* The background is the default theme color */\n",
              "  color: var(--sklearn-color-text-on-default-background);\n",
              "}\n",
              "\n",
              "/* On hover, darken the color of the background */\n",
              "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "/* Label box, darken color on hover, fitted */\n",
              "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
              "  color: var(--sklearn-color-text);\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Estimator label */\n",
              "\n",
              "#sk-container-id-1 div.sk-label label {\n",
              "  font-family: monospace;\n",
              "  font-weight: bold;\n",
              "  display: inline-block;\n",
              "  line-height: 1.2em;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-label-container {\n",
              "  text-align: center;\n",
              "}\n",
              "\n",
              "/* Estimator-specific */\n",
              "#sk-container-id-1 div.sk-estimator {\n",
              "  font-family: monospace;\n",
              "  border: 1px dotted var(--sklearn-color-border-box);\n",
              "  border-radius: 0.25em;\n",
              "  box-sizing: border-box;\n",
              "  margin-bottom: 0.5em;\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-0);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-0);\n",
              "}\n",
              "\n",
              "/* on hover */\n",
              "#sk-container-id-1 div.sk-estimator:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-2);\n",
              "}\n",
              "\n",
              "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-2);\n",
              "}\n",
              "\n",
              "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
              "\n",
              "/* Common style for \"i\" and \"?\" */\n",
              "\n",
              ".sk-estimator-doc-link,\n",
              "a:link.sk-estimator-doc-link,\n",
              "a:visited.sk-estimator-doc-link {\n",
              "  float: right;\n",
              "  font-size: smaller;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1em;\n",
              "  height: 1em;\n",
              "  width: 1em;\n",
              "  text-decoration: none !important;\n",
              "  margin-left: 1ex;\n",
              "  /* unfitted */\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted,\n",
              "a:link.sk-estimator-doc-link.fitted,\n",
              "a:visited.sk-estimator-doc-link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
              ".sk-estimator-doc-link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover,\n",
              "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
              ".sk-estimator-doc-link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "/* Span, style for the box shown on hovering the info icon */\n",
              ".sk-estimator-doc-link span {\n",
              "  display: none;\n",
              "  z-index: 9999;\n",
              "  position: relative;\n",
              "  font-weight: normal;\n",
              "  right: .2ex;\n",
              "  padding: .5ex;\n",
              "  margin: .5ex;\n",
              "  width: min-content;\n",
              "  min-width: 20ex;\n",
              "  max-width: 50ex;\n",
              "  color: var(--sklearn-color-text);\n",
              "  box-shadow: 2pt 2pt 4pt #999;\n",
              "  /* unfitted */\n",
              "  background: var(--sklearn-color-unfitted-level-0);\n",
              "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link.fitted span {\n",
              "  /* fitted */\n",
              "  background: var(--sklearn-color-fitted-level-0);\n",
              "  border: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "\n",
              ".sk-estimator-doc-link:hover span {\n",
              "  display: block;\n",
              "}\n",
              "\n",
              "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link {\n",
              "  float: right;\n",
              "  font-size: 1rem;\n",
              "  line-height: 1em;\n",
              "  font-family: monospace;\n",
              "  background-color: var(--sklearn-color-background);\n",
              "  border-radius: 1rem;\n",
              "  height: 1rem;\n",
              "  width: 1rem;\n",
              "  text-decoration: none;\n",
              "  /* unfitted */\n",
              "  color: var(--sklearn-color-unfitted-level-1);\n",
              "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
              "  /* fitted */\n",
              "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
              "  color: var(--sklearn-color-fitted-level-1);\n",
              "}\n",
              "\n",
              "/* On hover */\n",
              "#sk-container-id-1 a.estimator_doc_link:hover {\n",
              "  /* unfitted */\n",
              "  background-color: var(--sklearn-color-unfitted-level-3);\n",
              "  color: var(--sklearn-color-background);\n",
              "  text-decoration: none;\n",
              "}\n",
              "\n",
              "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
              "  /* fitted */\n",
              "  background-color: var(--sklearn-color-fitted-level-3);\n",
              "}\n",
              "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()),\n",
              "                (&#x27;smote&#x27;,\n",
              "                 SMOTE(k_neighbors=2, random_state=1,\n",
              "                       sampling_strategy={2: 135, 3: 43, 4: 105, 6: 45, 7: 43,\n",
              "                                          9: 135, 11: 58, 14: 155, 16: 43,\n",
              "                                          17: 47, 99: 167})),\n",
              "                (&#x27;extra_trees&#x27;, ExtraTreesClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;Pipeline<span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>Pipeline(steps=[(&#x27;vect&#x27;, CountVectorizer()),\n",
              "                (&#x27;smote&#x27;,\n",
              "                 SMOTE(k_neighbors=2, random_state=1,\n",
              "                       sampling_strategy={2: 135, 3: 43, 4: 105, 6: 45, 7: 43,\n",
              "                                          9: 135, 11: 58, 14: 155, 16: 43,\n",
              "                                          17: 47, 99: 167})),\n",
              "                (&#x27;extra_trees&#x27;, ExtraTreesClassifier())])</pre></div> </div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;CountVectorizer<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\">?<span>Documentation for CountVectorizer</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>CountVectorizer()</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">SMOTE</label><div class=\"sk-toggleable__content fitted\"><pre>SMOTE(k_neighbors=2, random_state=1,\n",
              "      sampling_strategy={2: 135, 3: 43, 4: 105, 6: 45, 7: 43, 9: 135, 11: 58,\n",
              "                         14: 155, 16: 43, 17: 47, 99: 167})</pre></div> </div></div><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;ExtraTreesClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html\">?<span>Documentation for ExtraTreesClassifier</span></a></label><div class=\"sk-toggleable__content fitted\"><pre>ExtraTreesClassifier()</pre></div> </div></div></div></div></div></div>"
            ],
            "text/plain": [
              "Pipeline(steps=[('vect', CountVectorizer()),\n",
              "                ('smote',\n",
              "                 SMOTE(k_neighbors=2, random_state=1,\n",
              "                       sampling_strategy={2: 135, 3: 43, 4: 105, 6: 45, 7: 43,\n",
              "                                          9: 135, 11: 58, 14: 155, 16: 43,\n",
              "                                          17: 47, 99: 167})),\n",
              "                ('extra_trees', ExtraTreesClassifier())])"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3273e4d7-fdc3-41ad-b5cf-8af15e8c0d9f",
      "metadata": {
        "id": "3273e4d7-fdc3-41ad-b5cf-8af15e8c0d9f"
      },
      "source": [
        "Print the results for the particular split of test data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63957f6a-e782-4f88-a226-8e41e6ea2c05",
      "metadata": {
        "id": "63957f6a-e782-4f88-a226-8e41e6ea2c05",
        "outputId": "ccbf12ac-4c25-4ae1-94dd-8186ba402f7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           2       0.61      0.87      0.71        23\n",
            "           3       0.00      0.00      0.00         1\n",
            "           4       0.50      0.42      0.45        12\n",
            "           6       0.00      1.00      0.00         0\n",
            "           9       0.61      0.52      0.56        21\n",
            "          11       0.50      0.14      0.22         7\n",
            "          13       0.64      0.79      0.71        57\n",
            "          14       0.66      0.58      0.61        33\n",
            "          17       1.00      0.00      0.00         2\n",
            "          99       0.56      0.47      0.51        47\n",
            "\n",
            "    accuracy                           0.61       203\n",
            "   macro avg       0.51      0.48      0.38       203\n",
            "weighted avg       0.61      0.61      0.59       203\n",
            "\n"
          ]
        }
      ],
      "source": [
        "clf_predict = text_clf.predict(X_test)\n",
        "print(classification_report(y_test, clf_predict, zero_division=1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "131b2808-6a94-4c7e-b5e1-952832511e85",
      "metadata": {
        "id": "131b2808-6a94-4c7e-b5e1-952832511e85"
      },
      "source": [
        "## Cross validation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7dea2210-71e8-4c09-8a20-347a24c61512",
      "metadata": {
        "id": "7dea2210-71e8-4c09-8a20-347a24c61512"
      },
      "source": [
        "Now we calculate the cross validation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb09cd66-9c74-40d7-b566-8789fcac9c48",
      "metadata": {
        "id": "eb09cd66-9c74-40d7-b566-8789fcac9c48"
      },
      "outputs": [],
      "source": [
        "class_counts = target.value_counts()\n",
        "max_class_count = max(class_counts.values)\n",
        "sampling_strategy_cross_val = {class_counts.index[i]: int(max_class_count * 0.15) + class_counts.values[i]\n",
        "                     for i in range(len(y_train.value_counts().index)) if class_counts.values[i] < max_class_count}\n",
        "cross_val_clf = Pipeline([\n",
        "                    ('vect', CountVectorizer()),\n",
        "                    ('smote', SMOTE(sampling_strategy=sampling_strategy_cross_val, random_state=1, k_neighbors=2)),\n",
        "                    ('extra_trees', ExtraTreesClassifier()),\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e64929b-894e-4eab-89a5-8f3511bbf8e2",
      "metadata": {
        "id": "5e64929b-894e-4eab-89a5-8f3511bbf8e2",
        "outputId": "f256a97b-0e15-4102-c8c1-f52e9f7ff6db"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.6409043854696029"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scores = cross_val_score(cross_val_clf, df.clean_events_sequence.sample(frac=1, random_state=1), target.sample(frac=1, random_state=1),\n",
        "                        cv=4, scoring='accuracy',n_jobs = -1)\n",
        "scores.mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9a8558d-4b91-4f10-83fd-bb11738944f9",
      "metadata": {
        "id": "d9a8558d-4b91-4f10-83fd-bb11738944f9"
      },
      "source": [
        "Create a custom scoring f1 function with zero_division parameter for cross validation to avoid nan values:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b9e7a78-c871-4a73-b78c-a061d729ea16",
      "metadata": {
        "id": "4b9e7a78-c871-4a73-b78c-a061d729ea16"
      },
      "outputs": [],
      "source": [
        "# Create a custom scoring function with zero_division parameter\n",
        "def custom_f1_score(y_true, y_pred):\n",
        "    return f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "# Wrap the custom scoring function using make_scorer\n",
        "f1_scorer = make_scorer(custom_f1_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7745109b-673e-4128-9d46-3cdab6b2da93",
      "metadata": {
        "id": "7745109b-673e-4128-9d46-3cdab6b2da93",
        "outputId": "db9c57cf-ad7c-4133-a2e8-a63fc21a047b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.61823676 0.61774323 0.65055584 0.57616849]\n",
            "0.615676079863001\n"
          ]
        }
      ],
      "source": [
        "scores = cross_val_score(cross_val_clf, df.clean_events_sequence.sample(frac=1, random_state=1), target.sample(frac=1, random_state=1),\n",
        "                        cv=4, scoring=f1_scorer,n_jobs = -1)\n",
        "print(scores)\n",
        "print(scores.mean())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d786ce5a-f7ea-4873-ad20-e37fb230e201",
      "metadata": {
        "id": "d786ce5a-f7ea-4873-ad20-e37fb230e201"
      },
      "source": [
        "F1 is calculated as:\n",
        "$$ F1 Score= 2×\\frac{Precision×Recall}{Precision+Recall}\n",
        "​\n",
        "$$\n",
        "There are some minority classes with no correct predictions ($recall=0$) resulting in a null value for the whole f1 score when using a non-custom f1 scorer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091c8078-9125-4d27-9bb7-4324688918bf",
      "metadata": {
        "id": "091c8078-9125-4d27-9bb7-4324688918bf"
      },
      "source": [
        "## GridsearchCV\n",
        "Now we use gridsearchCV to find the optimal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77e22f86-1790-4f29-badb-3baefd9a084d",
      "metadata": {
        "scrolled": true,
        "id": "77e22f86-1790-4f29-badb-3baefd9a084d",
        "outputId": "45d3ded7-70c6-4033-b7e7-bd8e3d77f144"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{99: 222,\n",
              " 14: 196,\n",
              " 2: 166,\n",
              " 9: 164,\n",
              " 4: 125,\n",
              " 11: 73,\n",
              " 17: 57,\n",
              " 6: 53,\n",
              " 3: 52,\n",
              " 16: 51,\n",
              " 7: 51}"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_counts = target.value_counts()\n",
        "max_class_count = max(class_counts.values)\n",
        "sampling_strategy_grid = {class_counts.index[i]: int(max_class_count * 0.15) + class_counts.values[i]\n",
        "                     for i in range(len(y_train.value_counts().index)) if class_counts.values[i] < max_class_count}\n",
        "sampling_strategy_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a495103-d65d-4f38-9dc9-c04afadd49b0",
      "metadata": {
        "id": "6a495103-d65d-4f38-9dc9-c04afadd49b0"
      },
      "outputs": [],
      "source": [
        "grid_clf = Pipeline([\n",
        "                    ('vect', CountVectorizer()),\n",
        "                    ('smote', SMOTE(sampling_strategy=sampling_strategy_grid, random_state=1, k_neighbors=2)),\n",
        "                    ('extra_trees', ExtraTreesClassifier()),\n",
        "                    ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55f0c64e-3389-44a4-8060-68f6b3405a5a",
      "metadata": {
        "id": "55f0c64e-3389-44a4-8060-68f6b3405a5a",
        "outputId": "d37d1752-0dca-4627-9c94-060ffb949066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Parameters: {'extra_trees__max_depth': None, 'extra_trees__n_estimators': 300, 'vect__max_features': 1000, 'vect__ngram_range': (1, 1)}\n",
            "Best Score F1: 0.6406886229518796\n",
            "Accuracy: 0.6548246439550787\n"
          ]
        }
      ],
      "source": [
        "# Define the parameter grid for GridSearchCV 15%\n",
        "param_grid = {\n",
        "    'vect__max_features': [500, 1000],       # Example parameter for CountVectorizer\n",
        "    'vect__ngram_range': [(1, 1), (1, 2), (1,3)],   # Unigrams, bigrams, trigrams\n",
        "    'extra_trees__n_estimators': [100, 200, 300, 400],        # Number of trees in ExtraTrees\n",
        "    'extra_trees__max_depth': [None, 10]        # Depth of each tree\n",
        "}\n",
        "\n",
        "# Cross-validation strategy set here to replicate results\n",
        "cv = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Define GridSearchCV with the pipeline and parameter grid\n",
        "grid_search = GridSearchCV(grid_clf, param_grid, cv=cv, scoring=f1_scorer, n_jobs=-1)\n",
        "\n",
        "# Fit GridSearchCV to the data\n",
        "grid_search.fit(df.clean_events_sequence, target)\n",
        "\n",
        "# Output the best parameters and the best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Score F1:\", grid_search.best_score_)\n",
        "print(\"Accuracy:\", str(np.mean(cross_val_score(grid_search.best_estimator_, df.clean_events_sequence, target, cv=cv, scoring='accuracy'))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc6c7f72",
      "metadata": {
        "id": "cc6c7f72"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "043c4514",
      "metadata": {
        "id": "043c4514"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4b46fda7",
      "metadata": {
        "id": "4b46fda7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "137fe5af",
      "metadata": {
        "id": "137fe5af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd26dd6-800e-46a6-983b-bc8d3e2a9bbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: umap-learn in /usr/local/lib/python3.10/dist-packages (0.5.7)\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn!=0.22.0,>=0.16 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.5.2)\n",
            "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.10/dist-packages (from hmmlearn) (1.13.1)\n",
            "Requirement already satisfied: numba>=0.51.2 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.60.0)\n",
            "Requirement already satisfied: pynndescent>=0.5 in /usr/local/lib/python3.10/dist-packages (from umap-learn) (0.5.13)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from umap-learn) (4.66.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.8.0)\n",
            "Collecting datashader (from umap-learn[plot])\n",
            "  Downloading datashader-0.16.3-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.4.3)\n",
            "Requirement already satisfied: holoviews in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (1.19.1)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.13.2)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.10/dist-packages (from umap-learn[plot]) (0.24.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.2->umap-learn) (0.43.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pynndescent>=0.5->umap-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.22.0,>=0.16->hmmlearn) (3.5.0)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (3.1.4)\n",
            "Requirement already satisfied: contourpy>=1.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (1.3.0)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (24.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (10.4.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (6.0.2)\n",
            "Requirement already satisfied: tornado>=6.2 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (6.3.3)\n",
            "Requirement already satisfied: xyzservices>=2021.09.1 in /usr/local/lib/python3.10/dist-packages (from bokeh->umap-learn[plot]) (2024.9.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->umap-learn[plot]) (2024.2)\n",
            "Requirement already satisfied: dask in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2024.10.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (1.0.0)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2.1.1)\n",
            "Collecting pyct (from datashader->umap-learn[plot])\n",
            "  Downloading pyct-0.5.0-py2.py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2.32.3)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: xarray in /usr/local/lib/python3.10/dist-packages (from datashader->umap-learn[plot]) (2024.10.0)\n",
            "Requirement already satisfied: panel>=1.0 in /usr/local/lib/python3.10/dist-packages (from holoviews->umap-learn[plot]) (1.4.5)\n",
            "Requirement already satisfied: pyviz-comms>=2.1 in /usr/local/lib/python3.10/dist-packages (from holoviews->umap-learn[plot]) (3.0.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->umap-learn[plot]) (3.2.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (3.4.2)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (2.36.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (2024.9.20)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image->umap-learn[plot]) (0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.9->bokeh->umap-learn[plot]) (3.0.2)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (3.7)\n",
            "Requirement already satisfied: markdown-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (3.0.0)\n",
            "Requirement already satisfied: linkify-it-py in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (2.0.3)\n",
            "Requirement already satisfied: mdit-py-plugins in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (0.4.2)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (6.2.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from panel>=1.0->holoviews->umap-learn[plot]) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->umap-learn[plot]) (1.16.0)\n",
            "Requirement already satisfied: click>=8.1 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (3.1.0)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (2024.10.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (1.4.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask->datashader->umap-learn[plot]) (8.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->datashader->umap-learn[plot]) (2024.8.30)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask->datashader->umap-learn[plot]) (3.20.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.10/dist-packages (from partd>=1.4.0->dask->datashader->umap-learn[plot]) (1.0.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->panel>=1.0->holoviews->umap-learn[plot]) (0.5.1)\n",
            "Requirement already satisfied: uc-micro-py in /usr/local/lib/python3.10/dist-packages (from linkify-it-py->panel>=1.0->holoviews->umap-learn[plot]) (1.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py->panel>=1.0->holoviews->umap-learn[plot]) (0.1.2)\n",
            "Downloading datashader-0.16.3-py2.py3-none-any.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyct-0.5.0-py2.py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyct, datashader\n",
            "Successfully installed datashader-0.16.3 pyct-0.5.0\n"
          ]
        }
      ],
      "source": [
        "! pip install hmmlearn umap-learn umap-learn[plot]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a85fd88",
      "metadata": {
        "id": "8a85fd88"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
        "from gensim.models import Word2Vec\n",
        "from hmmlearn import hmm\n",
        "import umap\n",
        "import umap.plot\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';')\n",
        "df.sample(5)"
      ],
      "metadata": {
        "id": "cnXTFBgEyfAS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "outputId": "5c1da27d-21a0-464b-a531-40b1e08d5f0d"
      },
      "id": "cnXTFBgEyfAS",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0  incident_id  \\\n",
              "674         674      4464669   \n",
              "861         861      4602871   \n",
              "495         495      4456317   \n",
              "216         216      4443009   \n",
              "26           26      4434195   \n",
              "\n",
              "                                     vehicles_sequence  \\\n",
              "674  [506, 506, 506, 506, 506, 506, 506, 506, 506, ...   \n",
              "861  [549, 549, 549, 549, 549, 549, 549, 549, 549, ...   \n",
              "495  [1087, 1087, 1087, 1087, 1087, 1087, 1087, 108...   \n",
              "216  [1020, 1020, 1020, 1020, 1020, 1020, 1020, 102...   \n",
              "26   [511, 511, 511, 511, 511, 511, 511, 511, 511, ...   \n",
              "\n",
              "                                       events_sequence  \\\n",
              "674  [4016, 2744, 4148, 2708, 4026, 4020, 4168, 414...   \n",
              "861  [4120, 3636, 3658, 2956, 2956, 2956, 2956, 295...   \n",
              "495  [2956, 2956, 2956, 2956, 2956, 2956, 4068, 363...   \n",
              "216  [2956, 2956, 2956, 2956, 2956, 2956, 2956, 295...   \n",
              "26   [4068, 3636, 3658, 2682, 4066, 3636, 3658, 406...   \n",
              "\n",
              "                          seconds_to_incident_sequence  approx_lat  \\\n",
              "674  [-11715, -10710, -10710, -10708, -10708, -1070...   50.731644   \n",
              "861  [-14343, -14331, -14331, -14009, -13894, -1389...   50.452170   \n",
              "495  [-14373, -14364, -14363, -14359, -14348, -1434...   50.556395   \n",
              "216  [-14375, -14346, -14335, -14323, -14303, -1429...   50.168094   \n",
              "26   [-14231, -14214, -14214, -14113, -14071, -1405...   50.711379   \n",
              "\n",
              "     approx_lon                                 train_kph_sequence  \\\n",
              "674    4.502946  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "861    4.318044  [0.0, 0.0, 0.0, 13.7, 32.1, 34.5, 49.3, 50.7, ...   \n",
              "495    3.923098  [104.4, 104.2, 103.9, 102.7, 81.3, 77.3, 0.0, ...   \n",
              "216    5.871434  [48.6, 93.2, 91.8, 94.0, 93.8, 94.4, 96.2, 94....   \n",
              "26     4.401537  [0.046875, 0.0, 0.0, 34.984375, 0.015625, 0.0,...   \n",
              "\n",
              "                                  dj_ac_state_sequence  \\\n",
              "674  [False, False, False, False, False, False, Fal...   \n",
              "861  [False, False, False, False, False, False, Fal...   \n",
              "495  [False, False, False, False, False, False, Fal...   \n",
              "216  [True, True, True, True, True, True, True, Tru...   \n",
              "26   [False, False, False, False, False, False, Fal...   \n",
              "\n",
              "                                  dj_dc_state_sequence  incident_type  \n",
              "674  [True, True, True, True, True, True, True, Fal...             13  \n",
              "861  [True, True, True, True, True, True, True, Tru...              6  \n",
              "495  [True, True, True, True, True, True, True, Tru...             99  \n",
              "216  [False, False, False, False, False, False, Fal...              2  \n",
              "26   [True, True, True, True, True, True, True, Tru...             13  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c0c9f75-2b8a-468c-a982-a943ab602e80\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>incident_id</th>\n",
              "      <th>vehicles_sequence</th>\n",
              "      <th>events_sequence</th>\n",
              "      <th>seconds_to_incident_sequence</th>\n",
              "      <th>approx_lat</th>\n",
              "      <th>approx_lon</th>\n",
              "      <th>train_kph_sequence</th>\n",
              "      <th>dj_ac_state_sequence</th>\n",
              "      <th>dj_dc_state_sequence</th>\n",
              "      <th>incident_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>674</th>\n",
              "      <td>674</td>\n",
              "      <td>4464669</td>\n",
              "      <td>[506, 506, 506, 506, 506, 506, 506, 506, 506, ...</td>\n",
              "      <td>[4016, 2744, 4148, 2708, 4026, 4020, 4168, 414...</td>\n",
              "      <td>[-11715, -10710, -10710, -10708, -10708, -1070...</td>\n",
              "      <td>50.731644</td>\n",
              "      <td>4.502946</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Fal...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>861</th>\n",
              "      <td>861</td>\n",
              "      <td>4602871</td>\n",
              "      <td>[549, 549, 549, 549, 549, 549, 549, 549, 549, ...</td>\n",
              "      <td>[4120, 3636, 3658, 2956, 2956, 2956, 2956, 295...</td>\n",
              "      <td>[-14343, -14331, -14331, -14009, -13894, -1389...</td>\n",
              "      <td>50.452170</td>\n",
              "      <td>4.318044</td>\n",
              "      <td>[0.0, 0.0, 0.0, 13.7, 32.1, 34.5, 49.3, 50.7, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>495</th>\n",
              "      <td>495</td>\n",
              "      <td>4456317</td>\n",
              "      <td>[1087, 1087, 1087, 1087, 1087, 1087, 1087, 108...</td>\n",
              "      <td>[2956, 2956, 2956, 2956, 2956, 2956, 4068, 363...</td>\n",
              "      <td>[-14373, -14364, -14363, -14359, -14348, -1434...</td>\n",
              "      <td>50.556395</td>\n",
              "      <td>3.923098</td>\n",
              "      <td>[104.4, 104.2, 103.9, 102.7, 81.3, 77.3, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>216</th>\n",
              "      <td>216</td>\n",
              "      <td>4443009</td>\n",
              "      <td>[1020, 1020, 1020, 1020, 1020, 1020, 1020, 102...</td>\n",
              "      <td>[2956, 2956, 2956, 2956, 2956, 2956, 2956, 295...</td>\n",
              "      <td>[-14375, -14346, -14335, -14323, -14303, -1429...</td>\n",
              "      <td>50.168094</td>\n",
              "      <td>5.871434</td>\n",
              "      <td>[48.6, 93.2, 91.8, 94.0, 93.8, 94.4, 96.2, 94....</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>26</td>\n",
              "      <td>4434195</td>\n",
              "      <td>[511, 511, 511, 511, 511, 511, 511, 511, 511, ...</td>\n",
              "      <td>[4068, 3636, 3658, 2682, 4066, 3636, 3658, 406...</td>\n",
              "      <td>[-14231, -14214, -14214, -14113, -14071, -1405...</td>\n",
              "      <td>50.711379</td>\n",
              "      <td>4.401537</td>\n",
              "      <td>[0.046875, 0.0, 0.0, 34.984375, 0.015625, 0.0,...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c0c9f75-2b8a-468c-a982-a943ab602e80')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c0c9f75-2b8a-468c-a982-a943ab602e80 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c0c9f75-2b8a-468c-a982-a943ab602e80');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8e5848a8-65e6-4361-989b-e2452a28200d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e5848a8-65e6-4361-989b-e2452a28200d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8e5848a8-65e6-4361-989b-e2452a28200d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 337,\n        \"min\": 26,\n        \"max\": 861,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          861,\n          26,\n          495\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 69569,\n        \"min\": 4434195,\n        \"max\": 4602871,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4602871,\n          4434195,\n          4456317\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vehicles_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 549, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088, 1088]\",\n          \"[511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 511, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682, 682]\",\n          \"[1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087, 1087]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"events_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[4120, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4396, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 4120, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 4026, 4016, 4026, 4020, 4066, 4066, 4066, 4066, 4066, 4068, 3506, 4024, 2744, 4056, 4032, 4026, 2708, 2740, 4148, 4026, 4030, 4018, 3634, 4072, 4126, 2708, 3236, 2744, 3254, 2852, 2854, 4124, 2858, 2708, 2658, 2688, 4124, 2956, 2956, 2956, 2956, 2956, 4180, 2956, 2956, 2956, 2708, 2742, 4026, 4120, 4120, 2956, 2956, 4180, 2956, 2956, 2956, 2686, 2708, 3234, 2742, 4168, 4140, 3234, 3986, 666, 4396, 4068, 4068, 4068, 4068, 4068, 4068, 4066, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4066, 2744, 4148, 4026, 2708, 4020, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4180, 2956, 2956, 2956, 2956, 4068, 2708, 4050, 4032, 2708, 4026, 2740, 4030, 4018, 4124, 3636, 3658, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4180, 2956, 4066, 2708, 3236, 3520, 356, 4396, 2742, 3256, 2668, 3986, 4004, 2852, 4110, 2854, 4026, 3986, 4002, 4110, 2852, 2854, 4026, 3986, 4004, 2852, 4110, 2854, 4026, 3986, 4002, 4110, 2852, 2854, 4026, 3986, 4004, 2852, 4110, 2854, 4026, 3986, 4002, 4110, 2852, 2854, 4026, 3986, 2742, 4004, 2852, 4110, 2854, 2708, 4026, 4030, 4148, 4018, 3986, 2742, 4002, 4110, 2852, 2854, 2708, 4026, 4148, 3986, 4004, 2852, 4110, 2854, 2708, 4026, 3986, 4002, 4110, 2852, 2854, 4026, 3986, 4004, 2852, 4110, 2854, 4026, 3986]\",\n          \"[4068, 3636, 3658, 2682, 4066, 3636, 3658, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4124, 4066, 3636, 4124, 4066, 3636, 4078, 4124, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4066, 3636, 3658, 4078, 3224, 4396, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4066, 3636, 3658, 4066, 2708, 4026, 4016, 4020, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 4068, 4068, 4068, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4066, 4068, 4068, 3658, 4068, 3658, 4068, 4016, 3658, 4130, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 4068, 4066, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 3658, 4068, 3658, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 4068, 2744, 2708, 4026, 4020, 3636, 2684, 2684, 4124, 2682, 4066, 3636, 4066, 3636, 4066, 3636, 4078, 4066, 3636, 3658, 2684, 4124, 4066, 3636, 4078, 4124, 4066, 3636, 2682, 4066, 3636, 4078, 4066, 3636, 3658, 3224, 4396, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4124, 4066, 3636, 4078, 4066, 3636, 4078, 4066, 3636, 4078, 2682, 2682, 4066, 3636, 4078, 4068, 3636, 3658, 4124, 4066, 3636, 3658, 4066, 3636, 4078, 2682, 4124, 4148, 2682, 958, 962, 4394, 2682, 2682, 2682, 4066, 3636, 4124, 2682, 4180, 2682, 2682, 4066, 2708, 2744, 4148, 3636, 4078, 2946, 4124, 4124, 2682, 4066, 3636, 4066, 3636, 4066, 3636, 3982, 4066, 3636, 3658, 4124, 3982, 4066, 3636, 3658, 3982]\",\n          \"[2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 4024, 3506, 2744, 4056, 4032, 2708, 4026, 2740, 4030, 4018, 4026, 3634, 4126, 4068, 2708, 2852, 2854, 4124, 2858, 2658, 2688, 3620, 2708, 4124, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 2742, 4026, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2684, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 3980, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2684, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4120, 2956, 2956, 4068, 2708, 2744, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 2742, 3636, 3658, 4120, 4148, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2682, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 2744, 4026, 3008]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seconds_to_incident_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[-14343, -14331, -14331, -14009, -13894, -13891, -13870, -13868, -13850, -13831, -13818, -13803, -13773, -13762, -13758, -13727, -13716, -13716, -13673, -13664, -13636, -13605, -13593, -13579, -13568, -13555, -13537, -13524, -13463, -13460, -13421, -13409, -13400, -13319, -13306, -13306, -13244, -13238, -13177, -13164, -13140, -13120, -13114, -13081, -13070, -13070, -13014, -12974, -12963, -12956, -12931, -12916, -12905, -12843, -12822, -12810, -12810, -12745, -12715, -12629, -12606, -12606, -12546, -12538, -12464, -12461, -12439, -12429, -12402, -12392, -12389, -12310, -12299, -12272, -12237, -12237, -12182, -12167, -12138, -12126, -12118, -12105, -12042, -12037, -12006, -12000, -11970, -11927, -11906, -11899, -11887, -11853, -11853, -11749, -11699, -11651, -11647, -11601, -11559, -11554, -11540, -11536, -11507, -11493, -11474, -11462, -11455, -11449, -11432, -11426, -11376, -11372, -11369, -11342, -11257, -11227, -11145, -11126, -11099, -11087, -10892, -10832, -10832, -10748, -10739, -10706, -10604, -10597, -10562, -10470, -10466, -10411, -10396, -10392, -10354, -10341, -10341, -10191, -10174, -10170, -10062, -10049, -10049, -9942, -9934, -9851, -9837, -9812, -9800, -9743, -9730, -9730, -9656, -9634, -9605, -9568, -9544, -9525, -9522, -9490, -9473, -9473, -9391, -9384, -9244, -9188, -9137, -9135, -9125, -9073, -9071, -9037, -9025, -9014, -9000, -8982, -8978, -8941, -8926, -8926, -8880, -8853, -8836, -8827, -8825, -8802, -8786, -8751, -8732, -8732, -8695, -8658, -8638, -8618, -8610, -8602, -8571, -8533, -8525, -8525, -8480, -8442, -8433, -8422, -8389, -8387, -8364, -8361, -8268, -8246, -8234, -8212, -8202, -7852, -7851, -7850, -7849, -6572, -6199, -5953, -5738, -5556, -5243, -5153, -5153, -5147, -5147, -5146, -5142, -5141, -5136, -5127, -5125, -5125, -5124, -5119, -5101, -5096, -5080, -5076, -4761, -4627, -4598, -4596, -4382, -4285, -4281, -4280, -4280, -4270, -4226, -4213, -4181, -4165, -4151, -4144, -4126, -4067, -4058, -4020, -3937, -3937, -3852, -3847, -3814, -3774, -3757, -3749, -3737, -3702, -3613, -3608, -3576, -3571, -3567, -3559, -3545, -3522, -13779, -13778, -13730, -13322, -13084, -12825, -12632, -12275, -11890, -11090, -10357, -10065, -9746, -9493, -8944, -8754, -8536, -8215, -7856, -7856, -7855, -7853, -7853, -6867, -6867, -6855, -6714, -6711, -6693, -6691, -6675, -6658, -6646, -6634, -6600, -6598, -6575, -6561, -6561, -6507, -6500, -6480, -6457, -6447, -6432, -6421, -6406, -6388, -6375, -6323, -6321, -6292, -6282, -6274, -6203, -6185, -6185, -6117, -6089, -6084, -6033, -6021, -6001, -5986, -5982, -5957, -5944, -5944, -5906, -5882, -5874, -5869, -5844, -5826, -5815, -5759, -5741, -5727, -5727, -5662, -5637, -5559, -5539, -5539, -5488, -5483, -5429, -5427, -5409, -5399, -5368, -5366, -5356, -5354, -5350, -5280, -5272, -5246, -5241, -5150, -5149, -5145, -5145, -5140, -4995, -4994, -4988, -4986, -4986, -4944, -4932, -4924, -4893, -4883, -4875, -4865, -4818, -4816, -4795, -4793, -4792, -4762, -4714, -4694, -4685, -4680, -4651, -4651, -4582, -4548, -4518, -4516, -4488, -4445, -4440, -4426, -4422, -4391, -4376, -4355, -4344, -4339, -4333, -4319, -4315, -4279, -4276, -4274, -4250, -4173, -4144, -4064, -4046, -4025, -4013, -3644, -3644, -3619, -3571, -3566, -3547, -3472, -3467, -3446, -3379, -3377, -3327, -3311, -3306, -3274, -3261, -3261, -3154, -3142, -3139, -3050, -3037, -3037, -2941, -2937, -2882, -2870, -2841, -2826, -2765, -2747, -2747, -2697, -2683, -2661, -2626, -2602, -2583, -2581, -2554, -2537, -2537, -2486, -2480, -2356, -2323, -2290, -2288, -2280, -2233, -2231, -2197, -2184, -2173, -2161, -2146, -2142, -2110, -2100, -2100, -2050, -2018, -1999, -1988, -1986, -1959, -1944, -1909, -1895, -1895, -1856, -1817, -1797, -1775, -1765, -1752, -1706, -1664, -1654, -1654, -1605, -1561, -1551, -1539, -1506, -1504, -1488, -1486, -1398, -1382, -1382, -1376, -1365, -1356, -1326, -1320, -1319, -1319, -861, -844, -757, -748, -574, -572, -572, -570, -568, -564, -386, -385, -384, -382, -380, -376, -231, -229, -229, -227, -226, -221, -100, -99, -97, -96, -94, -90, 36, 39, 39, 40, 42, 46, 493, 494, 495, 497, 498, 503, 606, 606, 608, 608, 610, 611, 611, 611, 611, 613, 616, 713, 713, 714, 715, 717, 718, 718, 718, 723, 970, 973, 973, 974, 976, 976, 980, 1102, 1103, 1104, 1106, 1108, 1112, 1414, 1417, 1417, 1418, 1420, 1424]\",\n          \"[-14231, -14214, -14214, -14113, -14071, -14055, -14055, -13732, -13717, -13717, -13704, -13172, -13123, -13123, -13088, -12580, -12566, -12566, -12555, -12225, -12205, -12205, -12183, -12027, -11746, -11746, -11662, -11425, -11397, -11380, -11160, -11145, -11134, -11129, -10991, -10967, -10967, -10961, -10881, -10865, -10865, -10854, -10607, -10584, -10584, -10355, -10338, -10338, -10327, -10109, -10094, -10094, -10082, -9876, -9857, -9857, -9566, -9550, -9550, -9539, -9157, -9127, -9127, -8754, -8739, -8739, -8727, -8577, -8577, -8513, -8498, -8498, -8414, -8032, -8012, -8012, -8004, -7858, -7841, -7841, -7636, -7601, -7601, -7318, -7311, -6951, -6950, -6949, -6394, -6116, -6098, -5897, -5837, -5673, -5654, -5403, -5040, -5009, -4617, -4601, -4246, -4233, -4019, -3792, -3538, -3303, -2998, -2967, -2873, -2849, -2696, -2671, -2456, -2315, -2108, -1759, -1356, -1336, -597, -542, 34, 49, 50, 1002, 1128, 1146, 1318, 1343, 1537, 1556, 2402, 3091, -14232, -14072, -13733, -13172, -12581, -12226, -12028, -11426, -11398, -11160, -11146, -10991, -10882, -10608, -10355, -10110, -9877, -9566, -9157, -8755, -8514, -8032, -7859, -7636, -7319, -6952, -6951, -6951, -6950, -6394, -6356, -6354, -6352, -6319, -6117, -6098, -5897, -5838, -5674, -5654, -5645, -5404, -5263, -5263, -5218, -5215, -5040, -5009, -4996, -4710, -4617, -4601, -4484, -4247, -4233, -4222, -4020, -3987, -3987, -3828, -3828, -3793, -3748, -3748, -3735, -3538, -3500, -3500, -3486, -3304, -3220, -3220, -3205, -2999, -2967, -2945, -2873, -2849, -2833, -2697, -2672, -2656, -2591, -2495, -2456, -2316, -2296, -2109, -1898, -1898, -1873, -1760, -1718, -1718, -1356, -1337, -1328, -1184, -1032, -1032, -1009, -996, -996, -995, -900, -661, -632, -597, -542, -482, -468, -115, -111, -79, 33, 42, 47, 47, 50, 61, 412, 625, 743, 868, 1127, 1146, 1318, 1343, 1537, 1555, 1687, 2401, 2445, 2445, 2482, 2802, 3091, 3114, 3114, 3198]\",\n          \"[-14373, -14364, -14363, -14359, -14348, -14345, -14307, -14293, -14191, -14189, -14147, -14141, -14130, -14122, -14090, -14077, -14032, -14014, -14000, -13998, -13993, -13991, -13984, -13978, -13962, -13944, -13925, -13906, -13904, -13887, -13869, -13839, -13825, -13795, -13732, -13679, -13657, -13617, -13606, -13603, -13570, -13557, -13465, -13456, -13450, -13414, -13399, -13388, -13374, -13324, -13312, -13281, -13271, -13236, -13231, -13211, -13190, -13159, -13056, -13026, -13021, -13012, -12909, -12907, -12901, -12901, -12900, -12896, -12896, -12891, -12867, -12866, -12866, -12844, -12836, -12813, -12807, -12617, -12615, -12564, -12501, -12500, -12500, -12500, -12448, -10037, -9974, -9974, -9921, -9818, -9802, -9794, -9775, -9774, -9740, -9729, -9698, -9688, -9665, -9649, -9633, -9633, -9586, -9553, -9543, -9536, -9470, -9455, -9455, -9401, -9397, -9334, -9309, -9237, -9198, -9182, -9182, -9158, -9133, -9104, -9080, -9078, -9060, -9043, -9025, -9006, -9000, -8990, -8989, -8983, -8981, -8966, -8953, -8928, -8909, -8909, -8864, -8856, -8855, -8843, -8836, -8805, -8796, -8794, -8722, -8704, -8704, -8662, -8658, -8640, -8628, -8569, -8548, -8548, -8499, -8493, -8487, -8485, -8480, -8456, -8423, -8412, -8412, -8369, -8331, -8314, -8305, -8289, -8287, -8279, -8254, -8236, -8236, -8160, -8150, -8128, -8100, -8095, -8092, -8049, -8034, -8023, -8002, -7904, -7904, -7886, -7846, -7798, -7794, -7790, -7785, -7768, -7765, -7749, -7731, -7730, -7723, -7719, -7704, -7702, -7640, -7628, -7563, -7542, -7542, -7474, -7420, -7408, -7333, -7316, -7316, -7265, -7218, -7198, -7148, -7126, -7126, -7072, -7047, -7000, -6996, -6973, -6938, -6938, -6920, -6895, -6892, -6885, -6838, -6817, -6809, -6787, -6743, -6723, -6678, -6667, -6667, -6603, -6588, -6548, -6527, -6485, -6461, -6422, -6392, -6385, -6269, -6269, -5852, -5852, -5734, -5649, -5620, -5567, -5546, -5515, -5506, -5492, -5447, -5425, -5425, -5406, -5373, -5361, -5344, -5335, -5331, -5316, -5270, -5264, -5261, -5228, -5197, -5197, -5178, -5165, -5159, -5115, -5113, -5098, -5096, -5063, -5043, -5043, -4999, -4985, -4982, -4976, -4973, -4966, -4931, -4891, -4864, -4864, -4814, -4798, -4732, -4682, -4669, -4617, -4596, -4596, -4576, -4516, -4515, -4455, -4438, -4360, -4338, -4334, -4331, -4287, -4280, -4272, -4265, -4168, -4168, -4150, -4148, -4122, -4119, -4107, -4062, -4059, -4054, -4028, -4024, -4006, -3997, -3954, -3933, -3933, -3885, -3877, -3874, -3858, -3840, -3831, -3796, -3762, -3731, -3731, -3686, -3659, -3658, -3653, -3645, -3607, -3591, -3591, -3517, -3508, -3506, -3502, -3490, -3486, -3453, -3435, -3435, -3337, -3334, -3289, -3282, -3271, -3263, -3239, -3205, -3205, -3169, -3151, -3135, -3134, -3128, -3127, -3119, -3113, -3098, -3081, -3062, -3043, -3042, -3025, -3005, -2978, -2961, -2961, -2941, -2939, -2882, -2828, -2806, -2766, -2752, -2749, -2715, -2705, -2705, -2624, -2616, -2609, -2570, -2547, -2500, -2500, -2472, -2450, -2439, -2410, -2399, -2364, -2358, -2333, -2310, -2227, -2210, -2133, -2103, -2089, -1904, -1104, -1104, -1062, -946, -925, -919, -909, -868, -801, -801, -725, -686, -676, -667, -629, -610, -601, -582, -582, -510, -497, -492, -481, -469, -460, -436, -422, -399, -381, -351, -321, -321, -255, -252, -232, -217, -207, -196, -176, -175, -157, -142, -123, -96, -96, -48, -11, 2, 6, 15, 47, 68, 68, 127, 140, 143, 152, 154, 177, 187, 192, 206, 216, 227, 247, 260, 261, 275, 279, 303, 313, 320, 331, 362, 382, 395, 492, 656, 1804, 1804, 1826, 1830, 1859, 1894, 1905, 1912, 1920, 1933, 1943, 1955, 1967, 1985, 1995, 2000, 2017, 2021, 2030, 2050, 2052, 2059, 2061, 2073, 2098, 2130, 2130, 2197, 2206, 2210, 2222, 2251, 2264, 2283, 2312, 2312, 2370, 2389, 2391, 2408, 2426, 2439, 2445, 2457, 2483, 2501, 2528, 2528, 2612, 2627, 2650, 2659, 2670, 2679, 2683, 2694, 2712, 2730, 2754, 2754, 2783, 2817, 2863, 2873, 2909, 2919, 2978, 2995, 2995, 3081, 3103, 3124, 3161, 3302, 3328, 3332, 3349, 3565, 3565, 3592]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22973079655021958,\n        \"min\": 50.16809428075269,\n        \"max\": 50.731644434065934,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          50.45217022688525,\n          50.711379075367645,\n          50.5563952067591\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7421235863023963,\n        \"min\": 3.923097847660312,\n        \"max\": 5.871434377311828,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4.318044292295082,\n          4.401536980147059,\n          3.923097847660312\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_kph_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[0.0, 0.0, 0.0, 13.7, 32.1, 34.5, 49.3, 50.7, 61.2, 69.9, 74.5, 72.8, 51.4, 45.5, 45.2, 0.0, 0.0, 0.0, 13.4, 20.6, 42.2, 65.0, 71.2, 75.0, 75.8, 76.8, 78.6, 78.6, 80.1, 80.1, 86.3, 84.8, 83.4, 0.0, 0.0, 0.0, 30.1, 37.1, 78.6, 75.2, 57.8, 33.8, 31.3, 0.0, 0.0, 0.0, 20.7, 52.0, 62.5, 65.8, 75.8, 63.5, 53.4, 25.6, 0.0, 0.0, 0.0, 37.8, 67.0, 0.0, 0.0, 0.0, 23.8, 29.5, 82.6, 84.0, 97.4, 101.8, 93.3, 81.5, 78.3, 31.1, 30.3, 0.1, 0.0, 0.0, 18.2, 28.7, 49.3, 52.0, 49.6, 46.1, 30.1, 28.2, 32.0, 30.7, 30.7, 19.6, 17.2, 16.8, 0.1, 0.0, 0.0, 11.5, 15.8, 20.6, 20.8, 28.4, 53.0, 55.3, 60.9, 62.3, 71.4, 74.3, 74.9, 71.8, 70.2, 68.8, 66.3, 64.1, 34.4, 34.6, 34.5, 35.7, 31.1, 30.8, 29.2, 23.8, 13.8, 0.0, 0.0, 0.0, 0.0, 11.4, 13.3, 19.5, 31.5, 31.2, 31.5, 67.4, 68.2, 82.4, 60.4, 55.9, 0.0, 0.0, 0.0, 56.3, 62.7, 63.9, 0.0, 0.0, 0.0, 25.4, 30.3, 88.0, 93.8, 94.8, 78.9, 0.1, 0.0, 0.0, 21.4, 31.8, 42.3, 46.7, 43.0, 30.1, 28.6, 0.1, 0.0, 0.0, 16.3, 16.8, 16.7, 21.6, 67.7, 69.3, 75.9, 100.7, 101.6, 95.8, 75.1, 60.1, 53.5, 44.4, 41.9, 0.1, 0.0, 0.0, 24.4, 74.9, 97.5, 100.9, 100.8, 71.5, 54.3, 0.0, 0.0, 0.0, 26.9, 95.0, 115.0, 114.2, 114.5, 114.0, 64.0, 0.0, 0.0, 0.0, 37.6, 98.6, 97.8, 93.2, 49.1, 47.3, 33.6, 33.8, 26.6, 21.7, 19.4, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.3, 0.2, 0.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 23.7, 29.0, 36.2, 36.1, 30.2, 25.0, 20.0, 26.4, 25.5, 0.0, 0.0, 0.0, 0.0, 0.0, 35.3, 28.7, 22.8, 23.5, 21.5, 18.3, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 54.2, 54.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.2, 0.1, 0.0, 0.0, 0.0, 0.1, 0.2, 0.1, 0.2, 0.0, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 37.6, 38.2, 55.7, 57.2, 68.8, 79.4, 84.4, 82.8, 61.1, 58.6, 0.0, 0.0, 0.0, 16.1, 27.1, 60.4, 80.1, 76.3, 69.9, 69.4, 74.0, 79.8, 82.5, 108.4, 109.4, 110.0, 100.9, 92.2, 0.2, 0.0, 0.0, 0.0, 36.0, 45.4, 94.4, 93.3, 63.1, 53.1, 47.7, 0.1, 0.0, 0.0, 34.8, 83.0, 85.9, 82.9, 62.3, 58.8, 53.5, 31.5, 0.1, 0.0, 0.0, 48.8, 69.3, 0.1, 0.0, 0.0, 33.2, 44.4, 104.1, 104.8, 109.0, 105.6, 71.2, 70.0, 65.7, 65.1, 56.0, 40.6, 37.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 13.9, 17.3, 17.1, 58.6, 58.9, 56.7, 54.1, 48.4, 47.3, 37.3, 35.5, 34.8, 29.8, 22.2, 17.3, 9.7, 0.1, 0.0, 0.0, 15.1, 25.4, 38.1, 40.7, 35.8, 52.0, 53.5, 57.5, 58.5, 67.6, 71.1, 77.1, 79.4, 80.2, 81.2, 80.2, 80.1, 50.2, 46.9, 44.0, 39.6, 34.9, 30.5, 30.1, 28.3, 23.2, 0.0, 0.0, 0.0, 0.0, 19.5, 23.1, 35.7, 43.6, 46.0, 54.7, 77.3, 78.3, 76.0, 55.5, 54.3, 0.2, 0.0, 0.0, 82.4, 89.5, 91.0, 0.2, 0.0, 0.0, 58.0, 63.9, 105.1, 99.8, 77.3, 62.9, 0.2, 0.0, 0.0, 39.9, 47.1, 50.2, 50.1, 36.2, 35.7, 35.4, 0.2, 0.0, 0.0, 18.8, 18.2, 19.3, 53.5, 82.5, 83.6, 88.4, 107.6, 108.3, 83.4, 71.4, 70.6, 66.1, 46.0, 44.7, 0.2, 0.0, 0.0, 20.2, 65.7, 81.6, 79.4, 79.1, 72.1, 60.1, 0.1, 0.0, 0.0, 18.4, 92.9, 112.7, 92.0, 85.9, 74.5, 53.8, 0.2, 0.0, 0.0, 26.6, 88.2, 88.8, 84.2, 57.6, 57.1, 51.0, 48.5, 29.5, 27.3, 27.3, 14.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\",\n          \"[0.046875, 0.0, 0.0, 34.984375, 0.015625, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.171875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.203125, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.0, 0.140625, 0.0, 0.0, 0.203125, 0.0, 0.0, 0.0, 90.953125, 90.953125, 0.078125, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.21875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.578125, 0.0, 0.0, 53.625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.078125, 0.078125, 0.03125, 0.0625, 0.0, 0.015625, 0.03125, 0.0, 0.40625, 0.0, 0.0, 0.0, 0.140625, 0.0, 0.21875, 0.0, 0.234375, 0.4375, 0.109375, 0.140625, 0.46875, 0.203125, 0.3125, 0.0, 0.390625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.71875, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 53.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 94.84375, 94.84375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 55.5, 57.109375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.328125, 0.0, 0.0, 0.0, 22.03125, 22.03125, 22.375, 108.3125, 35.015625, 31.234375, 0.0, 0.0, 0.0, 0.0, 54.84375, 48.1875, 42.75, 0.265625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 64.453125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 88.328125, 0.0, 0.0, 0.0, 0.0, 79.5625, 0.0, 0.0, 0.0, 101.359375]\",\n          \"[104.4, 104.2, 103.9, 102.7, 81.3, 77.3, 0.0, 0.0, 112.8, 112.9, 113.5, 108.2, 85.2, 69.8, 0.0, 0.0, 38.4, 87.7, 110.1, 110.7, 110.9, 111.2, 110.8, 104.8, 73.4, 52.4, 55.2, 55.7, 56.1, 55.6, 48.9, 0.0, 0.0, 0.0, 29.1, 36.1, 68.2, 95.8, 89.6, 80.9, 0.1, 0.0, 119.3, 123.0, 122.3, 96.5, 42.0, 0.0, 0.0, 80.8, 100.5, 135.5, 136.2, 117.5, 105.5, 55.9, 28.1, 35.2, 34.6, 8.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.0, 42.2, 81.2, 95.0, 119.3, 120.6, 132.4, 131.2, 108.2, 86.9, 38.1, 0.0, 0.0, 0.0, 67.2, 114.8, 111.0, 108.5, 0.0, 0.0, 0.0, 65.3, 71.7, 66.5, 35.4, 35.8, 0.0, 0.0, 0.0, 0.0, 34.4, 34.7, 51.2, 53.3, 60.5, 59.6, 59.4, 87.3, 95.8, 100.8, 100.5, 100.1, 99.9, 98.4, 71.2, 0.0, 0.0, 0.0, 58.6, 75.5, 78.6, 97.2, 106.4, 116.6, 115.6, 115.5, 0.0, 0.0, 0.0, 52.0, 63.1, 89.3, 89.2, 0.0, 0.0, 0.0, 62.8, 77.1, 87.3, 88.1, 87.2, 86.9, 0.0, 0.0, 0.0, 62.0, 117.5, 116.9, 114.5, 80.3, 76.2, 64.3, 0.0, 0.0, 0.0, 60.1, 58.2, 50.1, 35.1, 35.0, 35.2, 35.6, 34.9, 33.6, 0.0, 0.0, 0.0, 0.0, 24.2, 33.1, 32.2, 30.8, 33.3, 52.9, 55.4, 64.5, 85.1, 84.9, 86.6, 87.8, 86.6, 86.8, 87.7, 88.0, 0.0, 0.0, 0.0, 91.6, 85.1, 83.9, 0.0, 0.0, 0.0, 71.5, 62.9, 55.4, 0.1, 0.0, 0.0, 48.5, 37.5, 30.3, 30.3, 0.0, 0.0, 0.0, 0.0, 30.1, 32.7, 34.7, 36.8, 35.1, 35.9, 35.1, 36.5, 66.8, 0.1, 0.0, 0.0, 87.8, 111.0, 114.9, 108.4, 61.1, 35.1, 35.2, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.9, 36.8, 108.7, 116.6, 114.7, 111.4, 103.0, 0.0, 0.0, 0.0, 0.0, 74.8, 95.3, 93.3, 80.4, 71.0, 36.5, 34.5, 34.1, 34.3, 0.1, 0.0, 0.0, 0.0, 15.8, 26.0, 56.7, 58.8, 57.8, 56.9, 0.0, 0.0, 0.0, 38.7, 69.1, 75.0, 83.2, 87.4, 88.2, 79.0, 0.1, 0.0, 0.0, 0.0, 30.9, 84.4, 84.6, 80.6, 0.1, 0.0, 0.0, 0.0, 83.2, 82.9, 83.0, 78.8, 55.4, 36.7, 33.8, 34.7, 32.3, 28.8, 15.3, 0.1, 0.0, 0.0, 0.0, 0.0, 35.9, 35.7, 34.7, 37.9, 37.7, 37.2, 55.5, 58.9, 56.4, 72.8, 0.0, 0.0, 0.0, 56.2, 72.6, 77.8, 106.8, 113.3, 113.5, 84.6, 0.2, 0.0, 0.0, 47.1, 83.6, 83.6, 82.3, 70.4, 0.1, 0.0, 0.0, 101.6, 98.2, 97.9, 94.6, 72.0, 70.0, 0.1, 0.0, 0.0, 105.8, 104.7, 104.7, 103.6, 89.4, 75.2, 0.1, 0.0, 0.0, 43.9, 82.1, 104.8, 107.0, 111.0, 110.6, 108.4, 104.1, 81.6, 54.0, 53.9, 56.7, 56.6, 54.6, 41.2, 0.1, 0.0, 0.0, 0.0, 0.0, 34.8, 36.2, 68.0, 89.0, 63.2, 59.3, 0.0, 0.0, 0.0, 120.7, 124.2, 123.5, 73.9, 0.1, 0.0, 0.0, 29.5, 91.2, 109.6, 136.1, 134.6, 104.7, 91.8, 43.0, 27.2, 0.0, 17.2, 34.9, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 38.1, 65.8, 70.4, 66.7, 0.0, 0.0, 0.0, 94.1, 107.5, 110.8, 116.7, 72.4, 28.5, 0.0, 0.0, 0.0, 70.6, 85.0, 89.6, 98.9, 107.5, 112.8, 111.6, 108.1, 74.0, 48.2, 0.0, 0.0, 0.0, 58.1, 61.6, 82.4, 93.4, 93.3, 87.1, 67.3, 66.1, 56.0, 37.1, 0.0, 0.0, 0.0, 58.9, 85.8, 85.6, 83.7, 73.2, 0.0, 0.0, 0.0, 53.8, 69.3, 71.7, 78.6, 80.8, 97.0, 96.7, 93.4, 94.5, 88.8, 90.2, 79.5, 72.9, 72.0, 64.8, 64.6, 58.4, 58.2, 58.8, 51.1, 34.0, 18.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 21.9, 50.7, 61.3, 67.6, 72.8, 78.5, 79.1, 87.2, 89.3, 94.7, 100.8, 102.5, 105.7, 107.0, 109.0, 91.0, 91.0, 87.6, 84.4, 63.6, 0.0, 0.0, 0.0, 70.0, 82.9, 87.2, 99.1, 93.7, 54.9, 0.0, 0.0, 0.0, 32.1, 74.6, 77.6, 99.0, 104.1, 102.9, 97.3, 86.2, 47.3, 0.0, 0.0, 0.0, 94.3, 107.1, 114.6, 117.8, 116.8, 106.6, 100.1, 87.6, 49.8, 0.0, 0.0, 0.0, 0.0, 49.5, 103.4, 107.9, 109.9, 102.2, 0.0, 0.0, 0.0, 29.6, 29.0, 28.2, 26.8, 25.0, 7.4, 0.0, 0.0, 0.0, 0.0, 0.0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_ac_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_dc_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\",\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 40,\n        \"min\": 2,\n        \"max\": 99,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          6,\n          2,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events_list = []\n",
        "events_pre_incident = []\n",
        "events_post_incident = []\n",
        "\n",
        "for i, (events, seconds_to_incident_sequence) in tqdm(enumerate(zip(df[\"events_sequence\"], df[\"seconds_to_incident_sequence\"])), total=len(df)):\n",
        "    events = ast.literal_eval(events)\n",
        "    seconds_to_incident_sequence = ast.literal_eval(seconds_to_incident_sequence)\n",
        "\n",
        "    pre_incidents = []\n",
        "    post_incidents = []\n",
        "    event_seq = []\n",
        "\n",
        "    for event, time_to_incident in zip(events, seconds_to_incident_sequence):\n",
        "        event_seq.append(str(event))\n",
        "        if time_to_incident <= 0:\n",
        "            pre_incidents.append(str(event))\n",
        "        else:\n",
        "            post_incidents.append(str(event))\n",
        "\n",
        "    # Append the pre and post incident lists to the main lists\n",
        "    events_pre_incident.append(pre_incidents)\n",
        "    events_post_incident.append(post_incidents)\n",
        "    events_list.append(event_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rs6FuHq84zef",
        "outputId": "8088c864-f035-427f-ee51-2fc1095cf53f"
      },
      "id": "Rs6FuHq84zef",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1011/1011 [00:04<00:00, 233.25it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2vec = Word2Vec(sentences=events_pre_incident, vector_size=8, window=100, min_count=3, workers=4)\n",
        "embeddings = []\n",
        "labels = []\n",
        "labels_org = df[\"incident_type\"]\n",
        "\n",
        "for events, label in tqdm(zip(events_pre_incident, labels_org)):\n",
        "  embedding = np.zeros(8)\n",
        "  for event in events:\n",
        "    if event in word2vec.wv:\n",
        "      embedding += word2vec.wv[event]\n",
        "      embeddings.append(embedding)\n",
        "      labels.append(label)"
      ],
      "metadata": {
        "id": "kgIbXBmK8c3M"
      },
      "id": "kgIbXBmK8c3M",
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings)"
      ],
      "metadata": {
        "id": "yqrO7j7sJltA"
      },
      "id": "yqrO7j7sJltA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "umap_model = umap.UMAP(n_components=2, min_dist=0.1, n_neighbors=200, metric='cosine')\n",
        "X_umap = umap_model.fit_transform(embeddings)\n",
        "umap.plot.points(umap_model, labels=labels)"
      ],
      "metadata": {
        "id": "DdZrSzNI6QnD"
      },
      "id": "DdZrSzNI6QnD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6f6f454",
      "metadata": {
        "id": "a6f6f454"
      },
      "outputs": [],
      "source": [
        "class SCNB:\n",
        "    def __init__(self):\n",
        "        self.classifiers = {\n",
        "            'LogisticRegression': LogisticRegression(),\n",
        "            'DecisionTree': DecisionTreeClassifier(),\n",
        "            'RandomForest': RandomForestClassifier(),\n",
        "            'GaussianNB': GaussianNB(),\n",
        "            'KNN': KNeighborsClassifier(),\n",
        "            'SVM': SVC(probability=True),\n",
        "            'XGBoost': XGBClassifier()\n",
        "        }\n",
        "        self.models = {}\n",
        "        self.embedding_methods = {}\n",
        "        self.ensemble_model = None\n",
        "        self.hmm_model = None\n",
        "\n",
        "    def train_classifiers(self, X, y):\n",
        "        \"\"\"Entrena cada clasificador popular junto con XGBoost.\"\"\"\n",
        "        for name, clf in self.classifiers.items():\n",
        "            clf.fit(X, y)\n",
        "            self.models[name] = clf\n",
        "        print(\"All classifiers trained successfully.\")\n",
        "\n",
        "    def generate_embeddings(self, words_list):\n",
        "        \"\"\"Genera embeddings para una lista de palabras usando distintos encoders.\"\"\"\n",
        "        encoders = {\n",
        "            'CountVectorizer': CountVectorizer(),\n",
        "            'OneHotEncoder': OneHotEncoder(sparse=False),\n",
        "            'TfidfVectorizer': TfidfVectorizer()\n",
        "        }\n",
        "\n",
        "        for name, encoder in encoders.items():\n",
        "            self.embedding_methods[name] = encoder.fit_transform(words_list).toarray()\n",
        "\n",
        "        word2vec_model = Word2Vec(sentences=[words_list], vector_size=100, window=5, min_count=1, workers=4)\n",
        "        self.embedding_methods['Word2Vec'] = [word2vec_model.wv[word] for word in words_list if word in word2vec_model.wv]\n",
        "        print(\"Embeddings generated successfully.\")\n",
        "\n",
        "    def evaluate_models(self, X, y):\n",
        "        \"\"\"Evalúa cada modelo entrenado usando StratifiedKFold y genera una tabla con métricas.\"\"\"\n",
        "        results = []\n",
        "        skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            accuracies, recalls, precisions = [], [], []\n",
        "            for train_index, test_index in skf.split(X, y):\n",
        "                X_train, X_test = X[train_index], X[test_index]\n",
        "                y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "\n",
        "                accuracies.append(accuracy_score(y_test, y_pred))\n",
        "                recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
        "                precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "            results.append({\n",
        "                'Model': name,\n",
        "                'Accuracy Mean': np.mean(accuracies),\n",
        "                'Accuracy Std': np.std(accuracies),\n",
        "                'Accuracy Median': np.median(accuracies),\n",
        "                'Recall Mean': np.mean(recalls),\n",
        "                'Recall Std': np.std(recalls),\n",
        "                'Recall Median': np.median(recalls),\n",
        "                'Precision Mean': np.mean(precisions),\n",
        "                'Precision Std': np.std(precisions),\n",
        "                'Precision Median': np.median(precisions),\n",
        "            })\n",
        "\n",
        "        results_df = pd.DataFrame(results)\n",
        "        print(\"Evaluation complete.\")\n",
        "        return results_df\n",
        "\n",
        "    def train_ensemble_model(self, weights=None):\n",
        "        \"\"\"Crea y entrena un modelo de ensamblaje usando los mejores modelos entrenados.\"\"\"\n",
        "        estimators = [(name, model) for name, model in self.models.items()]\n",
        "        self.ensemble_model = VotingClassifier(estimators=estimators, voting='soft', weights=weights)\n",
        "        print(\"Ensemble model created successfully.\")\n",
        "\n",
        "    def train_hidden_markov_model(self, X, n_components=2):\n",
        "        \"\"\"Entrena un modelo Hidden Markov.\"\"\"\n",
        "        self.hmm_model = hmm.GaussianHMM(n_components=n_components)\n",
        "        self.hmm_model.fit(X)\n",
        "        print(\"Hidden Markov Model trained successfully.\")\n",
        "\n",
        "    def analyze_hmm(self):\n",
        "        \"\"\"Analiza el modelo Hidden Markov mostrando matrices de transición y demás estadísticas.\"\"\"\n",
        "        if self.hmm_model:\n",
        "            print(\"Transition matrix:\", self.hmm_model.transmat_)\n",
        "            print(\"Means:\", self.hmm_model.means_)\n",
        "            print(\"Covars:\", self.hmm_model.covars_)\n",
        "        else:\n",
        "            print(\"HMM model not trained yet.\")\n",
        "\n",
        "    def dimensionality_reduction_with_umap(self, X, y, min_dist_values=[0.1, 0.5], n_neighbors_values=[5, 10]):\n",
        "        \"\"\"Aplica reducción de dimensionalidad usando UMAP y muestra scatter plots.\"\"\"\n",
        "        for min_dist in min_dist_values:\n",
        "            for n_neighbors in n_neighbors_values:\n",
        "                umap_model = umap.UMAP(min_dist=min_dist, n_neighbors=n_neighbors)\n",
        "                X_umap = umap_model.fit_transform(X)\n",
        "\n",
        "                scatter_matrix(pd.DataFrame(X_umap), alpha=0.2, figsize=(10, 10), diagonal='kde', c=y)\n",
        "                plt.title(f\"UMAP Clustering (min_dist={min_dist}, n_neighbors={n_neighbors})\")\n",
        "                plt.show()\n",
        "\n",
        "        print(\"Dimensionality reduction with UMAP completed.\")\n",
        "\n",
        "    def clustering_with_algorithms(self, X, y):\n",
        "        \"\"\"Realiza clustering usando algoritmos populares y muestra scatter plots.\"\"\"\n",
        "        clusterers = {\n",
        "            'KMeans': KMeans(n_clusters=len(np.unique(y))),\n",
        "            'Agglomerative': AgglomerativeClustering(n_clusters=len(np.unique(y))),\n",
        "            'DBSCAN': DBSCAN()\n",
        "        }\n",
        "\n",
        "        for name, clusterer in clusterers.items():\n",
        "            clusters = clusterer.fit_predict(X)\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            scatter_matrix(pd.DataFrame(X), alpha=0.2, figsize=(10, 10), diagonal='kde', c=clusters)\n",
        "            plt.title(f\"Clustering with {name}\")\n",
        "            plt.show()\n",
        "\n",
        "        print(\"Clustering with various algorithms completed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67650822",
      "metadata": {
        "id": "67650822"
      },
      "outputs": [],
      "source": [
        "# Ejemplo de uso\n",
        "# Supongamos que tienes un DataFrame `df` con variables `X` y `y` definidas.\n",
        "\n",
        "# Crear instancia de SCNB\n",
        "scnb = SCNB()\n",
        "\n",
        "# Entrenar clasificadores\n",
        "X = df.drop(columns=['target'])\n",
        "y = df['target']\n",
        "scnb.train_classifiers(X, y)\n",
        "\n",
        "# Generar embeddings para una lista de palabras\n",
        "words_list = [\"gato\", \"perro\", \"pez\", \"pájaro\"]\n",
        "scnb.generate_embeddings(words_list)\n",
        "\n",
        "# Evaluar modelos\n",
        "results_df = scnb.evaluate_models(X, y)\n",
        "print(results_df)\n",
        "\n",
        "# Entrenar modelo de ensamblaje\n",
        "scnb.train_ensemble_model()\n",
        "\n",
        "# Entrenar y analizar un modelo HMM\n",
        "scnb.train_hidden_markov_model(X)\n",
        "scnb.analyze_hmm()\n",
        "\n",
        "# Red\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}