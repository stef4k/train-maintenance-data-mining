{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stef4k/train-maintenance-data-mining/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee",
      "metadata": {
        "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee"
      },
      "source": [
        "# Text classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9",
      "metadata": {
        "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import ast\n",
        "\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.base import TransformerMixin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import differential_evolution\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58",
      "metadata": {
        "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58"
      },
      "source": [
        "Manually remove the first ';' from the first row in csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
      "metadata": {
        "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
        "outputId": "60ce3bdf-6845-4a04-a944-d69c181c4dac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     incident_id                                  vehicles_sequence  \\\n",
              "17       4433927  [667, 667, 667, 667, 667, 667, 667, 667, 667, ...   \n",
              "326      4449091  [574, 574, 574, 574, 574, 574, 574, 574, 574, ...   \n",
              "\n",
              "                                       events_sequence  \\\n",
              "17   [2708, 2744, 4026, 4020, 2852, 3634, 2854, 406...   \n",
              "326  [4004, 4032, 4028, 2852, 4026, 4110, 2854, 274...   \n",
              "\n",
              "                          seconds_to_incident_sequence  approx_lat  \\\n",
              "17   [-14232, -14232, -14232, -14231, -14201, -1420...   50.897397   \n",
              "326  [-4468, -4468, -4467, -4466, -4466, -4466, -44...   51.160264   \n",
              "\n",
              "     approx_lon                                 train_kph_sequence  \\\n",
              "17     4.399000  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "326    4.158203  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
              "\n",
              "                                  dj_ac_state_sequence  \\\n",
              "17   [False, False, False, False, False, False, Fal...   \n",
              "326  [False, False, False, False, False, False, Fal...   \n",
              "\n",
              "                                  dj_dc_state_sequence  incident_type  \n",
              "17   [True, True, True, True, True, True, True, Tru...              9  \n",
              "326  [False, False, False, False, False, False, Fal...              2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a8492541-d420-4b3f-93b8-1d20b2a5dc9a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incident_id</th>\n",
              "      <th>vehicles_sequence</th>\n",
              "      <th>events_sequence</th>\n",
              "      <th>seconds_to_incident_sequence</th>\n",
              "      <th>approx_lat</th>\n",
              "      <th>approx_lon</th>\n",
              "      <th>train_kph_sequence</th>\n",
              "      <th>dj_ac_state_sequence</th>\n",
              "      <th>dj_dc_state_sequence</th>\n",
              "      <th>incident_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>4433927</td>\n",
              "      <td>[667, 667, 667, 667, 667, 667, 667, 667, 667, ...</td>\n",
              "      <td>[2708, 2744, 4026, 4020, 2852, 3634, 2854, 406...</td>\n",
              "      <td>[-14232, -14232, -14232, -14231, -14201, -1420...</td>\n",
              "      <td>50.897397</td>\n",
              "      <td>4.399000</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>326</th>\n",
              "      <td>4449091</td>\n",
              "      <td>[574, 574, 574, 574, 574, 574, 574, 574, 574, ...</td>\n",
              "      <td>[4004, 4032, 4028, 2852, 4026, 4110, 2854, 274...</td>\n",
              "      <td>[-4468, -4468, -4467, -4466, -4466, -4466, -44...</td>\n",
              "      <td>51.160264</td>\n",
              "      <td>4.158203</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a8492541-d420-4b3f-93b8-1d20b2a5dc9a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a8492541-d420-4b3f-93b8-1d20b2a5dc9a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a8492541-d420-4b3f-93b8-1d20b2a5dc9a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ec491887-983f-468e-a76a-d3672a76dcf1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ec491887-983f-468e-a76a-d3672a76dcf1')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ec491887-983f-468e-a76a-d3672a76dcf1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"incident_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 10722,\n        \"min\": 4433927,\n        \"max\": 4449091,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4449091,\n          4433927\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vehicles_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 574, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620, 620]\",\n          \"[667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 667, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056, 1056]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"events_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[4004, 4032, 4028, 2852, 4026, 4110, 2854, 2742, 4148, 2708, 4026, 4140, 4140, 4152, 2740, 4168, 4156, 4406, 4410, 4408, 4412, 4120, 2858, 2658, 2688, 4030, 4020, 4070, 3254, 3254, 3254, 3632, 4066, 4068, 2708, 4016, 4026, 4020, 4082, 4092, 4094, 4066, 4068, 4016, 2742, 4026, 2708, 4026, 4020, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4120, 2956, 2956, 2956, 3982, 2682, 3512, 4048, 4392, 2736, 2708, 3226, 862, 4066, 3238, 4020, 4026, 4028, 4026, 4016, 4026, 4020, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 148, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3982, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3780, 3788, 3636, 4124, 2956, 2956, 3982, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3982, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4078, 2956, 148, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 148, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 2956, 148, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4180, 4066, 2708, 3786, 3506, 4022, 2742, 4050, 4032, 2708, 4026, 2740, 4030, 4018, 4148, 3636, 3658, 4122, 4122, 2852, 2854, 4066, 4120, 2858, 2658, 2688, 3620, 2708, 3254, 3254, 3254, 3636, 3658, 4120, 2956, 2956, 2956, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4396, 1572, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 4066, 2708, 3636, 3658, 2744, 4148, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2686, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2686, 4124, 4124, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2894, 2894, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2858, 2956, 2686, 4080, 4124, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 2742, 4026, 4148, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956]\",\n          \"[2708, 2744, 4026, 4020, 2852, 3634, 2854, 4066, 4068, 4124, 3620, 3634, 2858, 2658, 2688, 3254, 3254, 3254, 4180, 4066, 4068, 2708, 2970, 4092, 4082, 4090, 4084, 4090, 4094, 4090, 3236, 2974, 4100, 4072, 4396, 4396, 4396, 4394, 2674, 4396, 4394, 1566, 1570, 4396, 4026, 4016, 4026, 4020, 4068, 3658, 4068, 3658, 4168, 4068, 3658, 666, 668, 4168, 2556, 4168, 4156, 4068, 3658, 4066, 3658, 4068, 3658, 1712, 4068, 3702, 3714, 3658, 4080, 4068, 3658, 4068, 3658, 4068, 3658, 4066, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 4168, 4068, 4068, 4168, 4068, 4068, 4068, 3658, 4026, 4016, 4020, 4066, 4068, 3256, 4066, 4068, 4092, 4082, 4094, 3492, 4396, 3492, 4396, 3492, 4396, 4394, 1566, 1570, 4396, 2884, 2742, 4026, 4148, 2708, 4020, 4026, 4070, 3632, 4120, 2682, 666, 3224, 2682, 4068, 3636, 4120, 4120, 4068, 3636, 4078, 666, 668, 4168, 4156, 4068, 3636, 4078, 666, 4168, 4168, 2682, 4068, 3636, 4078, 4066, 3636, 4078, 2682, 4068, 3636, 4078, 4122, 4068, 3636, 4078, 4120, 3254, 4180, 2686, 4080, 4120, 4068, 3636, 4078, 2684, 2846, 4120, 4068, 3636, 2682, 4068, 3636, 4078, 4066, 3636, 4068, 3636, 4078, 4068, 3636, 2684, 4120, 4068, 3636, 2682, 4068, 3636, 3658, 666, 668, 4168, 4156, 4068, 3636, 3658, 666, 4068, 3636, 3658, 4078, 666, 668, 4168, 4156, 4068, 3636, 3658, 4078, 4068, 3636, 3658, 4078, 4068, 3636, 4078, 666]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seconds_to_incident_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[-4468, -4468, -4467, -4466, -4466, -4466, -4464, -4442, -4442, -4441, -4441, -4432, -4402, -4400, -4372, -4367, -4365, -4351, -4349, -4340, -4338, -4313, -4251, -4249, -4249, -4238, -4237, -4237, -4222, -4221, -4215, -4185, -4183, -4182, -4175, -3937, -3937, -3934, -3636, -3636, -3627, -3443, -3443, 525, 1662, 1662, 1664, 1664, 1665, 2642, 2735, 2740, 2744, 2770, 2773, 2777, 2814, 2818, 2842, 2845, 2868, 2872, 3001, 3081, 3093, 3160, 3243, 3264, 3264, 3264, 3264, 3265, 3266, 3269, 3269, 3271, 3274, 3285, 3285, 3285, 3429, 3430, 3443, 3444, -14398, -14391, -14362, -14358, -14354, -14326, -14325, -14317, -14294, -14287, -14271, -14248, -14223, -14216, -14203, -14197, -14189, -14173, -14166, -14147, -14133, -14111, -14104, -14085, -14059, -14038, -14017, -13992, -13987, -13979, -13944, -13940, -13937, -13933, -13911, -13906, -13903, -13898, -13869, -13856, -13837, -13823, -13805, -13797, -13790, -13773, -13757, -13748, -13747, -13740, -13699, -13693, -13684, -13675, -13657, -13652, -13650, -13622, -13614, -13587, -13583, -13577, -13549, -13538, -13528, -13477, -13472, -13462, -13455, -13453, -13422, -13412, -13399, -13387, -13315, -13268, -13183, -13122, -13013, -12998, -12997, -12957, -12940, -12901, -12892, -12863, -12850, -12802, -12772, -12678, -12672, -12643, -12630, -12600, -12592, -12580, -12565, -12518, -12444, -12402, -12390, -12384, -12362, -12346, -12332, -12323, -12312, -12299, -12294, -12288, -12267, -12250, -12247, -12126, -12124, -12093, -12085, -12084, -12076, -12072, -12064, -12034, -12029, -11923, -11922, -11897, -11890, -11888, -11856, -11854, -11822, -11798, -11798, -11568, -11489, -11471, -11466, -11453, -11442, -11432, -11430, -11422, -11405, -11397, -11395, -11387, -11370, -11363, -11337, -11299, -11291, -11256, -11248, -11199, -11163, -11162, -11159, -11131, -11128, -11116, -11086, -11085, -11062, -11056, -10961, -10917, -10884, -10866, -10859, -10846, -10841, -10838, -10827, -10824, -10822, -10815, -10805, -10796, -10785, -10784, -10782, -10748, -10718, -10688, -10659, -10639, -10630, -10565, -10518, -10505, -10473, -10455, -10451, -10443, -10440, -10421, -10412, -10409, -10399, -10391, -10386, -10376, -10366, -10351, -10329, -10311, -10301, -10290, -10284, -10282, -10264, -10255, -10238, -10228, -10185, -10172, -10169, -10095, -10076, -10048, -10004, -9961, -9932, -9927, -9914, -9909, -9904, -9892, -9874, -9866, -9851, -9812, -9804, -9792, -9783, -9770, -9767, -9736, -9727, -9724, -9674, -9622, -9599, -9594, -9592, -9575, -9573, -9557, -9554, -9530, -9526, -9485, -9444, -9408, -9387, -9371, -9264, -9264, -9258, -9258, -9257, -9253, -9253, -9248, -9243, -9242, -9240, -9121, -9121, -9093, -9082, -8881, -8879, -8856, -8793, -8719, -8715, -8715, -8715, -8633, -8614, -8613, -8595, -8123, -8123, -7869, -7747, -7741, -7663, -7518, -7501, -7475, -7471, -7455, -7452, -7428, -7426, -7424, -7408, -7380, -7380, -7354, -7327, -7324, -7312, -7304, -7292, -7284, -7245, -7245, -7241, -7227, -7219, -7198, -7191, -7188, -7187, -7181, -7169, -7144, -7120, -7097, -7097, -7072, -7049, -7047, -7023, -6947, -6944, -6886, -6875, -6857, -6847, -6833, -6809, -6775, -6775, -6754, -6729, -6715, -6686, -6667, -6645, -6627, -6616, -6606, -6601, -6593, -6577, -6565, -6562, -6544, -6544, -6516, -6477, -6452, -6442, -6429, -6427, -6399, -6356, -6356, -6338, -6317, -6275, -6245, -6214, -6184, -6163, -6154, -6097, -6074, -6074, -6047, -6023, -6001, -5995, -5989, -5977, -5973, -5968, -5964, -5951, -5940, -5939, -5926, -5906, -5904, -5876, -5823, -5823, -5803, -5786, -5782, -5712, -5694, -5670, -5627, -5618, -5554, -5527, -5527, -5474, -5461, -5406, -5388, -5388, -5365, -5308, -5256, -5231, -5224, -5223, -5206, -5198, -5187, -5167, -5154, -5139, -5109, -5105, -5086, -5058, -5058, -4997, -4971, -4965, -4935, -4906, -4860, -4835, -4823, -4809, -4773, -4753, -4709, -4689, -4639, -4636, -4619, -4583, -4583, -4579, -4536, -4533, -4497, -4479, -4466, -4444, -4403, -4402, -4354, -4345, -4269, -4268, -4215, -4189, -4160, -4134, -4109, -4093, -4061, -4061, -4011, -4011, -3667, -3549, -3500, -3402, -3399, -3309, -3297, -3276, -3235, -3185, -3184, -3148, -3127, -3103, -3062, -3059, -3021, -3016, -2513, -2476, -2440, -2428, -2397, -2379, -2343, -2328, -2317, -2296, -2234, -2180, -2074, -2017, -2002, -1954, -1949, -1920, -1903, -1903, -1722, -1700, -1696, -1668, -1660, -1658, -1649, -1633, -1625, -1622, -1614, -1597, -1589, -1563, -1525, -1516, -1467, -1446, -1446, -1382, -1370, -1325, -1307, -1307, -1292, -1216, -1167, -1165, -1161, -1129, -1126, -1112, -1078, -1076, -1054, -1026, -1026, -949, -941, -939, -915, -912, -887, -866, -857, -841, -835, -832, -820, -817, -807, -796, -785, -784, -769, -765, -762, -739, -716, -716, -604, -568, -539, -509, -495, -486, -476, -472, -454, -421, -347, -316, -316, -266, -247, -236, -233, -206, -191, -186, -162, -138, -138, -99, -80, -73, -56, -42, -25, -1, 16, 25, 38, 47, 49, 84, 110, 110, 150, 166, 186, 198, 251, 266, 269, 356, 379, 380, 404, 423, 423, 486, 519, 533, 539, 544, 558, 576, 585, 601, 649, 660, 677, 689, 707, 710, 746, 783, 783, 822, 828, 833, 864, 867, 889, 892, 917, 922, 953, 958, 997, 1093, 1100, 1217, 1217, 1217, 2858, 2858, 2861, 2989, 2993, 3012, 3014, 3028, 3031, 3047, 3050, 3074, 3076, 3079, 3098, 3119, 3119, 3175, 3177, 3189, 3197, 3210, 3218, 3256, 3270, 3277, 3298, 3305, 3308, 3309, 3315, 3328, 3355, 3381, 3395, 3395, 3443, 3445, 3468, 3547, 3550]\",\n          \"[-14232, -14232, -14232, -14231, -14201, -14201, -14199, -14186, -14186, -14168, -14166, -14157, -14085, -14083, -14083, -14017, -14016, -14009, -14009, -13956, -13956, -13951, -13943, -13886, -13885, -13885, -13856, -13856, -13856, -13827, -13785, -13653, -13650, -13620, -10313, -10274, -9954, -9891, -9882, -9815, -9708, -9708, -9708, -9619, -6699, -6698, -6697, -6696, -4130, -2309, -1571, -1558, -1452, -1259, -1238, -1173, -1173, -1165, -1162, -1150, -1147, -759, -750, -485, -467, 21, 37, 64, 132, 180, 187, 204, 258, 434, 464, 638, 652, 843, 859, 1072, 1087, 1347, 1362, 1616, 1641, 1808, 1825, 2442, 2607, 2623, 2848, 2958, 2983, 3175, 3389, 3413, -14230, -14229, -14228, -14185, -14184, -14048, -13954, -13954, -13826, -13825, -13817, -10313, -10313, -10274, -10274, -9954, -9954, -9891, -9891, -9891, -9814, -6707, -6697, -6697, -6697, -6695, -6695, -6695, -6413, -6408, -5510, -5504, -4426, -4378, -4177, -4128, -2307, -2266, -1863, -1569, -1556, -1545, -1462, -1462, -1450, -1447, -1257, -1236, -1227, -1173, -1164, -1148, -1026, -757, -747, -736, -483, -465, -452, -142, 23, 39, 51, 78, 134, 206, 213, 215, 249, 249, 258, 259, 290, 436, 466, 473, 484, 484, 488, 640, 655, 678, 845, 861, 874, 1074, 1089, 1349, 1364, 1376, 1618, 1643, 1691, 1698, 1810, 1827, 2108, 2444, 2466, 2466, 2583, 2589, 2608, 2611, 2625, 2639, 2639, 2758, 2850, 2864, 2864, 2876, 2942, 2942, 2960, 2962, 2985, 2997, 2997, 3009, 3177, 3192, 3192, 3204, 3391, 3415, 3427, 3551]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.18587482924386237,\n        \"min\": 50.897397382142856,\n        \"max\": 51.16026408656331,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          51.16026408656331,\n          50.897397382142856\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.17026887818025122,\n        \"min\": 4.158203474031008,\n        \"max\": 4.399000030803571,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4.158203474031008,\n          4.399000030803571\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_kph_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26.2, 26.9, 28.1, 26.9, 29.1, 31.4, 34.1, 34.3, 40.3, 40.0, 31.8, 31.5, 0.0, 9.2, 12.2, 19.7, 0.0, 1.1, 1.1, 0.8, 0.8, 0.3, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 131.4, 133.2, 137.3, 137.7, 137.2, 138.1, 138.1, 137.9, 138.0, 137.5, 137.9, 138.1, 138.2, 138.1, 138.5, 138.4, 138.3, 138.3, 138.0, 138.4, 137.6, 138.3, 138.0, 138.1, 138.6, 138.3, 137.5, 137.5, 137.8, 138.1, 138.1, 138.3, 138.2, 138.4, 138.5, 138.0, 138.0, 138.2, 138.0, 137.9, 137.8, 137.8, 138.2, 138.2, 138.3, 137.9, 138.2, 137.8, 138.3, 138.1, 149.9, 155.3, 158.5, 160.1, 158.5, 157.7, 157.9, 158.2, 158.0, 157.2, 155.7, 154.8, 116.8, 93.9, 85.9, 72.3, 71.1, 62.8, 59.8, 59.4, 49.0, 46.8, 41.9, 39.5, 28.9, 0.1, 0.0, 17.2, 56.5, 62.5, 62.4, 62.5, 61.0, 32.4, 28.3, 23.6, 23.8, 32.7, 33.4, 32.8, 45.2, 78.1, 81.2, 72.5, 61.3, 32.0, 0.0, 0.0, 31.0, 86.7, 91.5, 93.6, 103.3, 106.8, 104.9, 104.5, 103.8, 103.2, 102.9, 105.1, 121.2, 132.4, 133.2, 138.1, 137.9, 137.9, 138.4, 138.1, 138.1, 138.2, 138.3, 137.8, 137.8, 135.8, 135.4, 132.8, 129.8, 128.5, 65.2, 60.8, 0.0, 0.0, 0.0, 0.0, 0.0, 19.4, 35.8, 77.6, 97.1, 112.0, 114.4, 126.0, 137.4, 138.0, 138.2, 138.4, 138.7, 138.5, 138.2, 137.6, 137.8, 138.5, 138.0, 138.3, 134.6, 131.5, 125.0, 89.1, 88.2, 85.9, 71.2, 69.2, 12.1, 0.0, 0.0, 12.8, 50.8, 66.7, 72.5, 84.9, 88.9, 92.0, 103.6, 105.6, 107.7, 112.4, 113.6, 114.4, 116.9, 116.7, 117.3, 118.3, 117.8, 118.1, 118.0, 117.9, 117.0, 0.0, 0.0, 0.0, 27.4, 79.6, 87.0, 101.4, 105.2, 109.4, 108.6, 108.2, 106.6, 105.6, 104.5, 99.5, 84.6, 61.6, 58.8, 61.4, 83.0, 103.8, 109.8, 110.6, 112.2, 112.3, 105.8, 99.4, 85.5, 86.5, 86.8, 86.5, 73.9, 0.0, 0.0, 15.9, 83.0, 87.3, 83.8, 82.4, 81.6, 78.4, 75.4, 74.2, 67.1, 53.8, 52.6, 51.2, 47.2, 39.9, 38.3, 14.3, 5.3, 0.1, 0.0, 18.5, 56.9, 56.1, 56.4, 53.6, 53.0, 47.7, 46.5, 34.5, 33.2, 28.8, 13.5, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.2, 22.5, 9.5, 0.0, 13.2, 47.0, 50.9, 54.5, 54.2, 49.4, 47.9, 46.0, 0.0, 0.0, 0.0, 0.0, 34.6, 39.7, 54.8, 53.5, 52.0, 51.0, 66.0, 65.8, 68.6, 77.3, 82.4, 83.8, 83.8, 83.6, 83.5, 83.4, 83.5, 67.4, 0.0, 0.0, 0.0, 0.0, 33.7, 38.8, 81.9, 85.1, 84.9, 89.3, 96.8, 112.7, 98.0, 65.7, 0.0, 0.0, 0.0, 0.0, 37.1, 59.9, 57.8, 56.8, 56.5, 72.3, 91.1, 108.4, 111.3, 94.9, 45.8, 3.6, 0.0, 0.0, 0.0, 0.0, 62.7, 99.7, 111.9, 89.7, 80.2, 0.0, 0.0, 0.0, 0.0, 30.2, 115.1, 113.4, 111.9, 118.2, 114.1, 111.3, 0.0, 0.0, 0.0, 0.0, 42.6, 80.6, 89.7, 97.0, 116.3, 117.0, 114.0, 110.5, 97.6, 84.2, 81.4, 75.4, 68.1, 64.9, 0.0, 0.0, 0.0, 0.0, 21.5, 29.5, 39.7, 74.3, 105.9, 122.9, 120.1, 0.1, 0.0, 0.0, 70.4, 89.5, 0.1, 0.0, 0.0, 0.0, 93.1, 137.5, 136.6, 136.1, 136.0, 133.4, 131.2, 126.6, 101.8, 74.6, 67.7, 53.5, 47.2, 0.1, 0.0, 0.0, 0.0, 33.5, 35.4, 34.4, 38.3, 64.0, 83.1, 100.0, 116.8, 114.7, 105.9, 52.2, 50.0, 30.7, 29.3, 0.2, 0.0, 0.0, 0.0, 30.3, 30.0, 49.6, 73.0, 90.4, 112.5, 111.9, 111.9, 113.8, 108.7, 60.7, 60.5, 47.4, 35.5, 32.5, 30.4, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 22.9, 18.6, 30.2, 31.5, 81.8, 85.8, 82.8, 101.4, 79.4, 77.6, 54.9, 45.7, 40.7, 28.7, 28.7, 7.9, 0.0, 0.0, 17.4, 81.8, 95.9, 120.6, 119.7, 114.3, 107.8, 96.6, 66.7, 28.3, 0.2, 0.0, 0.0, 10.8, 31.3, 31.6, 0.1, 0.0, 0.0, 0.0, 29.7, 29.2, 99.4, 114.6, 118.0, 130.6, 135.1, 134.7, 134.4, 133.9, 136.0, 137.9, 135.8, 127.5, 124.0, 0.2, 0.0, 0.0, 80.0, 92.3, 0.2, 0.0, 0.0, 0.0, 100.6, 96.9, 95.7, 93.4, 81.8, 81.0, 78.9, 53.7, 48.5, 0.2, 0.0, 0.0, 0.0, 2.6, 5.7, 32.2, 35.7, 44.7, 54.7, 59.0, 72.8, 80.3, 84.8, 96.4, 98.3, 102.8, 99.8, 89.4, 87.3, 59.3, 55.2, 51.0, 0.0, 0.0, 0.0, 87.7, 111.3, 116.5, 110.8, 108.2, 75.3, 10.8, 0.0, 0.0, 47.0, 1.3, 0.0, 0.0, 52.7, 78.5, 77.2, 77.0, 74.0, 64.9, 57.0, 0.1, 0.0, 0.0, 31.0, 59.2, 66.1, 64.1, 63.4, 57.4, 56.5, 70.4, 77.8, 77.4, 75.1, 73.9, 0.1, 0.0, 0.0, 51.0, 81.0, 92.0, 83.5, 67.9, 70.6, 71.7, 72.0, 56.4, 53.9, 0.0, 0.0, 0.0, 39.2, 72.8, 73.3, 72.3, 71.9, 73.6, 69.6, 67.9, 62.0, 39.4, 37.6, 36.2, 35.0, 29.5, 27.1, 2.0, 0.0, 0.0, 13.8, 19.0, 24.4, 45.8, 45.9, 38.9, 37.7, 27.1, 27.5, 29.2, 29.0, 27.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 31.1, 38.1, 57.2, 57.1, 55.7, 55.3, 55.9, 55.5, 48.0, 45.1, 41.0, 1.9, 0.0, 0.0, 40.0, 43.7, 52.8, 52.2, 51.5, 51.2, 79.6, 80.8, 84.9, 83.0, 82.9, 82.7, 82.4, 82.1, 82.0, 56.5, 0.2, 0.0, 0.0, 38.1, 43.1, 76.6, 80.6, 80.8]\",\n          \"[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.4, 0.0, 56.7, 0.1, 0.0, 59.3, 59.3, 55.1, 51.9, 40.3, 40.3, 0.2, 0.0, 0.3, 0.0, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.3, 0.0, 0.5, 0.0, 0.2, 0.0, 0.3, 0.0, 0.9, 0.0, 0.4, 0.0, 0.5, 27.6, 0.2, 0.3, 36.3, 1.0, 0.2, 0.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.671875, 106.6875, 36.65625, 23.40625, 0.109375, 0.0, 0.0, 0.0, 0.234375, 0.0, 0.0, 61.046875, 60.671875, 56.71875, 56.1875, 0.015625, 0.0, 0.0, 60.359375, 54.890625, 40.34375, 27.09375, 0.15625, 0.0, 0.0, 0.234375, 0.0, 0.0, 50.0, 0.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 41.953125, 41.953125, 0.125, 0.0, 0.0, 0.25, 0.0, 0.0, 0.0, 0.0, 0.0, 0.25, 0.0, 0.0, 0.140625, 0.0, 0.0, 0.046875, 0.0, 0.109375, 0.0, 0.0, 0.21875, 0.0, 0.0, 0.0, 0.109375, 0.0, 31.34375, 0.109375, 0.0, 0.0, 46.8125, 42.234375, 27.625, 25.328125, 0.15625, 0.0, 0.0, 68.8125, 0.046875, 0.0, 0.0, 0.0, 62.90625, 62.71875, 36.34375, 33.25, 0.15625, 0.0, 0.0, 0.0, 0.15625, 0.0, 0.0, 0.0, 0.171875, 0.0, 0.0, 67.34375]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_ac_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_dc_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\",\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True, True, True, True, True, True, True, True]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 2,\n        \"max\": 9,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2,\n          9\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';', index_col=0)\n",
        "df.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecVectorizer(TransformerMixin):\n",
        "    def __init__(self, size=100, window=5, min_count=1, workers=4):\n",
        "        self.size = size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.w2v_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.w2v_model = Word2Vec(sentences, vector_size=self.size, window=self.window,\n",
        "                                  min_count=self.min_count, workers=self.workers)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        transformed_data = np.array([\n",
        "            np.mean([self.w2v_model.wv[word] for word in sentence.split() if word in self.w2v_model.wv]\n",
        "                    or [np.zeros(self.\n",
        "                                 size)], axis=0)\n",
        "            for sentence in X\n",
        "        ])\n",
        "        return csr_matrix(transformed_data)\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.fit(X, y)\n",
        "        return self.transform(X, y)"
      ],
      "metadata": {
        "id": "fOHHiU-pDaet"
      },
      "id": "fOHHiU-pDaet",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class EnsembleModel:\n",
        "    def __init__(self, trained_models, trained_vectorizers):\n",
        "\n",
        "        self.trained_models = trained_models\n",
        "        self.trained_vectorizers = trained_vectorizers\n",
        "        self.optimized_weights = None\n",
        "\n",
        "    def _generate_prediction_matrix(self, X):\n",
        "        predictions = {}\n",
        "        for (vect_name, samp_name, clf_name), model in self.trained_models.items():\n",
        "            vectorizer = deepcopy(self.trained_vectorizers[vect_name])\n",
        "            X_vect = vectorizer.transform(X).toarray()\n",
        "\n",
        "            # Collect probability predictions for multiclass\n",
        "            predictions[(vect_name, samp_name, clf_name)] = model.predict_proba(X_vect)\n",
        "\n",
        "        # Convert predictions to a 3D array: (num_samples, num_classes, num_models)\n",
        "        return np.stack(list(predictions.values()), axis=2)\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        pred_matrix = self._generate_prediction_matrix(X_train)\n",
        "\n",
        "        # Define fitness function for genetic algorithm\n",
        "        def fitness(weights):\n",
        "            # Apply weights to model predictions\n",
        "            weighted_pred = np.tensordot(pred_matrix, weights, axes=([2], [0]))\n",
        "            # Get the predicted class with the highest weighted probability\n",
        "            final_pred = np.argmax(weighted_pred, axis=1)\n",
        "            # Return negative weighted F1 score for optimization\n",
        "            return -f1_score(y_train, final_pred, average='weighted')\n",
        "\n",
        "        # Genetic algorithm for weight optimization\n",
        "        num_models = pred_matrix.shape[2]\n",
        "        bounds = [(0, 1)] * num_models\n",
        "        result = differential_evolution(fitness, bounds)\n",
        "\n",
        "        # Store optimized weights\n",
        "        self.optimized_weights = result.x\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.optimized_weights is None:\n",
        "            raise ValueError(\"The ensemble model must be trained using `train_ensemble` before prediction.\")\n",
        "\n",
        "        # Generate predictions for the input data\n",
        "        pred_matrix = self._generate_prediction_matrix(X)\n",
        "        weighted_pred = np.tensordot(pred_matrix, self.optimized_weights, axes=([2], [0]))\n",
        "        ensemble_pred = np.argmax(weighted_pred, axis=1)\n",
        "        return ensemble_pred\n"
      ],
      "metadata": {
        "id": "6CDFeUgXDg3F"
      },
      "id": "6CDFeUgXDg3F",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Representations:\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def representation_a(self, df):\n",
        "            events_types_dict = {}\n",
        "            for events_sequence in df['events_sequence']:\n",
        "                row_list = ast.literal_eval(events_sequence)  # Transforming string into actual list\n",
        "                unique_events = set(row_list)\n",
        "                for event in unique_events:\n",
        "                    if not events_types_dict.get(event):\n",
        "                        events_types_dict[event] = 0\n",
        "                    events_types_dict[event] += 1\n",
        "\n",
        "            # Step 2: Sort events by frequency and calculate percentages\n",
        "            sorted_dict = dict(sorted(events_types_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "            sorted_events_perc_df = pd.DataFrame(list(sorted_dict.items()), columns=['event_type', 'frequency'])\n",
        "            sorted_events_perc_df['percentage'] = sorted_events_perc_df['frequency'] / df.shape[0] * 100\n",
        "            sorted_events_perc_df['event_type'] = sorted_events_perc_df['event_type'].astype(str)  # Ensure event_type is string\n",
        "\n",
        "            # Step 3: Filter events with percentage <= 85%\n",
        "            events_low_frequency = list(map(int, list(sorted_events_perc_df[sorted_events_perc_df.percentage <= 85].event_type)))\n",
        "\n",
        "            # Step 4: Clean the 'events_sequence' column\n",
        "            df['clean_events_sequence'] = (\n",
        "                df['events_sequence']\n",
        "                .apply(ast.literal_eval)  # Convert string to list\n",
        "                .apply(lambda x: [i for i in x if i in events_low_frequency])  # Filter low-frequency events\n",
        "                .astype(str)\n",
        "                .replace(r'[\\[\\],]', '', regex=True)  # Remove brackets and commas\n",
        "            )\n",
        "\n",
        "            # Step 5: Extract the target column\n",
        "            self.y = df['incident_type'].copy()  # Separate target column\n",
        "            self.X = df['clean_events_sequence'].copy()\n",
        "            return self.X, self.y\n",
        "\n",
        "  def representation_b(self, df):\n",
        "\n",
        "    before_incident = []\n",
        "    after_incident = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Convertir las cadenas a listas\n",
        "        events = ast.literal_eval(row['events_sequence'])\n",
        "        seconds = ast.literal_eval(row['seconds_to_incident_sequence'])\n",
        "        incident_type = row['incident_type']\n",
        "\n",
        "        # Filtrar eventos antes del incidente (t <= 0)\n",
        "        before_events = \" \".join([str(event) for event, time in zip(events, seconds) if time <= 0])\n",
        "        if before_events:\n",
        "            before_incident.append({\n",
        "                \"events_sequence\": before_events,\n",
        "                \"class\": incident_type\n",
        "            })\n",
        "\n",
        "        # Filtrar eventos después del incidente (t > 0)\n",
        "        after_events = \" \".join([str(event) for event, time in zip(events, seconds) if time > 0])\n",
        "        if after_events:\n",
        "            after_incident.append({\n",
        "                \"events_sequence\": after_events,\n",
        "                \"class\": 100\n",
        "            })\n",
        "\n",
        "    # Convertir listas a DataFrames\n",
        "    before_df = pd.DataFrame(before_incident)\n",
        "    after_df = pd.DataFrame(after_incident)\n",
        "\n",
        "    return before_df, after_df\n",
        "\n",
        "\n",
        "  def representation_c(self, df, sequence_length=30):\n",
        "\n",
        "    overlapping_sequences = []\n",
        "    step = sequence_length // 2  # Paso entre ventanas solapadas\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        # Convertir eventos y tiempos en listas\n",
        "        events = ast.literal_eval(row['events_sequence'])\n",
        "        seconds = ast.literal_eval(row['seconds_to_incident_sequence'])\n",
        "        incident_type = row['incident_type']\n",
        "\n",
        "        # Generar cadenas antes del incidente\n",
        "        for i in range(0, len(events) - sequence_length + 1, step):\n",
        "            sequence = [str(i) for i in events[i:i + sequence_length]]\n",
        "            seconds_slice = seconds[i:i + sequence_length]\n",
        "            sequence_class = incident_type\n",
        "\n",
        "            overlapping_sequences.append({\"sequence\": \" \".join(sequence), \"class\": sequence_class})\n",
        "\n",
        "    # Crear un nuevo DataFrame con las secuencias generadas\n",
        "    sequences_df = pd.DataFrame(overlapping_sequences)\n",
        "    return sequences_df.sequence, sequences_df['class']\n"
      ],
      "metadata": {
        "id": "3zsHqb8oDiyJ"
      },
      "id": "3zsHqb8oDiyJ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Experiment:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.le = LabelEncoder()\n",
        "        self.y = self.le.fit_transform(y)\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=7, stratify=y)\n",
        "        self.y_train = self.le.transform(self.y_train)\n",
        "        self.y_test = self.le.transform(self.y_test)\n",
        "        self.trained_models = {}\n",
        "        self.trained_vectorizers = {}\n",
        "        self.trainer_samplers = {}\n",
        "        self.results = []\n",
        "        self.sampling_strategies = {\n",
        "            \"SMOTE\": SMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            #\"Borderline-SMOTE\": BorderlineSMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            #\"ADASYN\": ADASYN(sampling_strategy='auto', random_state=1, n_neighbors=3),\n",
        "            #\"RandomOversampler\": RandomOverSampler(sampling_strategy='auto', random_state=1),\n",
        "            #\"SMOTE-ENN\": SMOTEENN(sampling_strategy='auto', random_state=1),\n",
        "            #\"SMOTE-Tomek\": SMOTETomek(sampling_strategy='auto', random_state=1)\n",
        "        }\n",
        "\n",
        "        self.vectorizers = {\n",
        "            #\"TFIDF\": TfidfVectorizer(),\n",
        "            \"Count\": CountVectorizer(),\n",
        "            #\"Word2Vec\": Word2VecVectorizer(size=100, window=5, min_count=1)\n",
        "        }\n",
        "        self.classifiers = {\n",
        "            'LogisticRegression': LogisticRegression(),\n",
        "            'DecisionTree': DecisionTreeClassifier(),\n",
        "            'RandomForest': RandomForestClassifier(),\n",
        "            'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
        "            'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
        "            'AdaBoostClassifier': AdaBoostClassifier(),\n",
        "            'GaussianNB': GaussianNB(),\n",
        "            'KNN': KNeighborsClassifier(),\n",
        "            'SVM': SVC(probability=True),\n",
        "            'XGBoost': XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), eval_metric=\"mlogloss\", use_label_encoder=False),\n",
        "        }\n",
        "\n",
        "\n",
        "    def duplicate_minor_classes(self, X, y, min_instances=5):\n",
        "        X = X.reset_index(drop=True)\n",
        "        y  = y.reset_index(drop=True)\n",
        "        class_counts = y.value_counts()\n",
        "\n",
        "        # Identificar las clases minoritarias\n",
        "        minor_classes = class_counts[class_counts < min_instances].index\n",
        "\n",
        "        # Filtrar las filas correspondientes a las clases minoritarias\n",
        "        minor_class_rows = X.loc[y.isin(minor_classes)]\n",
        "        minor_class_labels = y.loc[y.isin(minor_classes)]\n",
        "\n",
        "        # Duplicar las filas y etiquetas correspondientes\n",
        "        duplicated_X = pd.concat([minor_class_rows] * 2, ignore_index=True)\n",
        "        duplicated_y = pd.concat([minor_class_labels] * 2, ignore_index=True)\n",
        "\n",
        "        # Combinar con las filas y etiquetas originales\n",
        "        X_balanced = pd.concat([X, duplicated_X], ignore_index=True)\n",
        "        y_balanced = pd.concat([y, duplicated_y], ignore_index=True)\n",
        "\n",
        "        X_balanced.reset_index(drop=True, inplace=True)\n",
        "        y_balanced.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "\n",
        "    def test(self, model, model_name, vectorizer, vectorizer_name, sampler, sampler_name):\n",
        "        \"\"\"Test a model with Stratified K-Fold and return an array with metric results.\"\"\"\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "        accuracies, recalls, precisions, f1s = [], [], [], []\n",
        "        X, y = self.X_train, self.y_train\n",
        "\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "            X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "            X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "            model.fit(X_resampled, y_resampled)\n",
        "\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            # Collect metrics for each fold\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "            recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
        "            precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
        "            f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "        # Return average metrics as a result array\n",
        "        return [\n",
        "            model_name, vectorizer_name, sampler_name,\n",
        "            np.mean(accuracies), np.std(accuracies),\n",
        "            np.mean(recalls), np.std(recalls),\n",
        "            np.mean(precisions), np.std(precisions),\n",
        "            np.mean(f1s), np.std(f1s)\n",
        "        ]\n",
        "\n",
        "    def training(self):\n",
        "        results = []\n",
        "        progress_bar = tqdm(total = len(self.vectorizers.keys()) * len(self.sampling_strategies.keys()) * len(self.classifiers.keys()))\n",
        "        self.X_train, self.y_train = self.duplicate_minor_classes(self.X_train, pd.Series(self.y_train))\n",
        "        # Iterate over vectorizers, samplers, and classifiers\n",
        "        for vect_name, ovectorizer in self.vectorizers.items():\n",
        "            vectorizer = deepcopy(ovectorizer)\n",
        "            for samp_name, osampler in self.sampling_strategies.items():\n",
        "                sampler = deepcopy(osampler)\n",
        "                for clf_name, omodel in self.classifiers.items():\n",
        "                    model = deepcopy(omodel)\n",
        "\n",
        "                    result = self.test(\n",
        "                        model=model,\n",
        "                        model_name=clf_name,\n",
        "                        vectorizer=vectorizer,\n",
        "                        vectorizer_name=vect_name,\n",
        "                        sampler=sampler,\n",
        "                        sampler_name=samp_name\n",
        "                    )\n",
        "                    # Store the trained model and add result to the results list\n",
        "                    results.append(result)\n",
        "\n",
        "                    vectorizer = deepcopy(ovectorizer)\n",
        "                    sampler = deepcopy(osampler)\n",
        "                    model = deepcopy(omodel)\n",
        "                    vectorizer.fit(self.X_train)\n",
        "                    X_train = vectorizer.transform(self.X_train).toarray()\n",
        "                    X_resampled, y_resampled = sampler.fit_resample(X_train, self.y_train)\n",
        "                    model.fit(X_resampled, y_resampled)\n",
        "                    self.trained_models[(vect_name, samp_name, clf_name)] = model\n",
        "                    self.trained_vectorizers[vect_name] = vectorizer\n",
        "                    self.trainer_samplers[(vect_name, samp_name)] = sampler\n",
        "                    progress_bar.update(1)\n",
        "\n",
        "        # Convert results to a DataFrame for better readability\n",
        "        columns = [\n",
        "            'Model', 'Vectorizer', 'Sampler',\n",
        "            'Accuracy Mean', 'Accuracy Std',\n",
        "            'Recall Mean', 'Recall Std',\n",
        "            'Precision Mean', 'Precision Std',\n",
        "            'F1 Mean', 'F1 Std'\n",
        "        ]\n",
        "        results_df = pd.DataFrame(results, columns=columns)\n",
        "        self.results = results_df\n",
        "        return results_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z5-kRVEJxLjj"
      },
      "id": "Z5-kRVEJxLjj",
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df.events_sequence, df.incident_type\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "XWMj2hzMHL9L",
        "outputId": "3a3de317-4917-437b-8843-cac2d7aca5df"
      },
      "id": "XWMj2hzMHL9L",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bace606ab6b7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincident_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnsembleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_vectorizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-08851ed46d4f>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     result = self.test(\n\u001b[0m\u001b[1;32m    113\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-08851ed46d4f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, model_name, vectorizer, vectorizer_name, sampler, sampler_name)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1348\u001b[0m             \u001b[0mn_threads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose, prefer=prefer)(\n\u001b[0m\u001b[1;32m   1351\u001b[0m             path_func(\n\u001b[1;32m   1352\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         )\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio, n_threads)\u001b[0m\n\u001b[1;32m    453\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             ]\n\u001b[0;32m--> 455\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    456\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    711\u001b[0m                                  **options)\n\u001b[1;32m    712\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 713\u001b[0;31m         res = _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    714\u001b[0m                                callback=callback, **options)\n\u001b[1;32m    715\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_lbfgsb_py.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    143\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m             \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m             \u001b[0;31m# Make sure the function returns a true scalar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misscalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;34m\"\"\" returns the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/scipy/optimize/_optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_linear_loss.py\u001b[0m in \u001b[0;36mloss_gradient\u001b[0;34m(self, coef, X, y, sample_weight, l2_reg_strength, n_threads, raw_prediction)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mraw_prediction\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_intercept_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_intercept\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoef\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "5BnuWhimN5ri"
      },
      "id": "5BnuWhimN5ri",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = Representations(df).representation_a(df)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "_aqzrdn9ukm_"
      },
      "id": "_aqzrdn9ukm_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "3_juRB6oN_xb"
      },
      "id": "3_juRB6oN_xb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_before, df_after = Representations(df).representation_b(df)\n",
        "X, y = pd.concat([df_before.events_sequence, df_after.events_sequence]), pd.concat([df_before['class'], df_after['class']])\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "razuBz2VOucS"
      },
      "id": "razuBz2VOucS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "eVEC5TGedBmj"
      },
      "id": "eVEC5TGedBmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = Representations(df).representation_c(df, sequence_length=100)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7frHCFPeHhB",
        "outputId": "ee18289d-8f47-4f39-9301-4b78e7ad9654"
      },
      "id": "A7frHCFPeHhB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "  0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\n",
            " 10%|█         | 1/10 [02:33<23:00, 153.41s/it]\u001b[A\n",
            " 20%|██        | 2/10 [02:57<10:18, 77.29s/it] \u001b[A\n",
            " 30%|███       | 3/10 [04:24<09:33, 81.92s/it]\u001b[A\n",
            " 40%|████      | 4/10 [07:38<12:35, 125.92s/it]\u001b[A\n",
            " 50%|█████     | 5/10 [1:47:09<3:06:09, 2233.95s/it]\u001b[A\n",
            " 60%|██████    | 6/10 [1:49:17<1:41:11, 1517.79s/it]\u001b[A\n",
            " 70%|███████   | 7/10 [1:49:28<51:15, 1025.15s/it]  \u001b[A\n",
            " 80%|████████  | 8/10 [1:49:50<23:31, 705.86s/it] \u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "ljaZ-7XPdDtn"
      },
      "id": "ljaZ-7XPdDtn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiQypjdvdf2N"
      },
      "id": "wiQypjdvdf2N",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}