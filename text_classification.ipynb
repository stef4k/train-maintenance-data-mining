{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stef4k/train-maintenance-data-mining/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee",
      "metadata": {
        "id": "922f85b5-bc81-4059-afe8-a86d8ec6d0ee"
      },
      "source": [
        "# Text classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9",
      "metadata": {
        "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import ast\n",
        "import pickle\n",
        "from joblib import Parallel, delayed\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.base import TransformerMixin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import differential_evolution\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58",
      "metadata": {
        "id": "6aba5d7c-c622-4044-a1f9-b01a88659d58"
      },
      "source": [
        "Manually remove the first ';' from the first row in csv file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
      "metadata": {
        "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
        "outputId": "75c3a7fd-e06f-49ac-d9ef-d3d5189c66fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     incident_id                                  vehicles_sequence  \\\n",
              "239      4444313  [1041, 1041, 1041, 1041, 1041, 1041, 1041, 104...   \n",
              "49       4435067  [551, 551, 551, 551, 551, 551, 551, 551, 551, ...   \n",
              "\n",
              "                                       events_sequence  \\\n",
              "239  [3636, 3658, 4078, 4124, 2956, 2956, 2956, 295...   \n",
              "49   [4066, 4068, 4068, 4068, 4068, 3658, 4068, 365...   \n",
              "\n",
              "                          seconds_to_incident_sequence  approx_lat  \\\n",
              "239  [-12732, -12732, -12723, -12722, -12674, -1266...   49.678917   \n",
              "49   [-14130, -13826, -13647, -13282, -13024, -1299...   50.817266   \n",
              "\n",
              "     approx_lon                                 train_kph_sequence  \\\n",
              "239    5.815721  [0.0, 0.0, 0.0, 0.0, 21.3, 29.8, 34.8, 36.6, 3...   \n",
              "49     4.383308  [0.03125, 0.15625, 0.1875, 0.0, 0.140625, 0.0,...   \n",
              "\n",
              "                                  dj_ac_state_sequence  \\\n",
              "239  [True, True, True, True, True, True, True, Tru...   \n",
              "49   [False, False, False, False, False, False, Fal...   \n",
              "\n",
              "                                  dj_dc_state_sequence  incident_type  \n",
              "239  [False, False, False, False, False, False, Fal...             13  \n",
              "49   [True, True, True, True, True, True, True, Tru...              4  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-00fcef9b-ecc5-4154-9ebb-12eacaa980a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incident_id</th>\n",
              "      <th>vehicles_sequence</th>\n",
              "      <th>events_sequence</th>\n",
              "      <th>seconds_to_incident_sequence</th>\n",
              "      <th>approx_lat</th>\n",
              "      <th>approx_lon</th>\n",
              "      <th>train_kph_sequence</th>\n",
              "      <th>dj_ac_state_sequence</th>\n",
              "      <th>dj_dc_state_sequence</th>\n",
              "      <th>incident_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>4444313</td>\n",
              "      <td>[1041, 1041, 1041, 1041, 1041, 1041, 1041, 104...</td>\n",
              "      <td>[3636, 3658, 4078, 4124, 2956, 2956, 2956, 295...</td>\n",
              "      <td>[-12732, -12732, -12723, -12722, -12674, -1266...</td>\n",
              "      <td>49.678917</td>\n",
              "      <td>5.815721</td>\n",
              "      <td>[0.0, 0.0, 0.0, 0.0, 21.3, 29.8, 34.8, 36.6, 3...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>4435067</td>\n",
              "      <td>[551, 551, 551, 551, 551, 551, 551, 551, 551, ...</td>\n",
              "      <td>[4066, 4068, 4068, 4068, 4068, 3658, 4068, 365...</td>\n",
              "      <td>[-14130, -13826, -13647, -13282, -13024, -1299...</td>\n",
              "      <td>50.817266</td>\n",
              "      <td>4.383308</td>\n",
              "      <td>[0.03125, 0.15625, 0.1875, 0.0, 0.140625, 0.0,...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-00fcef9b-ecc5-4154-9ebb-12eacaa980a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-00fcef9b-ecc5-4154-9ebb-12eacaa980a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-00fcef9b-ecc5-4154-9ebb-12eacaa980a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-bbdcdfbc-16ee-498e-b068-8b5833266301\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-bbdcdfbc-16ee-498e-b068-8b5833266301')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-bbdcdfbc-16ee-498e-b068-8b5833266301 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"incident_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6537,\n        \"min\": 4435067,\n        \"max\": 4444313,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4435067,\n          4444313\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vehicles_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 551, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078, 1078]\",\n          \"[1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041, 1041]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"events_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[4066, 4068, 4068, 4068, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4066, 3658, 4066, 4022, 3506, 2742, 4050, 4032, 2708, 4026, 2740, 4030, 4018, 4026, 3636, 3658, 4122, 2688, 2682, 4066, 2708, 2744, 4026, 4148, 3636, 3658, 4124, 4124, 4068, 3636, 3658, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4066, 4124, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4068, 3636, 3658, 4078, 4066, 3636, 3658, 4066, 3636, 3658, 4068, 3636, 3658, 4068, 3636, 3658, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4066, 3636, 3658, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4066, 2708, 2742, 4026, 2882, 4048, 2736, 4020, 4016, 4028, 4016, 4026, 4020, 3658, 4016, 4114, 4168, 4140, 3986, 4114, 4004, 4032, 4028, 2852, 4016, 4026, 4110, 2854, 4020, 4140, 4152, 4016, 4016, 4168, 3702, 3714, 4066, 4156, 3256, 4016, 4066, 4050, 4032, 2708, 2742, 4026, 2740, 4030, 4018, 4026, 3632, 3632, 4122, 4148, 2708, 2742, 4148, 3982, 4120, 2682, 4048, 2736, 2686, 2708, 2708, 4020, 4028, 4026, 4026, 4016, 4020, 4026, 4066, 4066, 4412, 4068, 3636, 3658, 4078, 4120, 4066, 3636, 3658, 4120, 4066, 3636, 3658, 4066, 3636, 3658, 4066, 3636, 3658, 4068, 3636, 3658, 4068, 3636, 3658, 4068, 3636, 3658, 4078, 4066, 3636, 3658, 4078, 4068, 3636, 3658, 4078, 4068, 3636, 3658, 4078, 4068, 3636, 3658, 4120, 4068, 3636, 3658, 4120, 4078, 4068, 3636, 3658, 4078, 4068, 3636, 3658, 4078, 4068, 3636, 3658, 4068, 3636, 3658, 4078, 4120, 4068, 3636, 3658, 4078, 4068, 3636, 3658, 4078, 4066, 3636, 3658, 4120, 4180, 4066, 2708, 2744, 2852, 2854, 4124, 2858, 2658, 2688, 3254, 3254, 3254, 3636, 3658, 4124, 4068, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4078, 4124, 4068, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4124, 4068, 3636, 3658, 4124, 4068, 3636, 3658, 4124, 4068, 3636, 3658, 4124, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4078, 4124, 4066, 3636, 3658, 4124, 4066, 3636, 3658, 4124, 4078, 4124, 3982, 4054, 4066, 2736, 2708, 2708, 4020, 4028, 2708, 2742, 4026, 4148, 4394, 2740, 4030, 4020, 4026, 2972, 3234, 2982, 3636, 4120, 3224, 2690, 3224, 3224, 3224, 3224, 2690, 2706, 2708, 2742, 4148, 4120, 3224, 2690, 4168, 4140, 2708, 3986, 2742, 4002, 4032, 2708, 4026, 4028, 2852, 4110, 2854, 2982, 4148, 4030, 4020, 4026, 4140, 4162, 4150, 4152, 2742, 4148, 2742, 4148, 4168, 4156, 4406, 4410, 4408, 4412, 4120, 3636, 3658, 4120, 4078, 3224, 4396, 2690, 4120, 3224, 4396, 2706, 3234, 3256, 4120, 3224, 4396, 2690, 3224, 4396, 4120, 3224, 4396, 2970, 2980, 2708, 2970, 2980, 2970, 2980, 2742, 4148, 4066, 4024, 3506, 2744, 4056, 4032, 2708, 3636, 3658, 4026, 2740, 3540, 3256, 372, 4396, 4056, 4124, 4148, 4030, 4018, 4126, 3224, 4396, 2690, 4124, 2682, 2684, 4126, 3224, 4396, 4126, 3224, 4396, 2690, 2708, 3082, 4394, 1286, 1720, 1740, 1760, 1780, 3032, 2654, 4392, 1640, 1660, 1680, 1700, 1138, 3132, 1800, 1820, 1840, 1860, 2652, 2652, 3260, 4094, 1138, 4394, 1200, 1202, 1286, 4396, 1286, 2742, 4148, 2652, 4026, 3260, 4092, 4120, 2708, 2744, 4026, 4148, 4054, 4016, 4020, 4028, 2708, 2742, 4026, 4148, 2740, 4030, 4020, 4026, 4120, 4120, 4066, 3636, 3658, 4120, 4066, 3636, 3658, 4120, 4120, 3982]\",\n          \"[3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 2742, 4026, 4158, 3636, 3658, 4120, 4078, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 2744, 4026, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4168, 2956, 2956, 4140, 4142, 2956, 4148, 2956, 4140, 4162, 4150, 4152, 4168, 4156, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3980, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 2742, 4026, 4148, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3980, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4168, 2956, 4140, 4142, 2956, 4158, 2956, 4140, 2956, 4162, 2956, 4160, 2956, 4168, 2956, 4166, 3980, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4078, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 3760, 3768, 2744, 4026, 4158, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2892, 3636, 3658, 4078, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4168, 2956, 2956, 4140, 2956, 2956, 4142, 2956, 4148, 2956, 4140, 4162, 4150, 4152, 4168, 2956, 4156, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 3760, 3768, 3636, 3658, 2742, 4026, 3632, 4120, 4120, 2940, 2956, 2956, 2708, 2744, 4124, 2956, 2686, 2708, 3236]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seconds_to_incident_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[-14130, -13826, -13647, -13282, -13024, -12997, -12651, -12625, -12512, -12497, -12324, -12296, -12031, -11994, -11825, -11795, -11627, -11606, -11279, -11260, -11065, -11034, -10784, -10660, -10658, -10652, -10652, -10651, -10647, -10647, -10642, -10641, -10640, -10640, -10627, -10627, -10611, -10596, -10595, -10578, -10576, -10445, -10445, -10444, -10292, -10292, -10264, -10250, -10015, -9994, -9994, -9781, -9767, -9767, -9756, -9549, -9530, -9530, -9518, -9373, -9326, -9326, -9318, -9153, -9100, -9100, -9084, -8837, -8786, -8786, -8773, -8603, -8587, -8560, -8560, -8551, -8290, -8259, -8259, -8248, -8052, -7778, -7778, -7765, -7402, -7378, -7378, -7367, -7206, -7181, -7181, -7169, -6917, -6892, -6892, -6879, -6501, -6482, -6482, -6205, -6182, -6182, -6021, -6005, -6005, -5880, -5847, -5847, -5623, -5555, -5555, -5525, -5281, -5267, -5267, -5256, -5157, -5127, -5127, -5119, -5008, -4975, -4975, -4968, -4814, -4794, -4794, -4634, -4608, -4608, -4439, -4422, -4422, -4410, -4294, -4279, -4279, -4267, -4104, -4085, -4085, -4073, -3891, -3824, -3824, -3793, -3554, -3504, -3504, -3497, -3065, -3056, -1777, -1777, -1130, -1084, -1083, -1079, -1078, -1070, -973, -973, -925, -31, 161, 212, 212, 217, 237, 237, 394, 394, 395, 396, 396, 396, 396, 398, 426, 438, 449, 497, 510, 518, 519, 519, 519, 521, 777, 1102, 1108, 1190, 1191, 1452, 1452, 1452, 1457, 1458, 1459, 1459, 1509, 1516, 1520, 1522, 1546, 2090, 2090, 2127, 2135, 2154, 2154, 2155, 2156, 2156, 2165, 2167, 2167, 2168, 2239, 2240, 2245, 2245, 2299, 2520, 2608, -14398, -14382, -14382, -14360, -14352, -14100, -14003, -14003, -13975, -13831, -13798, -13798, -13695, -13660, -13660, -13540, -13528, -13528, -13383, -13294, -13294, -13090, -13055, -13055, -12804, -12790, -12790, -12788, -12564, -12531, -12531, -12520, -12276, -12258, -12258, -12248, -12093, -12077, -12077, -12067, -11765, -11671, -11671, -11650, -11483, -11373, -11373, -11364, -11360, -11088, -11066, -11066, -11056, -10884, -10820, -10820, -10812, -10562, -10533, -10533, -10345, -10282, -10282, -10275, -10272, -10130, -10107, -10107, -10096, -9888, -9872, -9872, -9862, -9702, -9576, -9576, -9559, -9365, -9347, -9335, -9231, -9159, -9157, -9152, -9064, -9062, -9062, -9056, -9012, -9011, -8249, -8249, -8211, -7975, -7943, -7943, -7918, -7761, -7743, -7743, -7733, -7725, -7517, -7509, -7509, -7499, -7494, -7348, -7319, -7319, -7312, -7304, -7148, -7032, -7032, -7019, -6775, -6725, -6725, -6717, -6712, -6559, -6519, -6519, -6511, -6509, -6245, -6218, -6218, -6209, -6200, -6008, -5914, -5914, -5887, -5505, -5493, -5493, -5481, -5474, -5329, -5303, -5303, -5296, -5294, -5021, -4991, -4991, -4983, -4913, -4727, -4718, -4718, -4707, -4706, -4464, -4432, -4432, -4421, -4187, -4162, -4162, -4143, -4018, -4002, -4002, -3992, -3875, -3849, -3849, -3832, -3732, -3720, -3720, -3709, -3554, -3451, -3451, -3429, -3194, -3175, -3175, -3165, -3163, -3009, -2969, -2969, -2958, -2951, -2816, -2786, -2786, -2778, -2575, -2550, -2550, -2536, -2534, -2327, -2303, -2303, -2292, -2290, -2094, -2008, -2008, -1994, -1746, -1685, -1685, -1680, -1678, -1364, -1099, -1082, -1082, -1081, -1080, -1072, -1068, -1068, -972, -972, -972, -972, -970, -967, -925, -924, -924, -897, -871, -781, -29, -19, 0, 16, 28, 58, 85, 126, 132, 154, 160, 162, 162, 178, 190, 204, 214, 219, 228, 238, 392, 392, 392, 393, 393, 393, 394, 394, 396, 396, 397, 421, 424, 424, 436, 440, 444, 448, 494, 494, 507, 507, 516, 518, 534, 536, 543, 545, 579, 610, 610, 612, 623, 633, 633, 640, 670, 675, 675, 684, 739, 775, 854, 861, 861, 867, 879, 879, 911, 916, 916, 985, 993, 1000, 1010, 1019, 1063, 1071, 1099, 1099, 1106, 1179, 1183, 1189, 1189, 1190, 1194, 1194, 1194, 1194, 1199, 1206, 1207, 1209, 1209, 1238, 1262, 1267, 1276, 1277, 1282, 1288, 1288, 1294, 1308, 1312, 1312, 1312, 1321, 1321, 1331, 1335, 1336, 1342, 1359, 1619, 1620, 1624, 1630, 1630, 1630, 1630, 1684, 1685, 1685, 1695, 1695, 1695, 1695, 1701, 1827, 1838, 1838, 1838, 1838, 1860, 1894, 1895, 1895, 1895, 1911, 1911, 1911, 1918, 1933, 1937, 1938, 1938, 1939, 1939, 1940, 1940, 1977, 2000, 2045, 2045, 2045, 2152, 2154, 2154, 2165, 2237, 2237, 2237, 2237, 2242, 2242, 2243, 2243, 2265, 2296, 2297, 2438, 2438, 2517, 2518, 2564, 2564, 2573, 2580, 2645]\",\n          \"[-12732, -12732, -12723, -12722, -12674, -12662, -12645, -12639, -12612, -12610, -12587, -12583, -12574, -12563, -12527, -12520, -12517, -12510, -12480, -12469, -12460, -12458, -12444, -12429, -12414, -12387, -12313, -12313, -12307, -12305, -12284, -12264, -12246, -12237, -12234, -12219, -12216, -12165, -12164, -12133, -12119, -12119, -12109, -12083, -12081, -12066, -12065, -12036, -12008, -11897, -11897, -11891, -11884, -11860, -11853, -11843, -11831, -11817, -11810, -11801, -11799, -11788, -11786, -11746, -11733, -11721, -11696, -11682, -11682, -11672, -11646, -11644, -11629, -11600, -11591, -11581, -11545, -11528, -11510, -11491, -11435, -11418, -11400, -11399, -11346, -11329, -11244, -11244, -11244, -9486, -9486, -9485, -9480, -9420, -9399, -9376, -9330, -9316, -9306, -9294, -9263, -9254, -9245, -9213, -9190, -9187, -9164, -9153, -9153, -9143, -9118, -9106, -9095, -9067, -9057, -9056, -9049, -9043, -9031, -9021, -9011, -9004, -8981, -8968, -8968, -8957, -8927, -8915, -8906, -8905, -8894, -8892, -8874, -8863, -8863, -8853, -8824, -8823, -8787, -8786, -8777, -8775, -8769, -8750, -8730, -8710, -8655, -8655, -8649, -8648, -8616, -8611, -8608, -8605, -8596, -8585, -8584, -8578, -8569, -8543, -8537, -8534, -8528, -8486, -8468, -8452, -8421, -8393, -8386, -8371, -8355, -8341, -8289, -8270, -8158, -8158, -7388, -7388, -7381, -7381, -7335, -7325, -7308, -7303, -7275, -7250, -7246, -7236, -7226, -7189, -7182, -7179, -7172, -7141, -7128, -7118, -7116, -7099, -7082, -7065, -7038, -7026, -7026, -7016, -6992, -6971, -6957, -6951, -6948, -6938, -6936, -6888, -6887, -6859, -6797, -6797, -6786, -6777, -6758, -6756, -6742, -6740, -6707, -6682, -6556, -6556, -6551, -6548, -6522, -6517, -6508, -6497, -6483, -6476, -6467, -6466, -6455, -6454, -6417, -6402, -6390, -6364, -6313, -6313, -6310, -6309, -6283, -6281, -6264, -6233, -6221, -6209, -6173, -6162, -6154, -6146, -6124, -6112, -6106, -6101, -6067, -5954, -5954, -5949, -5948, -5922, -5895, -5891, -5883, -5882, -5874, -5869, -5867, -5864, -5842, -5833, -5832, -5827, -5824, -5823, -5822, -5815, -5814, -5810, -5795, -5789, -5785, -5784, -5779, -5771, -5768, -5759, -5751, -5737, -5696, -5689, -5676, -5669, -5657, -5656, -5651, -5647, -5635, -5621, -5612, -5604, -5602, -5601, -5570, -5531, -5522, -5503, -5396, -5376, -5364, -5247, -5247, -5246, -4786, -4786, -4772, -4630, -4618, -4612, -4590, -4564, -4562, -4560, -4552, -4541, -4526, -4506, -4506, -4501, -4500, -4488, -4481, -4468, -4462, -4420, -4408, -4401, -4399, -4398, -4391, -4377, -4367, -4361, -4347, -4340, -4337, -4336, -4335, -4334, -4327, -4326, -4325, -4324, -4318, -4315, -4291, -4288, -4285, -4281, -4272, -4271, -4263, -4260, -4235, -4216, -4135, -4135, -4125, -4123, -4089, -4088, -4084, -4081, -4071, -4070, -4055, -4049, -4043, -4034, -4005, -3996, -3987, -3959, -3942, -3941, -3918, -3895, -3895, -3883, -3882, -3853, -3840, -3826, -3784, -3781, -3768, -3767, -3756, -3747, -3730, -3717, -3708, -3702, -3676, -3607, -3607, -3595, -3593, -3555, -3536, -3524, -3523, -3510, -3508, -3483, -3429, -3429, -3419, -3417, -3383, -3382, -3341, -3339, -3330, -3328, -3322, -3308, -3290, -3270, -3250, -3250, -3238, -3232, -3197, -3192, -3187, -3184, -3173, -3160, -3159, -3152, -3143, -3118, -3111, -3109, -3103, -3057, -3042, -3028, -3002, -2979, -2972, -2959, -2948, -2940, -2895, -2884, -2620, -2620, -2381, -2381, -2380, -1991, -1991, -1976, -1974, -1927, -1918, -1901, -1896, -1869, -1846, -1842, -1833, -1823, -1786, -1780, -1777, -1771, -1744, -1735, -1728, -1727, -1714, -1701, -1687, -1659, -1632, -1632, -1623, -1622, -1600, -1578, -1565, -1559, -1557, -1548, -1546, -1506, -1480, -1393, -1393, -1381, -1380, -1355, -1354, -1338, -1336, -1303, -1273, -1149, -1149, -1139, -1137, -1109, -1103, -1094, -1082, -1069, -1062, -1054, -1052, -1041, -1040, -1002, -985, -970, -945, -911, -911, -901, -898, -870, -868, -851, -819, -807, -795, -760, -749, -742, -735, -716, -706, -701, -697, -667, -641, -541, -541, -530, -528, -503, -475, -471, -462, -461, -452, -448, -445, -443, -420, -412, -411, -410, -405, -403, -401, -393, -392, -385, -373, -358, -354, -353, -348, -338, -337, -336, -330, -318, -292, -289, -282, -278, -272, -268, -256, -245, -229, -228, -226, -204, -179, -173, -161, -68, -53, -36, -22, -2, -2, 24, 24, 550, 550, 577, 645, 888, 904, 931, 968, 1133, 1199, 1277, 1324, 1382, 1388, 1393]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8049346393018473,\n        \"min\": 49.678917,\n        \"max\": 50.817266483724566,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          50.817266483724566,\n          49.678917\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.0128692521727045,\n        \"min\": 4.383307766666666,\n        \"max\": 5.8157212,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4.383307766666666,\n          5.8157212\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_kph_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[0.03125, 0.15625, 0.1875, 0.0, 0.140625, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.046875, 0.0, 0.109375, 0.0, 0.046875, 0.0, 0.15625, 0.0, 0.078125, 0.0, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.078125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.203125, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.28125, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.0, 0.296875, 0.0, 0.0, 0.0, 0.171875, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.15625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.25, 0.890625, 0.5625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 6.140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.0, 0.125, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.0, 13.421875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0625, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.15625, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.03125, 0.0, 0.0, 0.0, 0.0, 0.109375, 0.0, 0.0, 0.0, 0.0, 0.078125, 0.0, 0.0, 0.0, 0.140625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.046875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.828125, 0.828125, 0.15625, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.09375, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.015625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 18.59375]\",\n          \"[0.0, 0.0, 0.0, 0.0, 21.3, 29.8, 34.8, 36.6, 39.0, 38.9, 68.9, 79.4, 98.5, 116.2, 120.6, 118.0, 116.8, 113.8, 101.4, 96.8, 93.5, 92.5, 85.7, 68.1, 46.8, 0.1, 0.0, 0.0, 0.0, 0.0, 32.2, 72.1, 68.2, 65.9, 64.9, 64.1, 65.4, 53.8, 52.7, 0.0, 0.0, 0.0, 0.0, 30.1, 35.5, 68.0, 70.8, 48.3, 0.0, 0.0, 0.0, 0.0, 0.0, 34.9, 52.1, 69.5, 85.3, 88.6, 87.1, 85.0, 84.6, 81.7, 81.5, 89.6, 76.6, 53.2, 0.0, 0.0, 0.0, 0.0, 49.0, 54.4, 88.5, 124.9, 122.9, 118.1, 77.3, 51.8, 44.7, 36.8, 34.5, 33.0, 27.6, 27.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 21.8, 25.9, 24.1, 53.7, 61.7, 84.6, 104.0, 123.2, 124.7, 124.2, 66.4, 40.5, 40.5, 0.2, 0.0, 0.0, 0.0, 49.3, 81.2, 104.8, 119.8, 111.5, 110.4, 109.5, 108.4, 99.2, 73.7, 50.1, 38.8, 0.2, 0.0, 0.0, 0.0, 63.0, 91.0, 87.1, 86.0, 51.8, 48.5, 0.1, 0.0, 0.0, 0.0, 59.9, 63.1, 114.0, 113.9, 101.9, 98.4, 82.8, 67.8, 46.9, 0.1, 0.0, 0.0, 0.0, 0.0, 65.4, 76.4, 84.2, 88.9, 106.1, 122.6, 125.5, 128.4, 128.2, 126.3, 126.6, 126.2, 125.8, 74.5, 53.8, 49.0, 42.2, 34.9, 33.8, 28.4, 21.6, 16.1, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 25.8, 33.2, 36.1, 37.3, 38.7, 66.5, 76.0, 94.2, 110.3, 121.9, 119.4, 118.2, 115.0, 93.4, 83.3, 79.6, 78.7, 72.8, 61.9, 46.4, 0.1, 0.0, 0.0, 0.0, 29.3, 82.0, 91.7, 89.7, 89.1, 84.9, 83.6, 53.7, 51.4, 0.1, 0.0, 0.0, 0.0, 0.0, 40.7, 46.4, 54.2, 54.4, 51.3, 0.0, 0.0, 0.0, 0.0, 0.0, 48.5, 64.2, 79.9, 82.6, 89.7, 92.8, 93.5, 93.0, 90.2, 89.8, 84.5, 68.8, 53.7, 0.1, 0.0, 0.0, 0.0, 0.0, 38.8, 44.3, 84.5, 100.3, 95.4, 97.0, 103.5, 102.7, 101.2, 95.6, 69.7, 57.7, 52.0, 44.0, 0.0, 0.0, 0.0, 0.0, 0.0, 30.5, 89.5, 95.5, 106.7, 108.8, 118.9, 120.5, 120.8, 121.7, 124.1, 126.4, 126.6, 127.5, 126.0, 125.4, 125.3, 121.2, 120.6, 118.5, 115.0, 113.1, 111.9, 111.4, 109.3, 105.6, 104.5, 94.5, 83.8, 70.2, 52.2, 52.8, 57.2, 57.3, 55.7, 55.5, 59.3, 69.1, 89.6, 106.1, 115.5, 111.6, 108.2, 104.7, 67.2, 45.9, 38.7, 36.0, 28.3, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.4, 66.1, 80.1, 96.6, 101.2, 101.6, 102.1, 105.2, 97.3, 72.6, 56.3, 56.6, 57.7, 57.9, 58.1, 56.5, 53.8, 54.2, 70.7, 97.9, 109.9, 113.3, 113.3, 112.8, 112.6, 111.7, 110.9, 109.5, 108.8, 109.0, 109.1, 109.2, 109.2, 109.7, 109.7, 109.8, 109.8, 109.9, 110.0, 113.3, 113.1, 111.6, 111.0, 110.2, 110.1, 106.6, 98.4, 42.8, 0.1, 0.0, 0.0, 0.0, 0.0, 57.1, 62.6, 71.5, 78.0, 96.8, 99.2, 121.5, 128.8, 127.6, 126.8, 127.8, 128.2, 127.7, 85.6, 53.3, 53.2, 0.0, 0.0, 0.0, 0.0, 0.0, 39.7, 73.8, 78.4, 74.8, 74.5, 74.3, 74.4, 74.3, 73.7, 71.8, 69.5, 57.8, 45.1, 0.0, 0.0, 0.0, 0.0, 0.0, 35.2, 67.7, 67.6, 67.2, 48.7, 45.8, 0.1, 0.0, 0.0, 0.0, 0.0, 42.9, 46.3, 96.7, 97.4, 98.2, 98.7, 97.5, 83.3, 51.9, 0.1, 0.0, 0.0, 0.0, 0.0, 53.4, 63.4, 70.6, 74.8, 89.8, 106.2, 109.9, 121.4, 128.8, 128.0, 128.3, 127.9, 127.6, 77.2, 69.8, 57.3, 51.2, 38.1, 37.1, 35.5, 33.2, 28.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 28.0, 34.0, 38.9, 39.5, 39.5, 72.0, 81.2, 100.0, 116.2, 124.2, 122.7, 122.2, 122.5, 120.8, 119.4, 115.5, 114.1, 97.8, 75.3, 53.7, 0.1, 0.0, 0.0, 0.0, 0.0, 28.3, 80.5, 102.4, 101.2, 100.3, 96.5, 95.4, 67.8, 0.1, 0.0, 0.0, 0.0, 0.0, 35.9, 40.2, 54.1, 54.3, 51.5, 0.1, 0.0, 0.0, 0.0, 0.0, 41.1, 56.4, 71.7, 86.8, 94.5, 93.1, 91.2, 90.8, 87.9, 87.7, 80.9, 59.2, 42.7, 0.1, 0.0, 0.0, 0.0, 0.0, 41.8, 46.4, 81.8, 97.2, 97.1, 97.4, 108.3, 107.6, 105.9, 104.9, 82.9, 68.9, 60.8, 53.1, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 32.8, 83.1, 88.7, 101.3, 103.5, 114.7, 120.2, 121.4, 122.1, 122.1, 129.4, 128.9, 127.7, 125.6, 124.3, 123.8, 119.8, 119.4, 116.3, 113.7, 108.5, 107.1, 106.7, 104.3, 100.0, 99.6, 98.9, 95.8, 91.5, 97.7, 100.9, 105.4, 106.4, 108.2, 109.7, 113.7, 116.2, 119.7, 120.1, 120.4, 102.1, 74.2, 65.8, 45.8, 35.3, 25.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 11.2, 17.0, 20.5, 0.0, 0.0, 0.0, 19.8, 0.0, 0.0, 0.0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_ac_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_dc_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6,\n        \"min\": 4,\n        \"max\": 13,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4,\n          13\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';', index_col=0)\n",
        "df.sample(2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecVectorizer(TransformerMixin):\n",
        "    def __init__(self, size=100, window=5, min_count=1, workers=4):\n",
        "        self.size = size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.w2v_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.w2v_model = Word2Vec(sentences, vector_size=self.size, window=self.window,\n",
        "                                  min_count=self.min_count, workers=self.workers)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        transformed_data = np.array([\n",
        "            np.mean([self.w2v_model.wv[word] for word in sentence.split() if word in self.w2v_model.wv]\n",
        "                    or [np.zeros(self.\n",
        "                                 size)], axis=0)\n",
        "            for sentence in X\n",
        "        ])\n",
        "        return csr_matrix(transformed_data)\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.fit(X, y)\n",
        "        return self.transform(X, y)"
      ],
      "metadata": {
        "id": "fOHHiU-pDaet"
      },
      "id": "fOHHiU-pDaet",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleModel:\n",
        "    def __init__(self, trained_models, trained_vectorizers):\n",
        "\n",
        "        self.trained_models = trained_models\n",
        "        self.trained_vectorizers = trained_vectorizers\n",
        "        self.optimized_weights = None\n",
        "\n",
        "    def _generate_prediction_matrix(self, X):\n",
        "        predictions = {}\n",
        "        for (vect_name, samp_name, clf_name), model in self.trained_models.items():\n",
        "            vectorizer = deepcopy(self.trained_vectorizers[vect_name])\n",
        "            X_vect = vectorizer.transform(X).toarray()\n",
        "\n",
        "            predictions[(vect_name, samp_name, clf_name)] = model.predict_proba(X_vect)\n",
        "\n",
        "        return np.stack(list(predictions.values()), axis=2)\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        pred_matrix = self._generate_prediction_matrix(X_train)\n",
        "\n",
        "        def fitness(weights):\n",
        "            weighted_pred = np.tensordot(pred_matrix, weights, axes=([2], [0]))\n",
        "            final_pred = np.argmax(weighted_pred, axis=1)\n",
        "            return -f1_score(y_train, final_pred, average='weighted')\n",
        "\n",
        "        num_models = pred_matrix.shape[2]\n",
        "        bounds = [(0, 1)] * num_models\n",
        "        result = differential_evolution(fitness, bounds)\n",
        "\n",
        "        self.optimized_weights = result.x\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.optimized_weights is None:\n",
        "            raise ValueError(\"The ensemble model must be trained using `train_ensemble` before prediction.\")\n",
        "\n",
        "        pred_matrix = self._generate_prediction_matrix(X)\n",
        "        weighted_pred = np.tensordot(pred_matrix, self.optimized_weights, axes=([2], [0]))\n",
        "        ensemble_pred = np.argmax(weighted_pred, axis=1)\n",
        "        return ensemble_pred\n"
      ],
      "metadata": {
        "id": "6CDFeUgXDg3F"
      },
      "id": "6CDFeUgXDg3F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Representations:\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def representation_a(self, df):\n",
        "            events_types_dict = {}\n",
        "            for events_sequence in df['events_sequence']:\n",
        "                row_list = ast.literal_eval(events_sequence)\n",
        "                unique_events = set(row_list)\n",
        "                for event in unique_events:\n",
        "                    if not events_types_dict.get(event):\n",
        "                        events_types_dict[event] = 0\n",
        "                    events_types_dict[event] += 1\n",
        "\n",
        "            sorted_dict = dict(sorted(events_types_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "            sorted_events_perc_df = pd.DataFrame(list(sorted_dict.items()), columns=['event_type', 'frequency'])\n",
        "            sorted_events_perc_df['percentage'] = sorted_events_perc_df['frequency'] / df.shape[0] * 100\n",
        "            sorted_events_perc_df['event_type'] = sorted_events_perc_df['event_type'].astype(str)\n",
        "\n",
        "            events_low_frequency = list(map(int, list(sorted_events_perc_df[sorted_events_perc_df.percentage <= 85].event_type)))\n",
        "\n",
        "            df['clean_events_sequence'] = (\n",
        "                df['events_sequence']\n",
        "                .apply(ast.literal_eval)\n",
        "                .apply(lambda x: [i for i in x if i in events_low_frequency])\n",
        "                .astype(str)\n",
        "                .replace(r'[\\[\\],]', '', regex=True)\n",
        "            )\n",
        "\n",
        "            self.y = df['incident_type'].copy()\n",
        "            self.X = df['clean_events_sequence'].copy()\n",
        "            return self.X, self.y\n",
        "\n",
        "  def representation_b(self, df):\n",
        "\n",
        "    before_incident = []\n",
        "    after_incident = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        events = ast.literal_eval(row['events_sequence'])\n",
        "        seconds = ast.literal_eval(row['seconds_to_incident_sequence'])\n",
        "        incident_type = row['incident_type']\n",
        "\n",
        "        before_events = \" \".join([str(event) for event, time in zip(events, seconds) if time <= 0])\n",
        "        if before_events:\n",
        "            before_incident.append({\n",
        "                \"events_sequence\": before_events,\n",
        "                \"class\": incident_type\n",
        "            })\n",
        "\n",
        "        after_events = \" \".join([str(event) for event, time in zip(events, seconds) if time > 0])\n",
        "        if after_events:\n",
        "            after_incident.append({\n",
        "                \"events_sequence\": after_events,\n",
        "                \"class\": 100\n",
        "            })\n",
        "\n",
        "    before_df = pd.DataFrame(before_incident)\n",
        "    after_df = pd.DataFrame(after_incident)\n",
        "\n",
        "    return before_df, after_df\n",
        "\n",
        "\n",
        "  def representation_c(self, df, sequence_length=30):\n",
        "\n",
        "    overlapping_sequences = []\n",
        "    step = sequence_length // 2\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        events = ast.literal_eval(row['events_sequence'])\n",
        "        seconds = ast.literal_eval(row['seconds_to_incident_sequence'])\n",
        "        incident_type = row['incident_type']\n",
        "\n",
        "        for i in range(0, len(events) - sequence_length + 1, step):\n",
        "            sequence = [str(i) for i in events[i:i + sequence_length]]\n",
        "            seconds_slice = seconds[i:i + sequence_length]\n",
        "            sequence_class = incident_type\n",
        "\n",
        "            overlapping_sequences.append({\"sequence\": \" \".join(sequence), \"class\": sequence_class})\n",
        "\n",
        "    sequences_df = pd.DataFrame(overlapping_sequences)\n",
        "    return sequences_df.sequence, sequences_df['class']\n"
      ],
      "metadata": {
        "id": "3zsHqb8oDiyJ"
      },
      "id": "3zsHqb8oDiyJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Experiment:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.le = LabelEncoder()\n",
        "        self.y = self.le.fit_transform(y)\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=7, stratify=y)\n",
        "        self.y_train = self.le.transform(self.y_train)\n",
        "        self.y_test = self.le.transform(self.y_test)\n",
        "        self.trained_models = {}\n",
        "        self.trained_vectorizers = {}\n",
        "        self.trainer_samplers = {}\n",
        "        self.results = []\n",
        "        self.sampling_strategies = {\n",
        "            \"SMOTE\": SMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            \"Borderline-SMOTE\": BorderlineSMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            \"ADASYN\": ADASYN(sampling_strategy='auto', random_state=1, n_neighbors=3),\n",
        "            \"RandomOversampler\": RandomOverSampler(sampling_strategy='auto', random_state=1),\n",
        "            \"SMOTE-ENN\": SMOTEENN(sampling_strategy='auto', random_state=1),\n",
        "            \"SMOTE-Tomek\": SMOTETomek(sampling_strategy='auto', random_state=1)\n",
        "        }\n",
        "\n",
        "        self.vectorizers = {\n",
        "            \"TFIDF\": TfidfVectorizer(),\n",
        "            \"Count\": CountVectorizer(),\n",
        "            \"Word2Vec\": Word2VecVectorizer(size=100, window=5, min_count=1)\n",
        "        }\n",
        "        self.classifiers = {\n",
        "            'LogisticRegression': LogisticRegression(),\n",
        "            'DecisionTree': DecisionTreeClassifier(),\n",
        "            'RandomForest': RandomForestClassifier(),\n",
        "            'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
        "            'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
        "            'AdaBoostClassifier': AdaBoostClassifier(),\n",
        "            'GaussianNB': GaussianNB(),\n",
        "            'KNN': KNeighborsClassifier(),\n",
        "            'SVM': SVC(probability=True),\n",
        "            'XGBoost': XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), eval_metric=\"mlogloss\", use_label_encoder=False, n_jobs = 2),\n",
        "        }\n",
        "\n",
        "\n",
        "    def duplicate_minor_classes(self, X, y, min_instances=5):\n",
        "        X = X.reset_index(drop=True)\n",
        "        y  = y.reset_index(drop=True)\n",
        "        class_counts = y.value_counts()\n",
        "\n",
        "        minor_classes = class_counts[class_counts < min_instances].index\n",
        "\n",
        "        minor_class_rows = X.loc[y.isin(minor_classes)]\n",
        "        minor_class_labels = y.loc[y.isin(minor_classes)]\n",
        "\n",
        "        duplicated_X = pd.concat([minor_class_rows] * 2, ignore_index=True)\n",
        "        duplicated_y = pd.concat([minor_class_labels] * 2, ignore_index=True)\n",
        "\n",
        "        X_balanced = pd.concat([X, duplicated_X], ignore_index=True)\n",
        "        y_balanced = pd.concat([y, duplicated_y], ignore_index=True)\n",
        "\n",
        "        X_balanced.reset_index(drop=True, inplace=True)\n",
        "        y_balanced.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "\n",
        "    def test(self, model, model_name, vectorizer, vectorizer_name, sampler, sampler_name):\n",
        "        \"\"\"Test a model with Stratified K-Fold and return an array with metric results.\"\"\"\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "        accuracies, recalls, precisions, f1s = [], [], [], []\n",
        "        X, y = self.X_train, self.y_train\n",
        "\n",
        "        for train_index, test_index in skf.split(X, y):\n",
        "            X_train, X_test = X[train_index], X[test_index]\n",
        "            y_train, y_test = y[train_index], y[test_index]\n",
        "            X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "            X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "            X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "            model.fit(X_resampled, y_resampled)\n",
        "\n",
        "            y_pred = model.predict(X_test)\n",
        "\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "            recalls.append(recall_score(y_test, y_pred, average='weighted'))\n",
        "            precisions.append(precision_score(y_test, y_pred, average='weighted'))\n",
        "            f1s.append(f1_score(y_test, y_pred, average='weighted'))\n",
        "\n",
        "        return [\n",
        "            model_name, vectorizer_name, sampler_name,\n",
        "            np.mean(accuracies), np.std(accuracies),\n",
        "            np.mean(recalls), np.std(recalls),\n",
        "            np.mean(precisions), np.std(precisions),\n",
        "            np.mean(f1s), np.std(f1s)\n",
        "        ]\n",
        "\n",
        "    def training(self):\n",
        "        results = []\n",
        "        progress_bar = tqdm(total = len(self.vectorizers.keys()) * len(self.sampling_strategies.keys()) * len(self.classifiers.keys()))\n",
        "        self.X_train, self.y_train = self.duplicate_minor_classes(self.X_train, pd.Series(self.y_train))\n",
        "        for vect_name, ovectorizer in self.vectorizers.items():\n",
        "            vectorizer = deepcopy(ovectorizer)\n",
        "            for samp_name, osampler in self.sampling_strategies.items():\n",
        "                sampler = deepcopy(osampler)\n",
        "                for clf_name, omodel in self.classifiers.items():\n",
        "                    model = deepcopy(omodel)\n",
        "\n",
        "                    result = self.test(\n",
        "                        model=model,\n",
        "                        model_name=clf_name,\n",
        "                        vectorizer=vectorizer,\n",
        "                        vectorizer_name=vect_name,\n",
        "                        sampler=sampler,\n",
        "                        sampler_name=samp_name\n",
        "                    )\n",
        "                    results.append(result)\n",
        "\n",
        "                    vectorizer = deepcopy(ovectorizer)\n",
        "                    sampler = deepcopy(osampler)\n",
        "                    model = deepcopy(omodel)\n",
        "                    vectorizer.fit(self.X_train)\n",
        "                    X_train = vectorizer.transform(self.X_train).toarray()\n",
        "                    X_resampled, y_resampled = sampler.fit_resample(X_train, self.y_train)\n",
        "                    model.fit(X_resampled, y_resampled)\n",
        "                    self.trained_models[(vect_name, samp_name, clf_name)] = model\n",
        "                    self.trained_vectorizers[vect_name] = vectorizer\n",
        "                    self.trainer_samplers[(vect_name, samp_name)] = sampler\n",
        "                    progress_bar.update(1)\n",
        "\n",
        "        columns = [\n",
        "            'Model', 'Vectorizer', 'Sampler',\n",
        "            'Accuracy Mean', 'Accuracy Std',\n",
        "            'Recall Mean', 'Recall Std',\n",
        "            'Precision Mean', 'Precision Std',\n",
        "            'F1 Mean', 'F1 Std'\n",
        "        ]\n",
        "        results_df = pd.DataFrame(results, columns=columns)\n",
        "        self.results = results_df\n",
        "        return results_df\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z5-kRVEJxLjj"
      },
      "id": "Z5-kRVEJxLjj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Evolution:\n",
        "    def __init__(self, X, y):\n",
        "\n",
        "        self.original_X = X\n",
        "        self.original_y = y\n",
        "        self.sequences = [x.split() for x in X]\n",
        "        self.labels = y\n",
        "        self.keep = self.get_unique_events()\n",
        "        self.remove = []\n",
        "        self.candidates = self.get_candidates()\n",
        "        self.next = self.candidates[0] if self.candidates else None\n",
        "        self.best_score = None\n",
        "\n",
        "    def get_unique_events(self):\n",
        "        return list(set(event for seq in self.sequences for event in seq))\n",
        "\n",
        "    def get_candidates(self):\n",
        "        sequences_flat = [(i, event) for i, seq in enumerate(self.sequences) for event in seq]\n",
        "        df = pd.DataFrame(sequences_flat, columns=[\"seq_idx\", \"event\"])\n",
        "        total_occurrences = df.shape[0]\n",
        "\n",
        "        class_counts = df.groupby(\"event\").apply(\n",
        "            lambda x: len(set(self.labels[x[\"seq_idx\"].values]))\n",
        "        )\n",
        "        event_counts = df[\"event\"].value_counts()\n",
        "        gamma_scores = (class_counts / len(set(self.labels))) * (event_counts / total_occurrences)\n",
        "\n",
        "        return gamma_scores.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "    def calculate_fitness(self):\n",
        "        mutated_X = [' '.join(seq) for seq in self.sequences]\n",
        "        mutated_y = self.labels\n",
        "\n",
        "        skf = StratifiedKFold(n_splits=5)\n",
        "        f1_scores = []\n",
        "\n",
        "        def fit_model(train_idx, test_idx):\n",
        "            X_train = [mutated_X[i] for i in train_idx]\n",
        "            X_test = [mutated_X[i] for i in test_idx]\n",
        "            y_train, y_test = mutated_y[train_idx], mutated_y[test_idx]\n",
        "\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            X_train_transformed = vectorizer.fit_transform(X_train).toarray()\n",
        "            X_test_transformed = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "            smote = SMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3)\n",
        "            X_resampled, y_resampled = smote.fit_resample(X_train_transformed, y_train)\n",
        "\n",
        "            model = XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), eval_metric=\"mlogloss\", use_label_encoder=False, n_jobs = 2)\n",
        "            model.fit(X_resampled, y_resampled)\n",
        "\n",
        "            y_pred = model.predict(X_test_transformed)\n",
        "            return f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "        f1_scores = Parallel(n_jobs=-1)(\n",
        "            delayed(fit_model)(train_idx, test_idx) for train_idx, test_idx in skf.split(mutated_X, mutated_y)\n",
        "        )\n",
        "        mean_f1 = np.mean(f1_scores)\n",
        "        std_f1 = np.std(f1_scores)\n",
        "        return mean_f1 - std_f1\n",
        "\n",
        "    def mutate(self):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = self.calculate_fitness()\n",
        "            print(\"Starting score:\", self.best_score)\n",
        "            return\n",
        "\n",
        "        modified_sequences = [[event for event in seq if event != self.next] for seq in self.sequences]\n",
        "\n",
        "        original_sequences = self.sequences[:]\n",
        "\n",
        "        self.sequences = modified_sequences\n",
        "        fitness_score = self.calculate_fitness()\n",
        "\n",
        "        if fitness_score >= self.best_score:\n",
        "            self.remove.append(self.next)\n",
        "            self.keep.remove(self.next)\n",
        "            self.best_score = fitness_score\n",
        "            print(\"New best score:\", self.best_score)\n",
        "        elif self.best_score - fitness_score <= 0.03:\n",
        "            self.sequences = [seq for seq in modified_sequences if seq]\n",
        "            self.remove.append(self.next)\n",
        "            self.keep.remove(self.next)\n",
        "            print(\"Noise removed\", fitness_score)\n",
        "        else:\n",
        "            self.sequences = original_sequences\n",
        "            print(\"No changes\", fitness_score)\n",
        "\n",
        "        self.candidates = [e for e in self.candidates if e not in self.remove]\n",
        "        self.next = self.candidates[0] if self.candidates else None\n",
        "\n",
        "    def save_state(self, filename):\n",
        "        with open(filename, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "        print(f\"State saved to {filename}.\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_state(cls, filename):\n",
        "        with open(filename, 'rb') as f:\n",
        "            obj = pickle.load(f)\n",
        "        print(f\"State loaded from {filename}.\")\n",
        "        return obj\n",
        "\n",
        "    def run(self, generations):\n",
        "        c = 0\n",
        "        for _ in tqdm(range(generations)):\n",
        "            if not self.next:\n",
        "                break\n",
        "            self.mutate()\n",
        "            if c == 30:\n",
        "              self.save_state(\"evolution_state.pkl\")\n",
        "              files.download('evolution_state.pkl')\n",
        "              c = 0\n",
        "            c += 1\n"
      ],
      "metadata": {
        "id": "pPZNbecHzfTz"
      },
      "id": "pPZNbecHzfTz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def duplicate_minor_classes(X, y, min_instances=10):\n",
        "    X = X.reset_index(drop=True)\n",
        "    y  = y.reset_index(drop=True)\n",
        "    class_counts = y.value_counts()\n",
        "\n",
        "    minor_classes = class_counts[class_counts <= min_instances].index\n",
        "\n",
        "    minor_class_rows = X.loc[y.isin(minor_classes)]\n",
        "    minor_class_labels = y.loc[y.isin(minor_classes)]\n",
        "\n",
        "    duplicated_X = pd.concat([minor_class_rows] * 2, ignore_index=True)\n",
        "    duplicated_y = pd.concat([minor_class_labels] * 2, ignore_index=True)\n",
        "\n",
        "    X_balanced = pd.concat([X, duplicated_X], ignore_index=True)\n",
        "    y_balanced = pd.concat([y, duplicated_y], ignore_index=True)\n",
        "\n",
        "    X_balanced.reset_index(drop=True, inplace=True)\n",
        "    y_balanced.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    return X_balanced, y_balanced"
      ],
      "metadata": {
        "id": "FEn5DGXm_hyE"
      },
      "id": "FEn5DGXm_hyE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the pipeline with no sequence modification:"
      ],
      "metadata": {
        "id": "5JiAkAiE3qUo"
      },
      "id": "5JiAkAiE3qUo"
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = duplicate_minor_classes(df.events_sequence.apply(lambda x: \" \".join([str(i) for i in ast.literal_eval(x)])), df.incident_type)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWMj2hzMHL9L",
        "outputId": "585fae31-c1c8-416b-cf29-33b942fd6a77"
      },
      "id": "XWMj2hzMHL9L",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|| 180/180 [4:34:36<00:00, 91.54s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.83      0.78        24\n",
            "           1       1.00      1.00      1.00         3\n",
            "           2       0.92      0.69      0.79        16\n",
            "           3       0.50      0.25      0.33         4\n",
            "           4       0.67      1.00      0.80         2\n",
            "           5       0.62      0.78      0.69        23\n",
            "           6       0.00      0.00      0.00         5\n",
            "           7       0.88      0.80      0.84        64\n",
            "           8       0.70      0.53      0.60        30\n",
            "           9       0.67      1.00      0.80         2\n",
            "          10       0.75      1.00      0.86         6\n",
            "          11       0.51      0.66      0.57        35\n",
            "\n",
            "    accuracy                           0.71       214\n",
            "   macro avg       0.66      0.71      0.67       214\n",
            "weighted avg       0.72      0.71      0.71       214\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results.sort_values(by=['F1 Mean', \"F1 Std\"], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "sHHmhAIcqDQg",
        "outputId": "43aa0547-0e8f-4e60-9ff5-86c0ece50481",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "id": "sHHmhAIcqDQg",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Model Vectorizer            Sampler  Accuracy Mean  \\\n",
              "64   GradientBoostingClassifier      Count              SMOTE       0.718129   \n",
              "74   GradientBoostingClassifier      Count   Borderline-SMOTE       0.711111   \n",
              "84   GradientBoostingClassifier      Count             ADASYN       0.712281   \n",
              "114  GradientBoostingClassifier      Count        SMOTE-Tomek       0.702924   \n",
              "94   GradientBoostingClassifier      Count  RandomOversampler       0.695906   \n",
              "54   GradientBoostingClassifier      TFIDF        SMOTE-Tomek       0.683041   \n",
              "34   GradientBoostingClassifier      TFIDF  RandomOversampler       0.684211   \n",
              "4    GradientBoostingClassifier      TFIDF              SMOTE       0.676023   \n",
              "24   GradientBoostingClassifier      TFIDF             ADASYN       0.677193   \n",
              "14   GradientBoostingClassifier      TFIDF   Borderline-SMOTE       0.678363   \n",
              "\n",
              "     Accuracy Std  Recall Mean  Recall Std  Precision Mean  Precision Std  \\\n",
              "64       0.050224     0.718129    0.050224        0.722637       0.054595   \n",
              "74       0.039247     0.711111    0.039247        0.720117       0.038706   \n",
              "84       0.050360     0.712281    0.050360        0.717498       0.053334   \n",
              "114      0.046549     0.702924    0.046549        0.707647       0.050819   \n",
              "94       0.034696     0.695906    0.034696        0.716885       0.038906   \n",
              "54       0.044598     0.683041    0.044598        0.727463       0.046621   \n",
              "34       0.024253     0.684211    0.024253        0.702683       0.025199   \n",
              "4        0.026824     0.676023    0.026824        0.711289       0.030520   \n",
              "24       0.027529     0.677193    0.027529        0.707741       0.027595   \n",
              "14       0.035859     0.678363    0.035859        0.708378       0.030483   \n",
              "\n",
              "      F1 Mean    F1 Std  \n",
              "64   0.712874  0.053408  \n",
              "74   0.707874  0.040774  \n",
              "84   0.707082  0.052112  \n",
              "114  0.697597  0.048697  \n",
              "94   0.695975  0.038387  \n",
              "54   0.687537  0.046580  \n",
              "34   0.684024  0.024986  \n",
              "4    0.678414  0.028567  \n",
              "24   0.678114  0.029618  \n",
              "14   0.677870  0.038310  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-96659097-8a99-473e-9ea0-2948a69d87eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>Sampler</th>\n",
              "      <th>Accuracy Mean</th>\n",
              "      <th>Accuracy Std</th>\n",
              "      <th>Recall Mean</th>\n",
              "      <th>Recall Std</th>\n",
              "      <th>Precision Mean</th>\n",
              "      <th>Precision Std</th>\n",
              "      <th>F1 Mean</th>\n",
              "      <th>F1 Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>SMOTE</td>\n",
              "      <td>0.718129</td>\n",
              "      <td>0.050224</td>\n",
              "      <td>0.718129</td>\n",
              "      <td>0.050224</td>\n",
              "      <td>0.722637</td>\n",
              "      <td>0.054595</td>\n",
              "      <td>0.712874</td>\n",
              "      <td>0.053408</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>Borderline-SMOTE</td>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.039247</td>\n",
              "      <td>0.711111</td>\n",
              "      <td>0.039247</td>\n",
              "      <td>0.720117</td>\n",
              "      <td>0.038706</td>\n",
              "      <td>0.707874</td>\n",
              "      <td>0.040774</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>ADASYN</td>\n",
              "      <td>0.712281</td>\n",
              "      <td>0.050360</td>\n",
              "      <td>0.712281</td>\n",
              "      <td>0.050360</td>\n",
              "      <td>0.717498</td>\n",
              "      <td>0.053334</td>\n",
              "      <td>0.707082</td>\n",
              "      <td>0.052112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>114</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>SMOTE-Tomek</td>\n",
              "      <td>0.702924</td>\n",
              "      <td>0.046549</td>\n",
              "      <td>0.702924</td>\n",
              "      <td>0.046549</td>\n",
              "      <td>0.707647</td>\n",
              "      <td>0.050819</td>\n",
              "      <td>0.697597</td>\n",
              "      <td>0.048697</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>RandomOversampler</td>\n",
              "      <td>0.695906</td>\n",
              "      <td>0.034696</td>\n",
              "      <td>0.695906</td>\n",
              "      <td>0.034696</td>\n",
              "      <td>0.716885</td>\n",
              "      <td>0.038906</td>\n",
              "      <td>0.695975</td>\n",
              "      <td>0.038387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>SMOTE-Tomek</td>\n",
              "      <td>0.683041</td>\n",
              "      <td>0.044598</td>\n",
              "      <td>0.683041</td>\n",
              "      <td>0.044598</td>\n",
              "      <td>0.727463</td>\n",
              "      <td>0.046621</td>\n",
              "      <td>0.687537</td>\n",
              "      <td>0.046580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>RandomOversampler</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.024253</td>\n",
              "      <td>0.684211</td>\n",
              "      <td>0.024253</td>\n",
              "      <td>0.702683</td>\n",
              "      <td>0.025199</td>\n",
              "      <td>0.684024</td>\n",
              "      <td>0.024986</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>SMOTE</td>\n",
              "      <td>0.676023</td>\n",
              "      <td>0.026824</td>\n",
              "      <td>0.676023</td>\n",
              "      <td>0.026824</td>\n",
              "      <td>0.711289</td>\n",
              "      <td>0.030520</td>\n",
              "      <td>0.678414</td>\n",
              "      <td>0.028567</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>ADASYN</td>\n",
              "      <td>0.677193</td>\n",
              "      <td>0.027529</td>\n",
              "      <td>0.677193</td>\n",
              "      <td>0.027529</td>\n",
              "      <td>0.707741</td>\n",
              "      <td>0.027595</td>\n",
              "      <td>0.678114</td>\n",
              "      <td>0.029618</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>Borderline-SMOTE</td>\n",
              "      <td>0.678363</td>\n",
              "      <td>0.035859</td>\n",
              "      <td>0.678363</td>\n",
              "      <td>0.035859</td>\n",
              "      <td>0.708378</td>\n",
              "      <td>0.030483</td>\n",
              "      <td>0.677870</td>\n",
              "      <td>0.038310</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-96659097-8a99-473e-9ea0-2948a69d87eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-96659097-8a99-473e-9ea0-2948a69d87eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-96659097-8a99-473e-9ea0-2948a69d87eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1031d6e5-9248-416c-b59a-3b98a24bf272\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1031d6e5-9248-416c-b59a-3b98a24bf272')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1031d6e5-9248-416c-b59a-3b98a24bf272 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"GradientBoostingClassifier\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vectorizer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"TFIDF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sampler\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Borderline-SMOTE\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016188024629324994,\n        \"min\": 0.6760233918128655,\n        \"max\": 0.7181286549707602,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6771929824561403\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009771890250463664,\n        \"min\": 0.024253147781669863,\n        \"max\": 0.050360351208446764,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.027528894259508344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.016188024629324994,\n        \"min\": 0.6760233918128655,\n        \"max\": 0.7181286549707602,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6771929824561403\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.009771890250463664,\n        \"min\": 0.024253147781669863,\n        \"max\": 0.050360351208446764,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.027528894259508344\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007886957089541772,\n        \"min\": 0.7026829465350568,\n        \"max\": 0.7274628477686632,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.707741132885196\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.011090186557508956,\n        \"min\": 0.025198879888343875,\n        \"max\": 0.05459460053374582,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.02759457695202868\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013405785894073952,\n        \"min\": 0.6778704028329686,\n        \"max\": 0.7128737036885326,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.678114167259271\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.010084079889067583,\n        \"min\": 0.02498554943047707,\n        \"max\": 0.05340847869831244,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.029617763558057127\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = Representations(df).representation_a(df)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "_aqzrdn9ukm_",
        "outputId": "4321b299-58d4-4071-ed24-5bf007cfa21b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "id": "_aqzrdn9ukm_",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  2%|         | 4/180 [00:29<22:32,  7.68s/it]"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2acd672d5bbf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRepresentations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation_a\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mensemble\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnsembleModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrained_vectorizers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-204eb8911a2f>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    103\u001b[0m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m                     result = self.test(\n\u001b[0m\u001b[1;32m    106\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                         \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclf_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-6-204eb8911a2f>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(self, model, model_name, vectorizer, vectorizer_name, sampler, sampler_name)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_resample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_resampled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_resampled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;31m# fit the boosting stages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 783\u001b[0;31m         n_stages = self._fit_stages(\n\u001b[0m\u001b[1;32m    784\u001b[0m             \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[1;32m    877\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    878\u001b[0m             \u001b[0;31m# fit next stage of trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 879\u001b[0;31m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[1;32m    880\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    881\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csc\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m             tree.fit(\n\u001b[0m\u001b[1;32m    491\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_g_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 )\n\u001b[1;32m   1472\u001b[0m             ):\n\u001b[0;32m-> 1473\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1475\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1375\u001b[0m         \"\"\"\n\u001b[1;32m   1376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m         super()._fit(\n\u001b[0m\u001b[1;32m   1378\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    470\u001b[0m             )\n\u001b[1;32m    471\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 472\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmissing_values_in_feature_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    473\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "3_juRB6oN_xb"
      },
      "id": "3_juRB6oN_xb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_before, df_after = Representations(df).representation_b(df)\n",
        "X, y = pd.concat([df_before.events_sequence, df_after.events_sequence]), pd.concat([df_before['class'], df_after['class']])\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "razuBz2VOucS"
      },
      "id": "razuBz2VOucS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "eVEC5TGedBmj"
      },
      "id": "eVEC5TGedBmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = Representations(df).representation_c(df, sequence_length=100)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()\n",
        "ensemble = EnsembleModel(exp.trained_models, exp.trained_vectorizers)\n",
        "ensemble.fit(exp.X_train, exp.y_train)\n",
        "ensemble_predictions = ensemble.predict(exp.X_test)\n",
        "print(classification_report(exp.y_test, ensemble_predictions, zero_division=1))"
      ],
      "metadata": {
        "id": "A7frHCFPeHhB"
      },
      "id": "A7frHCFPeHhB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "ljaZ-7XPdDtn"
      },
      "id": "ljaZ-7XPdDtn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wiQypjdvdf2N"
      },
      "id": "wiQypjdvdf2N",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}