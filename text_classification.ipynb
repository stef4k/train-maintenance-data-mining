{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stef4k/train-maintenance-data-mining/blob/main/text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Importation"
      ],
      "metadata": {
        "id": "vXgvhkss9klA"
      },
      "id": "vXgvhkss9klA"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9",
      "metadata": {
        "id": "a04bb8ce-c910-492e-b169-c9cd7952bef9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import ast\n",
        "import pickle\n",
        "from joblib import Parallel, delayed\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.utils import parallel_backend\n",
        "import threading\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from gensim.models import Word2Vec\n",
        "from scipy.optimize import minimize\n",
        "from sklearn.base import TransformerMixin\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from imblearn.over_sampling import SMOTE, BorderlineSMOTE, ADASYN, RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN, SMOTETomek\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from tqdm import tqdm\n",
        "from scipy.optimize import differential_evolution\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Experiment`\n",
        "\n",
        "This class is a pipeline that automates the process of training and testing using cross-validation and prepares the system for ensemble evaluations. This class was developed as pipeline to do experiments more easily, since we want to test all possible model, sampling strategies and vectorizers available.\n",
        "\n",
        "One main limitation is the business knowledge we have, since we don not really understand at a very specific level this dataset, like what exactly each incident is, or what does each event in the events sequence represent, we didn't take any apriori assumtios of the data, and we decided to find by bute force what is the best vectorizer, sampling strategy and model for the given dataset. Additionally, we also tested different representations of the sequences in an attempt of removing any noise we were not aware of. These kinds of knowledge limitations impacted on the results we presented since we are treating the dataset as a black box in certain aspects.\n",
        "\n",
        "For the experiments, we are trateing the sequences of events as if they were words in a text. Therefore, we treated this problem as text classification and defined the pipeline as such. Therefore, we will test the **TFIDF Vectorier, Count Vectorizer and Word2Vec** to understand whichone can infere the best embedding space for the provided event sequences.\n",
        "\n",
        "Since we are dealing with a very small data sample (around 1k observations) we wanted to train our models in oversampled data using different sampling methods and find our which can improve the models performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`__init__(self, X, y)`**\n",
        "\n",
        "This class is initialized with the training data and the target column. Whithin this method it:\n",
        "  - Splits `X` (features) and `y` (labels) into training and test sets using **stratified sampling** to ensure class balance.\n",
        "  - Encodes categorical labels (`y`) into numerical values using `LabelEncoder`.\n",
        "\n",
        "We also initialize the following attributes to be accessible in all parts of the pipeline:\n",
        "  - **`self.X_train`, `self.X_test`, `self.y_train`, `self.y_test`:** Prepared splits for training and testing.\n",
        "  - **`self.trai`ed_models` and `self.trained_vectorizers`:** Empty dictionaries to store trained models and vectorizers.\n",
        "  - **`self.results`:** Stores evaluation results for comparison.\n",
        "  - **`sampling_strategies`:** A dictionary of techniques to handle class imbalance, we are using `SMOTE`, `Borderline-SMOTE`, `ADASYN`, `RandomOversampler`, `SMOTE-ENN`, `SMOTE-Tomek`.\n",
        "  - **`vectorizers`:** A dictionary of methods for text representation, we are using `TFIDF`, `Count`,`Word2Vec`\n",
        "  - **`classifiers`:** A set of preconfigured machine learning models, such as `LogisticRegression`,`DecisionTree`,`RandomForest`,`ExtraTreesClassifier`,`GradientBoostingClassifier`,`AdaBoostClassifier`,`GaussianNB`,`KNN`,`SVM`,`XGBoost`.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`duplicate_minor_classes(self, X, y, min_instances=5)`**\n",
        "\n",
        "Balances the dataset by duplicating underrepresented classes until they meet the minimum instance threshold.\n",
        "\n",
        "1. Identifies **minor classes** with fewer samples than `min_instances`.\n",
        "2. Duplicates rows corresponding to these classes.\n",
        "3. Merges the duplicated data back with the original dataset.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`test(self, model, model_name, vectorizer, vectorizer_name, sampler, sampler_name)`**\n",
        "\n",
        "Using cross-validation it evaluates a single combination of **Model**, **Vectorizer** and **Sampler**. It returns a list of evaluation metrics (mean and standard deviation) for the specified combination.\n",
        "\n",
        "1. Splits `X` and `y` into 5 folds using **StratifiedKFold**.\n",
        "2. For each fold:\n",
        "   - Transforms the data with the given vectorizer.\n",
        "   - Balances the data using the sampler.\n",
        "   - Trains the model on the resampled training data.\n",
        "   - Predicts on the validation data.\n",
        "3. Calculates metrics:\n",
        "   - Accuracy\n",
        "   - Recall\n",
        "   - Precision\n",
        "   - F1 Score\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "#### **`training(self)`**\n",
        "\n",
        "Runs experiments for all combinations of **Model**, **Vectorizer** and **Sampler**.\n",
        "\n",
        "1. Balances the training data using `duplicate_minor_classes`.\n",
        "2. Constructs a task list of all vectorizer-sampler-model combinations.\n",
        "3. Uses parallel processing to execute each task:\n",
        "   - For each combination, calls `test()` to evaluate.\n",
        "4. Aggregates results into a DataFrame for easy comparison.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`_process_task(self, task, progress_bar)`**\n",
        "\n",
        "Handles a single task from the `training()` pipeline to run it in parallel. Returns evaluation metrics for the task.\n",
        "\n",
        "1. Unpacks the task (vectorizer, sampler, model).\n",
        "2. Executes `test()` for the task.\n",
        "3. Updates the progress bar.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`test_ensemble(self)`**\n",
        "\n",
        "Evaluates the performance of an ensemble model across multiple folds. Returns evaluation metrics for the ensemble model.\n",
        "\n",
        "1. Splits the data using `StratifiedKFold`.\n",
        "2. For each fold:\n",
        "   - Trains all models, vectorizers, and samplers on the training data.\n",
        "   - Constructs an ensemble model using `EnsembleModel`.\n",
        "   - Tests the ensemble on the validation set.\n",
        "3. Calculates metrics for the ensemble’s performance.\n",
        "\n",
        "---\n",
        "\n",
        "#### **`_process_fold(self, X_train, X_test, y_train, y_test)`**\n",
        "\n",
        "Handles training and evaluation for a single fold in `test_ensemble` to be executed in parallel along all other folds. Returns evaluation metrics for the ensemble model for that fold.\n",
        "\n",
        "1. Trains all vectorizers and models on the training data.\n",
        "2. Stores trained models and vectorizers for the fold.\n",
        "3. Fits an ensemble model on the predictions.\n",
        "4. Evaluates the ensemble on the validation set.\n",
        "\n"
      ],
      "metadata": {
        "id": "Fr_q4Q0M63DE"
      },
      "id": "Fr_q4Q0M63DE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## `Word2VecVectorizer`\n",
        "This class converts sentences into numerical vectors using Word2Vec, which captures the semantic meaning of words. This class was defined to provide an interface similar to Scikit-Learn interface and avoid syntaxis issues later on in the `Experiment` class.\n",
        "\n",
        " - `__init__`: Lets you configure how the Word2Vec model works:\n",
        "   - `size` is the number of dimensions for word vectors (e.g., 100-dimensional space).\n",
        "   - `window` is the maximum distance between the current and predicted words.\n",
        "   - `min_count` is the minimum number of times a word must appear to be included in the model.\n",
        "   - `workers` determines how many CPU threads to use for training (faster with more workers).\n",
        " - `fit`: Takes a list of sentences, splits them into words, and trains a Word2Vec model to learn relationships between words.\n",
        " - `transform`: Takes sentences and converts them into vectors. It does this by averaging the vectors of all words in a sentence. If a word isn't in the Word2Vec model, it is ignored, and zeros are used instead.\n",
        " - `fit_transform`: A convenience method that combines the `fit` (training) and `transform` (vectorizing) steps.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "SgzoRMgu6W0K"
      },
      "id": "SgzoRMgu6W0K"
    },
    {
      "cell_type": "code",
      "source": [
        "class Word2VecVectorizer(TransformerMixin):\n",
        "    def __init__(self, size=100, window=5, min_count=1, workers=4):\n",
        "        self.size = size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.w2v_model = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        sentences = [sentence.split() for sentence in X]\n",
        "        self.w2v_model = Word2Vec(sentences, vector_size=self.size, window=self.window,\n",
        "                                  min_count=self.min_count, workers=self.workers)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        transformed_data = np.array([\n",
        "            np.mean([self.w2v_model.wv[word] for word in sentence.split() if word in self.w2v_model.wv]\n",
        "                    or [np.zeros(self.\n",
        "                                 size)], axis=0)\n",
        "            for sentence in X\n",
        "        ])\n",
        "        return csr_matrix(transformed_data)\n",
        "\n",
        "    def fit_transform(self, X, y=None):\n",
        "        self.fit(X, y)\n",
        "        return self.transform(X, y)"
      ],
      "metadata": {
        "id": "fOHHiU-pDaet"
      },
      "id": "fOHHiU-pDaet",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class NoSampler(TransformerMixin):\n",
        "    def __init__(self):\n",
        "      pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return X, y\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return X, y\n",
        "\n",
        "    def fit_resample(self, X, y=None):\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "B4W8MlJVFVf-"
      },
      "id": "B4W8MlJVFVf-",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Experiment:\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.le = LabelEncoder()\n",
        "        self.y = self.le.fit_transform(y)\n",
        "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)\n",
        "        self.y_train = self.le.transform(self.y_train)\n",
        "        self.y_test = self.le.transform(self.y_test)\n",
        "        self.trained_models = {}\n",
        "        self.trained_vectorizers = {}\n",
        "        self.trainer_samplers = {}\n",
        "        self.results = []\n",
        "        self.sampling_strategies = {\n",
        "            \"SMOTE\": SMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            \"Borderline-SMOTE\": BorderlineSMOTE(sampling_strategy='auto', random_state=1, k_neighbors=3),\n",
        "            \"ADASYN\": ADASYN(sampling_strategy='auto', random_state=1, n_neighbors=3),\n",
        "            \"RandomOversampler\": RandomOverSampler(sampling_strategy='auto', random_state=1),\n",
        "            \"SMOTE-ENN\": SMOTEENN(sampling_strategy='auto', random_state=1),\n",
        "            \"SMOTE-Tomek\": SMOTETomek(sampling_strategy='auto', random_state=1),\n",
        "            \"NoSamp\": NoSampler()\n",
        "        }\n",
        "\n",
        "        self.vectorizers = {\n",
        "            \"TFIDF\": TfidfVectorizer(),\n",
        "            \"Count\": CountVectorizer(),\n",
        "            \"Word2Vec\": Word2VecVectorizer(size=100, window=5, min_count=1)\n",
        "        }\n",
        "        self.classifiers = {\n",
        "            'LogisticRegression': LogisticRegression(),\n",
        "            'DecisionTree': DecisionTreeClassifier(),\n",
        "            'RandomForest': RandomForestClassifier(),\n",
        "            'ExtraTreesClassifier': ExtraTreesClassifier(),\n",
        "            'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
        "            'AdaBoostClassifier': AdaBoostClassifier(),\n",
        "            'GaussianNB': GaussianNB(),\n",
        "            'KNN': KNeighborsClassifier(),\n",
        "            'SVM': SVC(probability=True),\n",
        "            'XGBoost': XGBClassifier(objective=\"multi:softmax\", num_class=len(np.unique(y)), eval_metric=\"mlogloss\", use_label_encoder=False, n_jobs = 2),\n",
        "        }\n",
        "\n",
        "\n",
        "    def duplicate_minor_classes(self, X, y, min_instances=10):\n",
        "        X = X.reset_index(drop=True)\n",
        "        y  = y.reset_index(drop=True)\n",
        "        class_counts = y.value_counts()\n",
        "\n",
        "        minor_classes = class_counts[class_counts <= min_instances].index\n",
        "\n",
        "        minor_class_rows = X.loc[y.isin(minor_classes)]\n",
        "        minor_class_labels = y.loc[y.isin(minor_classes)]\n",
        "\n",
        "        duplicated_X = pd.concat([minor_class_rows] * 2, ignore_index=True)\n",
        "        duplicated_y = pd.concat([minor_class_labels] * 2, ignore_index=True)\n",
        "\n",
        "        X_balanced = pd.concat([X, duplicated_X], ignore_index=True)\n",
        "        y_balanced = pd.concat([y, duplicated_y], ignore_index=True)\n",
        "\n",
        "        X_balanced.reset_index(drop=True, inplace=True)\n",
        "        y_balanced.reset_index(drop=True, inplace=True)\n",
        "\n",
        "        return X_balanced, y_balanced\n",
        "\n",
        "\n",
        "    def test(self, model, model_name, vectorizer, vectorizer_name, sampler, sampler_name):\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "        accuracies, recalls, precisions, f1s = [], [], [], []\n",
        "\n",
        "        for train_index, test_index in skf.split(self.X, self.y):\n",
        "            X_train, X_test = self.X.iloc[train_index], self.X.iloc[test_index]\n",
        "            y_train, y_test = self.y[train_index], self.y[test_index]\n",
        "\n",
        "            X_train, y_train = self.duplicate_minor_classes(X_train, pd.Series(y_train))\n",
        "\n",
        "\n",
        "            X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "            X_test = vectorizer.transform(X_test).toarray()\n",
        "            X_resampled, y_resampled = sampler.fit_resample(X_train, y_train)\n",
        "            model.fit(X_resampled, y_resampled)\n",
        "            y_pred = model.predict(X_test)\n",
        "            accuracies.append(accuracy_score(y_test, y_pred))\n",
        "            recalls.append(recall_score(y_test, y_pred, average=\"weighted\"))\n",
        "            precisions.append(precision_score(y_test, y_pred, average=\"weighted\"))\n",
        "            f1s.append(f1_score(y_test, y_pred, average=\"weighted\"))\n",
        "\n",
        "        return [\n",
        "            model_name,\n",
        "            vectorizer_name,\n",
        "            sampler_name,\n",
        "            np.mean(accuracies),\n",
        "            np.std(accuracies),\n",
        "            np.mean(recalls),\n",
        "            np.std(recalls),\n",
        "            np.mean(precisions),\n",
        "            np.std(precisions),\n",
        "            np.mean(f1s),\n",
        "            np.std(f1s),\n",
        "        ]\n",
        "\n",
        "    def training(self):\n",
        "        results = []\n",
        "        task_list = []\n",
        "\n",
        "        for vect_name, vectorizer in self.vectorizers.items():\n",
        "            for samp_name, sampler in self.sampling_strategies.items():\n",
        "                for clf_name, model in self.classifiers.items():\n",
        "                    task_list.append(\n",
        "                        (clf_name, vect_name, samp_name, deepcopy(model), deepcopy(vectorizer), deepcopy(sampler))\n",
        "                    )\n",
        "\n",
        "        progress_bar = tqdm(total=len(task_list))\n",
        "        with parallel_backend(\"threading\"):\n",
        "            parallel_results = Parallel(n_jobs=-1)(\n",
        "                delayed(self._process_task)(task, progress_bar) for task in task_list\n",
        "            )\n",
        "\n",
        "        results.extend(parallel_results)\n",
        "        progress_bar.close()\n",
        "\n",
        "        # Save results and models\n",
        "        self.results = pd.DataFrame(\n",
        "            results,\n",
        "            columns=[\n",
        "                \"Model\",\n",
        "                \"Vectorizer\",\n",
        "                \"Sampler\",\n",
        "                \"Accuracy Mean\",\n",
        "                \"Accuracy Std\",\n",
        "                \"Recall Mean\",\n",
        "                \"Recall Std\",\n",
        "                \"Precision Mean\",\n",
        "                \"Precision Std\",\n",
        "                \"F1 Mean\",\n",
        "                \"F1 Std\",\n",
        "            ],\n",
        "        )\n",
        "        return self.results\n",
        "\n",
        "    def _process_task(self, task, progress_bar):\n",
        "        clf_name, vect_name, samp_name, model, vectorizer, sampler = task\n",
        "        result = self.test(\n",
        "            model=model,\n",
        "            model_name=clf_name,\n",
        "            vectorizer=vectorizer,\n",
        "            vectorizer_name=vect_name,\n",
        "            sampler=sampler,\n",
        "            sampler_name=samp_name,\n",
        "        )\n",
        "        progress_bar.update(1)\n",
        "        return result\n",
        "\n",
        "    def test_ensemble(self):\n",
        "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
        "        tasks = []\n",
        "\n",
        "        for train_index, test_index in skf.split(self.X, self.y):\n",
        "            X_train, X_test = self.X[train_index], self.X[test_index]\n",
        "            y_train, y_test = self.y[train_index], self.y[test_index]\n",
        "            X_train, y_train = self.duplicate_minor_classes(X_train, pd.Series(y_train))\n",
        "            tasks.append((X_train, X_test, y_train, y_test))\n",
        "\n",
        "        total_tasks = len(tasks)\n",
        "        progress_bar = tqdm(total=total_tasks, desc=\"Processing folds\")\n",
        "        lock = threading.Lock()  # Lock to manage updates safely across threads\n",
        "\n",
        "        def process_with_progress(*args):\n",
        "            result = self._process_fold(*args)\n",
        "            with lock:\n",
        "                progress_bar.update(1)\n",
        "            return result\n",
        "\n",
        "        with parallel_backend(\"threading\"):\n",
        "            results = Parallel(n_jobs=-1)(\n",
        "                delayed(process_with_progress)(X_train, X_test, y_train, y_test)\n",
        "                for X_train, X_test, y_train, y_test in tasks\n",
        "            )\n",
        "\n",
        "        progress_bar.close()\n",
        "\n",
        "        accuracies, recalls, precisions, f1s = zip(*results)\n",
        "\n",
        "        return [\n",
        "            \"Ensemble\", \"Multiple\", \"Multiple\",\n",
        "            np.mean(accuracies), np.std(accuracies),\n",
        "            np.mean(recalls), np.std(recalls),\n",
        "            np.mean(precisions), np.std(precisions),\n",
        "            np.mean(f1s), np.std(f1s)\n",
        "        ]\n",
        "\n",
        "    def _process_fold(self, X_train, X_test, y_train, y_test):\n",
        "        fold_trained_models = {}\n",
        "        fold_trained_vectorizers = {}\n",
        "\n",
        "        for vectorizer_name, vectorizer_i in self.vectorizers.items():\n",
        "            vectorizer = deepcopy(vectorizer_i)\n",
        "            X_train_vect = vectorizer.fit_transform(X_train).toarray()\n",
        "            X_test_vect = vectorizer.transform(X_test).toarray()\n",
        "            fold_trained_vectorizers[vectorizer_name] = vectorizer\n",
        "\n",
        "            for sampler_name, sampler_i in self.sampling_strategies.items():\n",
        "                sampler = deepcopy(sampler_i)\n",
        "                X_resampled, y_resampled = sampler.fit_resample(X_train_vect, y_train)\n",
        "\n",
        "                for model_name, model in self.classifiers.items():\n",
        "                    trained_model = deepcopy(model)\n",
        "                    trained_model.fit(X_resampled, y_resampled)\n",
        "                    fold_trained_models[(vectorizer_name, sampler_name, model_name)] = trained_model\n",
        "\n",
        "        ensemble = EnsembleModel(fold_trained_models, fold_trained_vectorizers)\n",
        "        ensemble.fit(X_train, y_train)\n",
        "\n",
        "        y_pred = ensemble.predict(X_test)\n",
        "\n",
        "        accuracy = accuracy_score(y_test, y_pred)\n",
        "        recall = recall_score(y_test, y_pred, average=\"weighted\")\n",
        "        precision = precision_score(y_test, y_pred, average=\"weighted\")\n",
        "        f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "\n",
        "        return accuracy, recall, precision, f1\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Z5-kRVEJxLjj"
      },
      "id": "Z5-kRVEJxLjj",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#`EnsembleModel`\n",
        "\n",
        "This class combines predictions from multiple models to make a more accurate \"group decision.\"\n",
        " - `__init__`: Takes two things:\n",
        "   - A dictionary of trained models.\n",
        "   - A dictionary of vectorizers (tools that convert text into numbers).\n",
        "   It stores these for use during prediction and training.\n",
        " - `_generate_prediction_matrix`: This creates a matrix where:\n",
        "   - Each row corresponds to an input sample.\n",
        "   - Each column is a model's prediction probabilities for each class.\n",
        " - `fit`: Uses an optimization algorithm (`differential_evolution`) to find the best set of weights for combining the model predictions. The goal is to maximize prediction performance (e.g., F1-score).\n",
        " - `predict`: Uses the optimized weights to combine predictions from all models and makes the final decision by selecting the class with the highest combined probability."
      ],
      "metadata": {
        "id": "JmL3JegE6lTx"
      },
      "id": "JmL3JegE6lTx"
    },
    {
      "cell_type": "code",
      "source": [
        "class EnsembleModel:\n",
        "    def __init__(self, trained_models, trained_vectorizers):\n",
        "\n",
        "        self.trained_models = trained_models\n",
        "        self.trained_vectorizers = trained_vectorizers\n",
        "        self.optimized_weights = None\n",
        "\n",
        "    def _generate_prediction_matrix(self, X):\n",
        "        predictions = {}\n",
        "        for (vect_name, samp_name, clf_name), model in self.trained_models.items():\n",
        "            vectorizer = deepcopy(self.trained_vectorizers[vect_name])\n",
        "            X_vect = vectorizer.transform(X).toarray()\n",
        "\n",
        "            predictions[(vect_name, samp_name, clf_name)] = model.predict_proba(X_vect)\n",
        "\n",
        "        return np.stack(list(predictions.values()), axis=2)\n",
        "\n",
        "    def fit(self, X_train, y_train):\n",
        "        pred_matrix = self._generate_prediction_matrix(X_train)\n",
        "\n",
        "        def fitness(weights):\n",
        "            weighted_pred = np.tensordot(pred_matrix, weights, axes=([2], [0]))\n",
        "            final_pred = np.argmax(weighted_pred, axis=1)\n",
        "            return -f1_score(y_train, final_pred, average='weighted')\n",
        "\n",
        "        num_models = pred_matrix.shape[2]\n",
        "        bounds = [(0, 1)] * num_models\n",
        "        result = differential_evolution(fitness, bounds)\n",
        "\n",
        "        self.optimized_weights = result.x\n",
        "\n",
        "    def predict(self, X):\n",
        "        if self.optimized_weights is None:\n",
        "            raise ValueError(\"The ensemble model must be trained using `train_ensemble` before prediction.\")\n",
        "\n",
        "        pred_matrix = self._generate_prediction_matrix(X)\n",
        "        weighted_pred = np.tensordot(pred_matrix, self.optimized_weights, axes=([2], [0]))\n",
        "        ensemble_pred = np.argmax(weighted_pred, axis=1)\n",
        "        return ensemble_pred\n"
      ],
      "metadata": {
        "id": "6CDFeUgXDg3F"
      },
      "id": "6CDFeUgXDg3F",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# `Representations`\n",
        "\n",
        "This class creates different ways to represent sequences of events for machine learning analysis.\n",
        " - `representation_a`: Focuses on filtering events:\n",
        "   - Counts the frequency of each event in all sequences.\n",
        "   - Keeps only the events that occur less than 85% of the time, filtering out common noise.\n",
        "   - Outputs a \"cleaned\" sequence of events and the labels (e.g., incident types).\n",
        " - `representation_b`: Splits event sequences into two parts:\n",
        "   - Events *before* an incident (labeled with the incident type).\n",
        "   - Events *after* an incident (labeled as \"unknown\").\n",
        " - `representation_c`: Breaks sequences into overlapping chunks:\n",
        "   - For example, if you have a sequence of 50 events, it divides them into smaller parts (e.g., chunks of 30 events with 15 overlapping).\n",
        "   - Each chunk is labeled with the incident type.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "7kLLXN5Y6viF"
      },
      "id": "7kLLXN5Y6viF"
    },
    {
      "cell_type": "code",
      "source": [
        "class Representations:\n",
        "  def __init__(self, df):\n",
        "    self.df = df\n",
        "\n",
        "  def representation_a(self, df):\n",
        "            events_types_dict = {}\n",
        "            for events_sequence in df['events_sequence']:\n",
        "                row_list = ast.literal_eval(events_sequence)\n",
        "                unique_events = set(row_list)\n",
        "                for event in unique_events:\n",
        "                    if not events_types_dict.get(event):\n",
        "                        events_types_dict[event] = 0\n",
        "                    events_types_dict[event] += 1\n",
        "\n",
        "            sorted_dict = dict(sorted(events_types_dict.items(), key=lambda item: item[1], reverse=True))\n",
        "            sorted_events_perc_df = pd.DataFrame(list(sorted_dict.items()), columns=['event_type', 'frequency'])\n",
        "            sorted_events_perc_df['percentage'] = sorted_events_perc_df['frequency'] / df.shape[0] * 100\n",
        "            sorted_events_perc_df['event_type'] = sorted_events_perc_df['event_type'].astype(str)\n",
        "\n",
        "            events_low_frequency = list(map(int, list(sorted_events_perc_df[sorted_events_perc_df.percentage <= 85].event_type)))\n",
        "\n",
        "            df['clean_events_sequence'] = (\n",
        "                df['events_sequence']\n",
        "                .apply(ast.literal_eval)\n",
        "                .apply(lambda x: [i for i in x if i in events_low_frequency])\n",
        "                .astype(str)\n",
        "                .replace(r'[\\[\\],]', '', regex=True)\n",
        "            )\n",
        "\n",
        "            self.y = df['incident_type'].copy()\n",
        "            self.X = df['clean_events_sequence'].copy()\n",
        "            return self.X, self.y\n",
        "\n",
        "  def representation_b(self, df):\n",
        "\n",
        "    before_incident = []\n",
        "    after_incident = []\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        events = ast.literal_eval(row['events_sequence'])\n",
        "        seconds = ast.literal_eval(row['seconds_to_incident_sequence'])\n",
        "        incident_type = row['incident_type']\n",
        "\n",
        "        before_events = \" \".join([str(event) for event, time in zip(events, seconds) if time <= 0])\n",
        "        if before_events:\n",
        "            before_incident.append({\n",
        "                \"events_sequence\": before_events,\n",
        "                \"class\": incident_type\n",
        "            })\n",
        "\n",
        "        after_events = \" \".join([str(event) for event, time in zip(events, seconds) if time > 0])\n",
        "        if after_events:\n",
        "            after_incident.append({\n",
        "                \"events_sequence\": after_events,\n",
        "                \"class\": 100\n",
        "            })\n",
        "\n",
        "    before_df = pd.DataFrame(before_incident)\n",
        "    after_df = pd.DataFrame(after_incident)\n",
        "\n",
        "    return before_df, after_df\n",
        "\n",
        "\n",
        "  def representation_c(self, df, sequence_length=30):\n",
        "\n",
        "    overlapping_sequences = []\n",
        "    step = sequence_length // 2\n",
        "\n",
        "    for _, row in df.iterrows():\n",
        "        events = ast.literal_eval(row['events_sequence'])\n",
        "        seconds = ast.literal_eval(row['seconds_to_incident_sequence'])\n",
        "        incident_type = row['incident_type']\n",
        "\n",
        "        for i in range(0, len(events) - sequence_length + 1, step):\n",
        "            sequence = [str(i) for i in events[i:i + sequence_length]]\n",
        "            seconds_slice = seconds[i:i + sequence_length]\n",
        "            sequence_class = incident_type\n",
        "\n",
        "            overlapping_sequences.append({\"sequence\": \" \".join(sequence), \"class\": sequence_class})\n",
        "\n",
        "    sequences_df = pd.DataFrame(overlapping_sequences)\n",
        "    return sequences_df.sequence, sequences_df['class']\n"
      ],
      "metadata": {
        "id": "3zsHqb8oDiyJ"
      },
      "id": "3zsHqb8oDiyJ",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments"
      ],
      "metadata": {
        "id": "ic7u7acl7qJj"
      },
      "id": "ic7u7acl7qJj"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
      "metadata": {
        "id": "a174a224-01e6-49d4-bc6b-9334422de2bf",
        "outputId": "1f6cbb64-6d73-431f-b4c4-4ddd40b944a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     incident_id                                  vehicles_sequence  \\\n",
              "895      4604847  [537, 537, 537, 537, 537, 537, 537, 537, 537, ...   \n",
              "120      4438631  [651, 651, 651, 651, 651, 651, 651, 651, 651, ...   \n",
              "\n",
              "                                       events_sequence  \\\n",
              "895  [3658, 4068, 3658, 4068, 3658, 4068, 3658, 406...   \n",
              "120  [2956, 2956, 2956, 2956, 2956, 2956, 2956, 295...   \n",
              "\n",
              "                          seconds_to_incident_sequence  approx_lat  \\\n",
              "895  [-14388, -14257, -14222, -14127, -14090, -1381...   50.782892   \n",
              "120  [-14392, -14375, -14366, -14361, -14347, -1430...   50.561930   \n",
              "\n",
              "     approx_lon                                 train_kph_sequence  \\\n",
              "895    4.421971  [0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, ...   \n",
              "120    3.869858  [94.0, 85.7, 77.5, 71.5, 37.2, 28.7, 28.1, 28....   \n",
              "\n",
              "                                  dj_ac_state_sequence  \\\n",
              "895  [False, False, False, False, False, False, Fal...   \n",
              "120  [False, False, False, False, False, False, Fal...   \n",
              "\n",
              "                                  dj_dc_state_sequence  incident_type  \n",
              "895  [True, True, True, True, True, True, True, Tru...              2  \n",
              "120  [True, True, True, True, True, True, True, Tru...              9  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd8fb9b0-ca85-4623-993b-d3849657fd09\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>incident_id</th>\n",
              "      <th>vehicles_sequence</th>\n",
              "      <th>events_sequence</th>\n",
              "      <th>seconds_to_incident_sequence</th>\n",
              "      <th>approx_lat</th>\n",
              "      <th>approx_lon</th>\n",
              "      <th>train_kph_sequence</th>\n",
              "      <th>dj_ac_state_sequence</th>\n",
              "      <th>dj_dc_state_sequence</th>\n",
              "      <th>incident_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>895</th>\n",
              "      <td>4604847</td>\n",
              "      <td>[537, 537, 537, 537, 537, 537, 537, 537, 537, ...</td>\n",
              "      <td>[3658, 4068, 3658, 4068, 3658, 4068, 3658, 406...</td>\n",
              "      <td>[-14388, -14257, -14222, -14127, -14090, -1381...</td>\n",
              "      <td>50.782892</td>\n",
              "      <td>4.421971</td>\n",
              "      <td>[0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, ...</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>120</th>\n",
              "      <td>4438631</td>\n",
              "      <td>[651, 651, 651, 651, 651, 651, 651, 651, 651, ...</td>\n",
              "      <td>[2956, 2956, 2956, 2956, 2956, 2956, 2956, 295...</td>\n",
              "      <td>[-14392, -14375, -14366, -14361, -14347, -1430...</td>\n",
              "      <td>50.561930</td>\n",
              "      <td>3.869858</td>\n",
              "      <td>[94.0, 85.7, 77.5, 71.5, 37.2, 28.7, 28.1, 28....</td>\n",
              "      <td>[False, False, False, False, False, False, Fal...</td>\n",
              "      <td>[True, True, True, True, True, True, True, Tru...</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd8fb9b0-ca85-4623-993b-d3849657fd09')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd8fb9b0-ca85-4623-993b-d3849657fd09 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd8fb9b0-ca85-4623-993b-d3849657fd09');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1c236d53-79ef-4a3d-a493-b5b57682aade\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1c236d53-79ef-4a3d-a493-b5b57682aade')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1c236d53-79ef-4a3d-a493-b5b57682aade button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 2,\n  \"fields\": [\n    {\n      \"column\": \"incident_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 117532,\n        \"min\": 4438631,\n        \"max\": 4604847,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          4438631,\n          4604847\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"vehicles_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651, 651]\",\n          \"[537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 537, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 546, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080, 1080]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"events_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3980, 2686, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2684, 2682, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 4120, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 2744, 4026, 4148, 4124, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4180, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 3792, 3792, 3782, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 4180, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2682, 3620, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 2742, 4120, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 4180, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 3980, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 4120, 3720, 3728, 4148, 3636, 3658, 2684, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2862, 2708, 2744, 4148, 4124, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956]\",\n          \"[3658, 4068, 3658, 4068, 3658, 4068, 3658, 4066, 3658, 4066, 3658, 4068, 3658, 4068, 3658, 4066, 3658, 4066, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 4068, 4068, 4068, 3658, 4068, 3658, 4068, 3658, 4068, 3658, 4066, 3658, 4068, 4028, 4050, 4032, 4026, 2708, 2742, 4026, 2740, 4030, 4020, 4026, 4016, 4026, 4020, 3658, 4068, 4066, 4066, 4066, 4066, 3658, 4066, 3658, 4066, 4066, 4066, 4066, 4066, 4068, 4066, 4066, 4068, 4068, 4066, 4066, 4066, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4068, 3658, 4066, 4068, 4028, 4026, 2708, 2742, 4026, 2740, 4030, 4020, 4148, 2972, 3234, 2976, 4100, 2852, 2854, 4120, 2858, 2658, 2688, 3620, 3254, 3254, 3254, 4180, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3008, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 3008, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 4120, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4120, 2956, 2956, 4066, 4066, 4066, 4068, 4068, 4066, 4066, 4068, 4068, 4066, 4066, 4066, 4066, 4066, 4066, 4066, 4066, 4066, 4066, 4068, 4066, 4028, 2752, 4026, 4032, 4026, 4016, 4020, 4026, 2742, 2708, 4026, 4020, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 2708, 2882, 3234, 4048, 2736, 4020, 4016, 4028, 4026, 4026, 4016, 4020, 4026, 3256, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4066, 3658, 4068, 3658, 4068, 4068, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 4068, 3636, 3658, 2684, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 4068, 3636, 4120, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4066, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 3658, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 4120, 3636, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 3636, 4120, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4068, 2708, 3636, 4024, 3506, 2744, 4056, 4032, 2708, 4026, 4030, 4018, 4148, 4126, 3236, 4126, 2708, 2852, 2854, 4124, 2658, 2858, 2688, 3254, 3254, 3254, 4072, 4124, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2682, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4124, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 2956, 4124, 2956, 2956, 2956, 3982, 4054, 4066, 2736, 2708, 4080, 3256, 2708, 4020, 4028, 4024, 4026, 4026, 4016, 4020, 4066, 4066, 4066, 4066, 4066, 4066, 4066, 4066, 4068, 4068, 3658, 4068, 3658]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"seconds_to_incident_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[-14392, -14375, -14366, -14361, -14347, -14300, -14293, -14290, -14254, -14203, -14203, -14147, -14142, -14090, -14088, -14072, -14071, -14033, -14010, -14010, -13965, -13949, -13945, -13940, -13936, -13929, -13893, -13851, -13824, -13824, -13786, -13716, -13665, -13652, -13599, -13578, -13578, -13496, -13495, -13438, -13422, -13350, -13321, -13316, -13313, -13265, -13256, -13247, -13239, -13169, -13169, -13139, -13104, -13101, -13089, -13039, -13035, -13030, -13002, -12998, -12978, -12969, -12922, -12896, -12896, -12848, -12839, -12836, -12818, -12801, -12792, -12752, -12749, -12709, -12704, -12691, -12691, -12682, -12650, -12620, -12614, -12607, -12571, -12545, -12545, -12474, -12464, -12462, -12458, -12445, -12441, -12401, -12335, -12335, -12239, -12237, -12193, -12185, -12172, -12171, -12163, -12135, -12120, -12120, -12104, -12100, -12100, -12077, -12058, -12043, -12041, -12036, -12034, -12026, -12020, -12002, -11984, -11965, -11945, -11943, -11925, -11906, -11885, -11863, -11863, -11782, -11727, -11709, -11668, -11653, -11650, -11617, -11594, -11594, -11527, -11519, -11513, -11482, -11475, -11464, -11456, -11428, -11417, -11382, -11378, -11363, -11354, -11339, -11272, -11249, -11071, -11057, -11057, -10992, -10980, -10947, -10902, -10892, -10881, -10880, -10842, -10823, -10799, -10763, -10755, -10712, -10681, -10652, -10652, -10628, -10533, -10497, -10497, -10464, -10412, -10361, -10337, -10299, -10277, -10172, -10122, -10044, -9905, -9905, -9905, -9497, -9276, -9276, -9161, -9156, -9133, -9062, -9060, -9039, -9013, -9010, -8950, -8931, -8896, -8876, -8876, -8745, -8665, -8665, -8631, -8553, -8499, -8436, -8434, -8407, -8338, -8302, -8293, -8188, -8188, -8148, -8051, -8038, -8030, -8012, -8011, -7973, -7961, -7930, -7921, -7909, -7899, -7871, -7862, -7854, -7786, -7768, -7768, -7716, -7713, -7639, -7617, -7540, -7490, -7363, -7363, -7285, -7248, -7229, -7227, -7204, -7180, -7161, -7146, -7141, -7132, -7131, -7126, -7125, -7124, -7107, -7088, -7064, -7006, -7006, -6957, -6953, -6918, -6877, -6869, -6868, -6856, -6848, -6814, -6804, -6802, -6718, -6685, -6685, -6638, -6633, -6588, -6564, -6496, -6481, -6481, -6438, -6432, -6426, -6425, -6419, -6389, -6352, -6248, -6248, -6199, -6159, -6104, -6079, -6066, -6048, -6045, -6038, -6004, -5992, -5992, -5919, -5910, -5885, -5858, -5852, -5849, -5798, -5783, -5768, -5742, -5710, -5710, -5646, -5592, -5589, -5584, -5580, -5565, -5564, -5562, -5545, -5529, -5528, -5526, -5520, -5517, -5500, -5498, -5434, -5422, -5353, -5247, -5247, -5192, -5130, -5116, -5028, -5013, -5013, -4966, -4918, -4901, -4844, -4824, -4824, -4751, -4726, -4673, -4669, -4647, -4607, -4607, -4594, -4594, -4561, -4557, -4550, -4501, -4484, -4476, -4456, -4417, -4398, -4358, -4348, -4348, -4300, -4282, -4229, -4197, -4122, -4093, -4049, -4019, -4007, -3872, -3786, -3772, -3772, -3681, -3654, -3601, -3578, -3541, -3530, -3514, -3465, -3438, -3438, -3390, -3376, -3355, -3344, -3338, -3321, -3278, -3271, -3268, -3229, -3199, -3199, -3164, -3156, -3108, -3106, -3089, -3087, -3056, -3028, -3028, -2987, -2969, -2964, -2956, -2952, -2943, -2903, -2864, -2844, -2844, -2812, -2746, -2693, -2677, -2626, -2611, -2611, -2541, -2540, -2479, -2460, -2369, -2340, -2336, -2332, -2286, -2279, -2272, -2265, -2227, -2227, -2170, -2166, -2150, -2102, -2098, -2093, -2064, -2058, -2034, -2022, -1963, -1923, -1923, -1889, -1882, -1879, -1862, -1841, -1830, -1788, -1785, -1741, -1727, -1727, -1682, -1650, -1642, -1633, -1590, -1575, -1575, -1414, -1404, -1402, -1397, -1384, -1380, -1340, -1315, -1315, -1229, -1226, -1171, -1161, -1142, -1140, -1128, -1092, -1072, -1072, -1036, -1020, -1004, -1002, -996, -994, -984, -977, -958, -941, -920, -898, -896, -876, -849, -819, -801, -801, -708, -655, -636, -589, -576, -573, -540, -528, -528, -454, -444, -437, -398, -388, -375, -365, -332, -320, -273, -265, -241, -229, -210, -132, -109, 115, 361, 361, 1008, 1009, 1009, 1026, 1028, 1050, 1061, 1090, 1140, 1151, 1165, 1167, 1213, 1232, 1256, 1288, 1296, 1338, 1362, 1391, 1391, 1427, 1519, 1546, 1546, 1588, 1638, 1678, 1696, 1730, 1754, 1857, 1908, 1913, 1915, 2071, 2071, 2299, 2372, 2372, 2514, 2519, 2541, 2609, 2611, 2628, 2648, 2649, 2687, 2700, 2722, 2739, 2739, 2867, 2892, 2892, 2926, 2989, 3036, 3102, 3104, 3123, 3171, 3197, 3206, 3232, 3252, 3252, 3308, 3386, 3398, 3405, 3422, 3423, 3457, 3467, 3495, 3502, 3513, 3520, 3544, 3552, 3558]\",\n          \"[-14388, -14257, -14222, -14127, -14090, -13817, -13687, -13438, -13395, -13282, -13266, -13105, -13013, -12810, -12758, -12376, -12334, -12279, -12255, -12020, -11994, -11828, -11817, -11498, -11433, -11225, -10790, -10559, -10241, -10215, -10025, -9993, -9816, -9797, -9564, -9546, -9317, -9300, -9041, -8932, -8932, -8931, -8930, -8924, -8924, -8924, -8919, -8918, -8917, -8917, -8758, -8757, -8756, -8490, -8211, -7994, -7750, -7547, -7334, -7292, -7009, -6948, -6771, -6421, -6190, -5583, -5385, -5101, -4612, -4334, -4156, -4012, -3737, -3410, -3274, -3141, -3106, -2934, -2913, -2769, -2747, -2543, -2515, -2380, -2350, -2182, -2150, -1926, -1872, -1659, -1239, -438, -437, -165, -165, -165, -160, -152, -150, -97, -64, -49, 97, 100, 116, 117, 118, 180, 181, 181, 181, 199, 200, 242, 242, 301, 325, 439, 464, 532, 538, 556, 596, 599, 670, 680, 726, 741, 762, 765, 813, 846, 853, 871, 920, 938, 971, 987, 1013, 1028, 1071, 1120, 1411, 1434, 1500, 1530, 1570, 1576, 1628, 1638, 1641, 1644, 1693, 1723, 1742, 1780, 1817, 1826, 1865, 1875, 1931, 1959, 1977, 2025, 2036, 2041, 2060, 2067, 2070, 2077, 2108, 2113, 2129, 2144, 2184, 2201, 2213, 2246, 2252, 2255, 2261, 2276, 2303, 2319, 2350, 2371, 2398, 2401, 2404, 2423, 2433, 2435, 2444, 2447, 2470, 2499, 2531, 2566, 2602, 2604, 2624, 2637, 2653, 2675, 2689, 2692, 2696, 2709, 2712, 2714, 2760, 2767, 2809, 2812, 2885, 2998, 3076, 3084, 3103, 3108, 3136, 3139, 3142, 3161, 3165, 3167, 3171, 3193, 3203, 3217, 3236, 3264, 3274, 3296, 3300, 3302, 3316, 3319, 3357, 3405, 3437, 3450, 3455, 3477, 3480, 3493, 3496, 3519, 3526, 3558, 3575, 3592, 3595, -14259, -14129, -13819, -13440, -13284, -13107, -12812, -12378, -12281, -12022, -11830, -11500, -11227, -10793, -10561, -10244, -10028, -9818, -9566, -9319, -9043, -8934, -8932, -8932, -8932, -8926, -8925, -8919, -8919, -8761, -8760, -8760, -8759, -8492, -8459, -8370, -8366, -8340, -8336, -8313, -8311, -8291, -8288, -8248, -8243, -8240, -8213, -8188, -8188, -8161, -8135, -8132, -8127, -8111, -8107, -8092, -8089, -8063, -8060, -8056, -8052, -8043, -7996, -7980, -7980, -7958, -7935, -7928, -7898, -7879, -7854, -7843, -7831, -7821, -7799, -7790, -7780, -7752, -7734, -7734, -7715, -7688, -7663, -7646, -7614, -7549, -7519, -7519, -7499, -7468, -7436, -7414, -7400, -7386, -7375, -7370, -7359, -7352, -7336, -7294, -7261, -7241, -7233, -7227, -7219, -7202, -7195, -7188, -7182, -7177, -7167, -7156, -7126, -7111, -7077, -7065, -7047, -7012, -6950, -6931, -6863, -6849, -6825, -6821, -6797, -6773, -6727, -6727, -6684, -6662, -6645, -6629, -6614, -6575, -6552, -6520, -6510, -6506, -6490, -6475, -6470, -6466, -6452, -6424, -6403, -6403, -6381, -6363, -6356, -6331, -6324, -6322, -6297, -6248, -6237, -6208, -6193, -5969, -5969, -5939, -5915, -5903, -5887, -5877, -5843, -5842, -5816, -5809, -5787, -5782, -5775, -5766, -5763, -5706, -5688, -5656, -5615, -5586, -5560, -5560, -5537, -5522, -5466, -5457, -5455, -5446, -5409, -5388, -5342, -5342, -5325, -5282, -5236, -5218, -5188, -5182, -5173, -5136, -5123, -5103, -4941, -4941, -4924, -4903, -4897, -4879, -4829, -4824, -4815, -4814, -4779, -4742, -4713, -4691, -4682, -4645, -4642, -4622, -4614, -4537, -4537, -4519, -4516, -4496, -4454, -4451, -4448, -4443, -4438, -4419, -4409, -4405, -4373, -4366, -4337, -4311, -4311, -4291, -4273, -4264, -4217, -4212, -4210, -4195, -4192, -4190, -4158, -4139, -4139, -4116, -4102, -4094, -4071, -4068, -4065, -4051, -4048, -4014, -3979, -3979, -3951, -3935, -3927, -3906, -3902, -3892, -3889, -3873, -3869, -3867, -3851, -3847, -3822, -3818, -3815, -3780, -3777, -3772, -3739, -3694, -3694, -3660, -3640, -3633, -3600, -3576, -3565, -3560, -3553, -3540, -3535, -3513, -3497, -3475, -3456, -3447, -3412, -3392, -3392, -3370, -3322, -3304, -3277, -3260, -3260, -3240, -3216, -3196, -3189, -3181, -3179, -3176, -3143, -3108, -3087, -3045, -3031, -3014, -3006, -2992, -2960, -2954, -2936, -2914, -2879, -2853, -2847, -2798, -2771, -2749, -2668, -2642, -2635, -2622, -2595, -2586, -2569, -2545, -2517, -2482, -2471, -2429, -2426, -2382, -2352, -2323, -2291, -2254, -2245, -2184, -2152, -2134, -2093, -2089, -2086, -2025, -2022, -2001, -1986, -1957, -1944, -1928, -1874, -1857, -1845, -1829, -1812, -1784, -1775, -1749, -1739, -1728, -1705, -1702, -1695, -1683, -1678, -1661, -1633, -1633, -1615, -1490, -1488, -1447, -1433, -1428, -1368, -1347, -1320, -1241, -1231, -1197, -509, -460, -459, -446, -445, -440, -439, -167, -166, -153, -153, -150, 299, 724, 739, 1068, 1118, 1691, 1721, 1928, 1957, 2142, 2181, 2317, 2348, 2497, 2529, 2810, 2883, 3215, 3233, 3355, 3524, -14394, -14384, -14379, -14369, -14368, -14312, -14290, -14267, -14259, -14252, -14223, -14074, -14067, -14030, -14002, -13987, -13961, -13949, -13935, -13934, -13884, -13882, -13836, -13834, -13802, -13794, -13783, -13775, -13771, -13762, -13744, -13734, -13728, -13690, -13682, -13680, -13640, -13632, -13628, -13593, -13585, -13583, -13539, -13531, -13490, -13481, -13472, -13467, -13445, -13416, -13302, -13302, -13275, -13250, -13244, -13226, -13202, -13192, -13183, -13180, -13171, -13149, -13130, -13121, -13099, -13074, -13065, -13054, -13014, -13003, -13001, -12954, -12934, -12911, -12893, -12889, -12870, -12840, -12821, -12793, -12786, -12775, -12772, -12768, -12746, -12742, -12730, -12727, -12693, -12687, -12658, -12641, -12596, -12590, -12566, -12521, -12521, -12520, -12458, -12452, -12349, -12345, -12331, -12316, -12291, -12290, -12285, -12278, -12272, -12263, -12234, -12217, -12208, -12187, -12185, -12160, -12152, -12141, -12114, -12105, -12103, -12066, -12049, -12045, -12033, -11995, -11986, -11978, -11968, -11877, -11839, -11832, -11747, -11744, -11743, -11739, -11658, -11643, -11584, -11570, -11567, -11551, -11537, -11503, -11485, -11470, -11412, -11409, -11372, -11369, -11308, -11273, -11263, -11242, -11242, -11192, -11153, -11114, -11096, -11075, -11029, -11003, -10984, -10984, -10918, -10864, -10826, -10795, -10795, -10740, -10739, -10723, -10710, -10705, -10681, -10668, -10647, -10641, -10627, -10620, -10591, -10587, -10574, -10570, -10474, -10446, -10432, -10401, -10393, -10389, -10307, -10270, -10251, -10249, -10216, -10201, -10201, -10145, -10142, -10137, -10134, -10114, -10100, -10071, -10058, -10040, -10028, -10002, -9997, -9987, -9984, -9973, -9961, -9958, -9900, -9891, -9887, -9870, -9865, -9856, -9839, -9836, -9830, -9823, -9786, -9783, -9779, -9762, -9759, -9728, -9713, -9707, -9688, -9661, -9654, -9638, -9628, -9599, -9562, -9474, -9459, -9445, -9407, -9404, -9391, -9389, -9373, -9371, -9365, -9362, -9361, -9338, -9334, -9322, -9321, -9303, -9298, -9286, -9281, -9277, -9274, -9271, -9265, -9260, -9252, -9234, -9210, -9203, -9192, -9184, -9159, -9156, -9115, -9107, -9097, -9091, -9074, -9064, -9049, -9048, -9046, -9035, -9015, -8999, -8968, -8953, -8943, -8929, -8905, -8903, -8896, -8892, -8882, -8839, -8835, -8822, -8818, -8785, -8764, -8749, -8738, -8710, -8704, -8692, -8667, -8621, -8557, -8499, -8451, -8192, -8190, -8144, -8127, -8092, -8070, -8068, -8055, -8029, -8003, -7977, -7899, -7836, -7796, -7790, -7777, -7771, -7743, -7726, -7682, -7661, -7604, -7599, -7548, -7534, -7531, -7506, -7471, -7394, -7376, -7292, -7273, -7239, -7222, -7209, -7207, -7159, -7146, -7143, -7129, -7102, -7096, -7077, -7063, -7003, -6997, -6960, -6952, -6950, -6943, -6920, -6910, -6890, -6854, -6809, -6769, -6728, -6726, -6699, -6670, -6661, -6595, -6572, -6572, -6504, -6500, -6498, -6430, -6427, -6408, -6392, -6363, -6350, -6329, -6306, -6306, -6286, -6275, -6259, -6242, -6214, -6204, -6179, -6171, -6161, -6139, -6136, -6128, -6116, -6110, -6093, -6076, -6076, -6057, -5921, -5919, -5881, -5866, -5863, -5762, -5759, -5731, -5682, -5682, -5635, -5629, -5571, -5569, -5554, -5521, -5511, -5471, -5467, -5439, -5431, -5420, -5412, -5394, -5385, -5380, -5378, -5330, -5313, -5313, -5252, -5206, -5179, -5170, -5149, -5091, -5087, -5055, -4980, -4960, -4897, -4891, -4808, -4770, -4727, -4699, -4697, -4690, -4649, -4647, -4615, -4605, -4596, -4586, -4572, -4568, -4538, -4523, -4490, -4467, -4450, -4442, -4441, -4422, -4408, -4375, -4361, -4348, -4330, -4295, -4276, -4256, -4249, -4240, -4213, -4180, -4165, -4154, -4130, -4094, -4085, -4074, -4032, -4029, -4006, -4003, -3915, -3905, -3893, -3875, -3866, -3859, -3636, -3633, -3627, -3627, -3626, -3622, -3622, -3622, -3621, -3598, -3556, -3546, -3515, -3497, -3485, -3483, -3482, -3419, -3419, -3418, -3316, -3314, -3307, -3012, -2623, -2612, -2583, -2570, -2480, -2477, -2459, -2457, -2441, -2407, -2380, -2376, -2353, -2313, -2288, -2242, -2208, -2156, -2127, -2125, -2103, -2065, -2044, -2025, -1987, -1983, -1955, -1941, -1915, -1881, -1874, -1871, -1860, -1829, -1812, -1758, -1754, -1622, -1589, -1582, -1481, -1423, -1298, -1292, -1167, -1146, -1142, -1082, -1065, -1036, -1034, -1033, -1005, -997, -960, -948, -931, -923, -911, -903, -872, -871, -868, -867, -823, -806, -735, -684, -665, -550, -514, -476, -459, -459, -458, -457, -456, -454, -444, -439, -439, -438, -438, -166, -165, -152, 725, 1069, 1692, 1929, 2143, 2318, 2498, 2810, 3215, 3356, 3403, 3525, 3557]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lat\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1562437332554138,\n        \"min\": 50.56193008035427,\n        \"max\": 50.78289208695988,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          50.56193008035427,\n          50.78289208695988\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"approx_lon\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3904028597482544,\n        \"min\": 3.869858441062802,\n        \"max\": 4.421971460108025,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          3.869858441062802,\n          4.421971460108025\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"train_kph_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[94.0, 85.7, 77.5, 71.5, 37.2, 28.7, 28.1, 28.4, 0.3, 0.0, 0.0, 14.8, 24.5, 40.1, 43.6, 61.9, 60.5, 0.0, 0.0, 0.0, 34.2, 63.8, 70.8, 80.8, 87.2, 87.6, 75.2, 0.1, 0.0, 0.0, 25.0, 82.5, 80.2, 78.2, 0.0, 0.0, 0.0, 85.3, 85.1, 86.0, 81.5, 48.0, 30.5, 31.5, 32.0, 27.4, 22.8, 13.6, 0.0, 0.0, 0.0, 0.0, 30.1, 32.3, 33.8, 33.2, 33.0, 32.4, 51.8, 56.2, 52.1, 70.1, 0.2, 0.0, 0.0, 50.5, 66.8, 71.6, 95.7, 118.9, 114.2, 66.8, 66.4, 0.1, 0.0, 0.0, 0.0, 0.0, 37.7, 85.7, 81.6, 72.8, 0.1, 0.0, 0.0, 90.3, 86.6, 85.7, 84.4, 69.3, 62.9, 0.1, 0.0, 0.0, 111.5, 112.9, 93.4, 86.4, 76.7, 75.7, 66.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 41.0, 83.8, 106.9, 108.5, 107.1, 106.6, 102.6, 93.4, 67.0, 56.9, 53.5, 51.9, 51.9, 52.8, 47.3, 0.0, 0.0, 0.0, 33.5, 38.0, 77.0, 76.8, 64.6, 63.2, 0.0, 0.0, 0.0, 123.3, 134.5, 133.5, 136.7, 135.2, 132.9, 132.0, 133.8, 131.7, 126.5, 121.8, 93.7, 78.9, 61.9, 40.4, 0.0, 0.0, 0.0, 0.0, 22.5, 22.4, 31.0, 44.0, 45.6, 62.3, 63.5, 118.3, 132.2, 120.1, 125.4, 124.9, 65.1, 0.1, 0.0, 0.0, 15.8, 0.2, 0.0, 0.0, 62.6, 113.0, 93.5, 76.8, 56.4, 37.5, 22.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 42.9, 43.8, 48.2, 94.5, 94.4, 92.0, 89.9, 89.8, 64.9, 47.5, 0.1, 0.0, 0.0, 0.1, 0.0, 0.0, 29.9, 116.8, 117.8, 60.6, 55.2, 31.6, 37.0, 17.6, 0.2, 0.0, 0.0, 22.8, 55.9, 88.2, 100.8, 123.6, 125.0, 111.4, 116.0, 122.5, 118.9, 113.5, 111.2, 112.3, 110.9, 108.0, 0.2, 0.0, 0.0, 69.0, 70.4, 54.9, 38.8, 31.4, 0.0, 0.0, 0.0, 14.4, 49.4, 47.5, 47.0, 42.8, 42.8, 78.5, 107.1, 110.8, 109.7, 109.4, 108.4, 108.3, 105.3, 76.6, 49.2, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 52.8, 74.8, 77.7, 95.2, 98.3, 112.0, 110.9, 109.6, 0.0, 0.0, 0.0, 45.6, 46.3, 27.1, 58.3, 0.1, 0.0, 0.0, 62.5, 77.6, 86.8, 88.2, 87.1, 57.2, 0.2, 0.0, 0.0, 0.0, 41.5, 76.8, 81.0, 82.9, 78.0, 76.9, 61.5, 0.1, 0.0, 0.0, 60.8, 60.1, 48.5, 32.0, 30.5, 29.7, 34.1, 31.4, 19.7, 0.0, 0.0, 0.0, 19.6, 31.7, 31.3, 31.4, 32.7, 64.0, 65.0, 60.9, 69.9, 82.9, 82.8, 82.6, 81.2, 80.5, 80.5, 81.0, 85.7, 85.9, 0.0, 0.0, 0.0, 76.2, 72.7, 73.0, 0.2, 0.0, 0.0, 68.6, 69.2, 53.3, 0.0, 0.0, 0.0, 51.8, 37.3, 25.9, 26.2, 0.1, 0.0, 0.0, 0.0, 0.0, 25.4, 26.6, 30.2, 40.6, 41.4, 39.9, 38.5, 40.6, 66.4, 0.1, 0.0, 0.0, 75.5, 86.7, 87.5, 57.8, 38.4, 34.0, 30.6, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 38.4, 39.2, 99.6, 99.3, 97.2, 93.8, 85.3, 0.2, 0.0, 0.0, 74.2, 76.3, 71.9, 60.9, 49.5, 39.4, 36.9, 27.6, 26.9, 0.0, 0.0, 0.0, 11.2, 18.1, 47.6, 49.4, 53.7, 53.5, 0.0, 0.0, 0.0, 29.0, 54.7, 57.4, 63.0, 67.0, 73.1, 69.1, 0.1, 0.0, 0.0, 29.6, 82.9, 70.1, 66.7, 0.2, 0.0, 0.0, 99.2, 98.2, 75.8, 70.6, 45.2, 29.8, 29.8, 30.5, 33.8, 30.0, 20.3, 0.0, 0.0, 0.0, 22.6, 24.4, 26.3, 33.5, 33.4, 34.0, 39.1, 40.8, 46.1, 48.7, 0.0, 0.0, 0.0, 61.7, 77.4, 80.2, 94.3, 95.9, 95.1, 66.0, 63.8, 0.0, 0.0, 0.0, 42.8, 63.5, 62.2, 61.7, 0.1, 0.0, 0.0, 86.7, 83.1, 82.2, 80.9, 65.9, 61.0, 0.1, 0.0, 0.0, 94.3, 95.1, 70.9, 68.2, 45.8, 44.9, 43.4, 0.1, 0.0, 0.0, 48.2, 90.0, 92.9, 92.3, 90.6, 90.1, 87.5, 84.5, 65.7, 54.1, 48.4, 46.7, 47.0, 40.2, 34.1, 0.0, 0.0, 0.0, 33.9, 39.1, 63.1, 83.4, 69.7, 65.7, 0.1, 0.0, 0.0, 108.3, 108.1, 107.7, 108.8, 109.2, 108.4, 109.1, 117.7, 113.5, 79.5, 73.4, 61.9, 60.1, 54.5, 35.8, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 27.5, 27.0, 25.7, 40.0, 41.7, 41.8, 41.8, 111.5, 129.1, 132.9, 128.3, 122.7, 65.2, 0.6, 0.0, 0.0, 20.5, 0.3, 0.0, 0.0, 62.4, 128.8, 126.5, 98.6, 54.4, 37.5, 22.7, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 40.5, 41.6, 60.5, 92.7, 97.6, 119.4, 132.0, 132.4, 92.4, 78.2, 2.1, 0.0, 0.0, 3.2, 0.0, 0.0, 41.9, 134.5, 133.2, 59.1, 58.7, 53.2, 31.2, 29.8, 29.6, 0.7, 0.0, 0.0, 20.0, 73.9, 96.4, 107.4, 127.1, 128.7, 135.6, 136.8, 137.8, 138.2, 138.0, 136.2, 133.8, 134.0, 130.9]\",\n          \"[0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.3, 0.0, 0.7, 0.0, 0.4, 0.0, 0.5, 0.0, 1.8, 0.0, 0.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.7, 0.0, 1.2, 1.3, 0.2, 0.9, 0.0, 0.1, 0.0, 0.7, 0.0, 0.7, 0.0, 0.5, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.2, 30.8, 35.7, 38.5, 58.8, 95.4, 95.0, 110.4, 108.2, 0.1, 0.0, 0.0, 0.0, 90.2, 84.7, 71.2, 53.9, 57.0, 58.6, 67.3, 64.0, 31.8, 26.7, 0.1, 0.0, 0.0, 15.3, 35.7, 30.2, 37.5, 45.9, 109.6, 95.6, 91.5, 83.9, 0.0, 0.0, 0.0, 85.9, 109.1, 114.5, 109.8, 97.7, 0.1, 0.0, 0.0, 93.4, 103.0, 106.6, 111.1, 110.0, 110.2, 111.8, 76.3, 64.3, 30.8, 0.1, 0.0, 0.0, 18.8, 106.1, 116.5, 117.0, 119.0, 105.7, 49.8, 0.0, 0.0, 0.0, 56.5, 64.1, 72.2, 98.0, 99.1, 99.2, 93.5, 89.3, 49.7, 0.0, 0.0, 0.0, 83.4, 82.7, 84.0, 72.6, 70.4, 73.3, 74.8, 71.5, 67.8, 50.6, 45.4, 44.6, 27.0, 26.4, 2.0, 0.0, 0.0, 0.0, 0.0, 0.0, 23.2, 34.6, 35.3, 36.1, 35.5, 34.8, 35.5, 35.1, 35.0, 37.3, 25.1, 0.1, 0.0, 0.0, 8.2, 46.0, 47.2, 46.9, 47.5, 47.4, 0.1, 0.0, 0.0, 20.6, 35.6, 44.7, 46.1, 47.7, 47.6, 17.2, 0.2, 0.0, 0.0, 20.8, 30.6, 0.2, 0.0, 0.0, 0.2, 0.5, 0.4, 0.2, 1.8, 0.0, 1.0, 0.5, 0.7, 1.2, 1.3, 0.2, 1.1, 0.1, 1.0, 0.7, 0.5, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 32.1, 33.4, 34.9, 34.5, 49.5, 50.7, 36.9, 34.2, 35.3, 35.1, 34.9, 0.0, 0.0, 0.0, 0.0, 24.5, 28.2, 34.1, 55.3, 58.4, 58.0, 57.2, 83.3, 85.7, 85.8, 85.1, 83.4, 0.0, 0.0, 0.0, 0.0, 27.6, 38.5, 60.6, 56.4, 47.7, 46.8, 47.1, 47.4, 48.7, 49.6, 49.2, 0.0, 0.0, 0.0, 0.0, 34.0, 65.7, 63.1, 54.1, 0.0, 0.0, 0.0, 0.0, 36.3, 75.5, 74.9, 74.6, 74.0, 71.2, 68.6, 57.8, 42.1, 0.0, 0.0, 0.0, 24.3, 37.4, 47.4, 58.4, 74.8, 74.1, 79.0, 84.6, 88.1, 95.8, 99.4, 95.8, 93.8, 90.7, 81.1, 65.9, 0.0, 0.0, 0.0, 71.0, 75.2, 79.1, 79.5, 67.7, 0.0, 0.0, 0.0, 0.0, 25.1, 50.4, 66.7, 77.2, 89.3, 84.5, 100.9, 99.4, 98.9, 72.5, 46.2, 45.5, 45.0, 43.0, 0.0, 0.0, 0.0, 0.0, 18.8, 31.2, 39.7, 39.8, 39.8, 39.2, 28.2, 28.2, 19.4, 0.0, 0.0, 0.0, 0.0, 21.6, 37.0, 33.6, 34.0, 35.2, 35.1, 47.9, 56.2, 69.4, 68.4, 66.5, 64.5, 63.8, 58.3, 58.1, 56.8, 54.1, 0.0, 0.0, 0.0, 0.0, 15.5, 86.6, 91.9, 91.6, 90.5, 68.1, 0.1, 0.0, 0.0, 0.0, 56.6, 92.6, 102.4, 114.3, 116.8, 118.1, 51.5, 32.9, 0.0, 0.0, 0.0, 0.0, 19.4, 19.8, 36.3, 65.7, 73.1, 89.2, 91.3, 113.8, 109.2, 134.1, 126.8, 116.2, 85.0, 78.7, 12.4, 0.0, 0.0, 0.0, 0.0, 0.0, 24.6, 74.2, 75.9, 77.1, 77.8, 76.7, 75.7, 72.5, 66.0, 35.0, 34.4, 0.0, 0.0, 0.0, 0.0, 18.1, 31.0, 34.1, 34.6, 34.3, 34.9, 34.4, 34.6, 0.0, 0.0, 0.0, 0.0, 13.3, 25.5, 47.9, 47.2, 45.8, 43.9, 43.3, 0.1, 0.0, 0.0, 0.0, 15.6, 26.2, 47.9, 47.4, 45.5, 45.2, 41.5, 40.6, 40.2, 36.8, 36.0, 32.8, 32.3, 31.8, 30.0, 30.1, 27.2, 0.0, 0.0, 0.0, 0.0, 22.0, 32.8, 36.2, 46.4, 45.1, 44.7, 47.8, 61.7, 65.6, 69.2, 67.9, 59.3, 56.2, 55.5, 0.0, 0.0, 0.0, 0.0, 54.9, 60.6, 0.0, 0.0, 0.0, 0.0, 41.1, 75.4, 83.3, 82.0, 81.5, 77.3, 0.1, 0.0, 0.0, 68.3, 81.2, 86.1, 80.7, 72.5, 56.4, 42.5, 0.0, 0.0, 0.0, 38.7, 49.5, 56.4, 0.0, 0.0, 0.0, 28.7, 46.5, 73.8, 116.4, 116.5, 95.6, 0.0, 0.0, 0.0, 8.6, 63.3, 61.9, 0.0, 0.0, 0.0, 63.2, 110.3, 118.5, 0.0, 0.0, 0.0, 86.4, 92.5, 96.4, 65.9, 62.9, 65.9, 53.2, 53.5, 46.6, 0.0, 0.0, 0.0, 16.1, 62.7, 92.5, 117.0, 112.1, 102.4, 98.9, 99.3, 103.2, 102.7, 98.5, 73.2, 59.5, 0.0, 0.0, 0.0, 0.0, 97.1, 97.0, 70.9, 46.6, 35.6, 36.9, 37.4, 36.7, 0.1, 0.0, 0.0, 0.0, 0.2, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.1, 0.0, 0.1, 0.0, 0.2, 0.0, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.1, 0.4, 73.2, 70.0, 68.2, 66.9, 66.7, 36.7, 36.3, 31.7, 27.4, 23.6, 0.1, 0.0, 0.0, 18.4, 42.4, 54.1, 55.1, 68.7, 85.0, 86.8, 98.7, 99.6, 113.7, 114.6, 123.3, 121.2, 120.9, 119.3, 118.4, 116.5, 106.5, 98.2, 94.7, 124.0, 117.8, 117.6, 123.4, 124.9, 126.0, 121.4, 121.5, 121.8, 127.2, 123.5, 112.4, 116.4, 117.0, 110.2, 73.2, 0.1, 0.0, 0.0, 0.0, 16.8, 29.9, 69.3, 102.4, 111.5, 119.0, 119.2, 119.2, 120.7, 115.4, 111.0, 110.0, 116.1, 117.6, 122.2, 123.8, 117.3, 115.3, 54.3, 56.1, 52.2, 56.4, 56.1, 56.4, 57.1, 56.4, 86.2, 89.3, 88.9, 88.5, 86.3, 88.1, 87.3, 87.0, 85.5, 49.3, 41.8, 34.4, 30.3, 37.2, 37.0, 0.0, 0.0, 0.0, 0.0, 38.2, 37.4, 30.2, 27.9, 34.5, 58.2, 98.3, 100.2, 106.4, 113.5, 119.2, 117.6, 98.2, 112.7, 114.2, 114.4, 114.7, 122.8, 124.6, 122.8, 118.4, 113.8, 113.2, 98.8, 91.8, 90.5, 87.1, 70.4, 66.2, 62.5, 51.8, 28.0, 27.5, 36.8, 62.3, 61.0, 60.7, 59.3, 62.9, 65.6, 74.5, 74.8, 74.8, 71.8, 70.6, 76.9, 80.9, 58.2, 26.2, 25.5, 26.9, 27.3, 33.2, 20.8, 0.0, 0.0, 0.0, 29.0, 24.5, 64.4, 62.3, 33.4, 37.8, 0.1, 0.0, 0.0, 27.9, 30.2, 0.0, 0.0, 0.0, 0.0, 0.0, 28.5, 46.0, 44.5, 47.5, 38.6, 49.3, 52.1, 48.9, 45.4, 57.2, 61.4, 60.9, 55.3, 26.5, 22.5, 20.5, 18.9, 20.4, 21.2, 29.2, 26.6, 24.0, 23.7, 0.0, 0.0, 0.0, 27.0, 27.6, 28.4, 35.4, 54.1, 56.0, 34.5, 33.0, 34.7, 35.1, 34.6, 33.4, 30.4, 29.8, 32.1, 39.8, 41.3, 107.1, 118.3, 122.7, 122.7, 120.4, 118.0, 97.6, 92.0, 81.2, 69.8, 51.8, 50.2, 48.0, 56.7, 57.5, 50.0, 53.0, 47.8, 37.9, 37.5, 34.2, 28.9, 27.1, 0.1, 0.0, 0.0, 6.2, 28.8, 40.7, 48.3, 72.6, 76.2, 88.6, 88.4, 88.9, 89.2, 89.1, 83.4, 82.4, 82.8, 83.5, 81.8, 86.0, 84.1, 80.6, 78.1, 77.7, 77.7, 78.6, 76.8, 68.2, 51.8, 55.8, 55.2, 56.6, 57.2, 53.2, 52.6, 54.0, 52.0, 69.8, 77.6, 76.0, 72.7, 68.3, 66.5, 62.3, 45.6, 0.1, 0.0, 10.6, 42.9, 60.7, 78.9, 83.6, 83.3, 80.5, 79.4, 75.6, 64.8, 63.4, 59.2, 58.3, 49.5, 51.4, 54.4, 44.3, 36.5, 37.0, 37.4, 0.1, 0.0, 33.3, 27.4, 0.0, 0.0, 0.0, 21.3, 32.8, 30.6, 67.4, 70.1, 78.9, 67.7, 45.5, 0.0, 0.0, 43.5, 59.3, 56.2, 53.6, 54.2, 57.5, 60.0, 0.0, 0.0, 39.8, 51.4, 86.7, 65.4, 60.0, 0.1, 0.0, 52.5, 0.2, 0.0, 0.0, 59.6, 87.1, 84.6, 83.8, 82.8, 78.8, 77.4, 71.4, 53.3, 40.0, 0.0, 0.0, 46.4, 59.4, 106.0, 111.9, 113.0, 111.8, 111.5, 111.2, 61.3, 0.0, 0.0, 14.5, 82.1, 85.2, 108.9, 111.4, 109.6, 0.0, 0.0, 0.0, 88.1, 92.8, 92.5, 68.0, 68.7, 63.5, 55.3, 52.8, 39.3, 0.0, 0.0, 0.0, 0.0, 11.5, 62.4, 92.2, 112.5, 108.0, 116.5, 114.1, 108.7, 98.7, 98.5, 90.5, 67.3, 57.7, 0.1, 0.0, 0.0, 0.0, 89.6, 90.0, 66.2, 52.0, 53.1, 48.9, 47.8, 0.1, 0.0, 0.0, 15.6, 29.3, 58.4, 59.1, 79.5, 113.8, 122.8, 133.2, 135.7, 131.2, 130.5, 129.2, 128.8, 127.0, 117.7, 108.5, 106.1, 0.1, 0.0, 0.0, 92.7, 106.6, 69.3, 55.7, 36.3, 27.0, 26.7, 0.0, 0.0, 0.0, 17.4, 16.9, 16.2, 17.9, 52.1, 99.8, 102.2, 110.1, 109.5, 109.3, 103.7, 89.4, 77.7, 71.5, 54.1, 49.8, 0.1, 0.0, 32.0, 75.8, 100.7, 110.2, 109.4, 89.6, 65.5, 0.2, 0.0, 0.0, 18.8, 100.0, 114.8, 113.7, 114.5, 113.6, 76.7, 0.1, 0.0, 0.0, 34.9, 97.8, 97.2, 90.4, 33.2, 33.2, 38.1, 38.2, 26.6, 21.6, 20.3, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 17.1, 27.0, 33.3, 38.8, 47.9, 47.9, 45.5, 38.1, 29.0, 28.1, 55.1, 66.1, 60.1, 59.8, 51.0, 37.2, 33.7, 33.8, 56.1, 59.5, 57.7, 55.0, 39.3, 35.3, 28.3, 27.2, 60.4, 58.0, 57.7, 57.4, 57.1, 52.6, 68.6, 57.1, 56.6, 16.7, 21.4, 20.1, 22.6, 16.0, 18.1, 20.0, 0.0, 21.5, 25.4, 35.9, 68.5, 113.7, 115.5, 116.8, 134.9, 133.9, 131.9, 136.4, 131.3, 132.6, 129.1, 125.8, 115.5, 115.5, 114.8, 114.6, 106.3, 74.5, 24.6, 0.0, 14.7, 20.3, 18.7, 0.0, 0.9, 0.9, 0.5, 0.1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.1, 0.1, 0.0, 0.2, 0.2, 0.1, 0.0, 0.1, 0.3, 0.1, 0.0, 0.4, 0.0]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_ac_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\",\n          \"[False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"dj_dc_state_sequence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\",\n          \"[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incident_type\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 2,\n        \"max\": 9,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          9,\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "df = pd.read_csv('sncb_data_challenge.csv', delimiter=';', index_col=0)\n",
        "df.sample(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Raw Dataset\n",
        "We want to test the performance of the pre-defined pipeline as a baseline for future experiments. That is, the data won't have any further transformation other than those defined in the `Experiment` class."
      ],
      "metadata": {
        "id": "5JiAkAiE3qUo"
      },
      "id": "5JiAkAiE3qUo"
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df['events_sequence'], df.incident_type\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()"
      ],
      "metadata": {
        "id": "NZWqprZKULJC",
        "outputId": "d557e5b8-9dfe-4bd9-dee9-df22f9abbc72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "NZWqprZKULJC",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  1%|▏         | 3/210 [01:25<1:38:26, 28.53s/it]\n",
            "\n",
            "  0%|          | 0/210 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 1/210 [00:09<32:42,  9.39s/it]\u001b[A\n",
            "  1%|          | 2/210 [00:18<32:08,  9.27s/it]\u001b[A\n",
            "  1%|▏         | 3/210 [00:29<34:44, 10.07s/it]\u001b[A\n",
            "  2%|▏         | 4/210 [00:43<39:10, 11.41s/it]\u001b[A\n",
            "  2%|▏         | 5/210 [01:15<1:05:17, 19.11s/it]\u001b[A\n",
            "  3%|▎         | 6/210 [01:19<47:41, 14.03s/it]  \u001b[A\n",
            "  3%|▎         | 7/210 [01:25<38:23, 11.35s/it]\u001b[A\n",
            "  4%|▍         | 8/210 [04:43<3:58:05, 70.72s/it]\u001b[A\n",
            "  4%|▍         | 9/210 [09:42<7:55:55, 142.07s/it]\u001b[A\n",
            "  5%|▍         | 10/210 [09:58<5:43:29, 103.05s/it]\u001b[A\n",
            "  5%|▌         | 11/210 [10:10<4:09:08, 75.12s/it] \u001b[A\n",
            "  6%|▌         | 12/210 [10:27<3:10:00, 57.58s/it]\u001b[A\n",
            "  6%|▌         | 13/210 [10:51<2:35:57, 47.50s/it]\u001b[A\n",
            "  7%|▋         | 14/210 [27:17<18:00:43, 330.83s/it]\u001b[A\n",
            "  7%|▋         | 15/210 [27:49<13:02:58, 240.92s/it]\u001b[A\n",
            "  8%|▊         | 16/210 [27:54<9:08:37, 169.68s/it] \u001b[A\n",
            "  8%|▊         | 17/210 [28:00<6:28:14, 120.70s/it]\u001b[A\n",
            "  9%|▊         | 18/210 [31:03<7:26:04, 139.40s/it]\u001b[A\n",
            "  9%|▉         | 19/210 [33:53<7:52:37, 148.47s/it]\u001b[A\n",
            " 10%|▉         | 20/210 [34:11<5:46:00, 109.27s/it]\u001b[A\n",
            " 10%|█         | 21/210 [34:21<4:10:09, 79.41s/it] \u001b[A\n",
            " 10%|█         | 22/210 [34:38<3:10:27, 60.79s/it]\u001b[A\n",
            " 11%|█         | 23/210 [35:03<2:35:44, 49.97s/it]\u001b[A\n",
            " 11%|█▏        | 24/210 [37:01<3:38:12, 70.39s/it]\u001b[A\n",
            " 12%|█▏        | 25/210 [37:36<3:04:49, 59.94s/it]\u001b[A\n",
            " 12%|█▏        | 26/210 [37:41<2:12:39, 43.26s/it]\u001b[A\n",
            " 13%|█▎        | 27/210 [37:46<1:37:16, 31.89s/it]\u001b[A\n",
            " 13%|█▎        | 28/210 [41:15<4:18:16, 85.15s/it]\u001b[A\n",
            " 14%|█▍        | 29/210 [44:02<5:30:40, 109.62s/it]\u001b[A\n",
            " 14%|█▍        | 30/210 [44:17<4:03:28, 81.16s/it] \u001b[A\n",
            " 15%|█▍        | 31/210 [44:24<2:55:32, 58.84s/it]\u001b[A\n",
            " 15%|█▌        | 32/210 [44:38<2:15:12, 45.57s/it]\u001b[A\n",
            " 16%|█▌        | 33/210 [45:03<1:56:06, 39.36s/it]\u001b[A\n",
            " 16%|█▌        | 34/210 [1:01:31<15:50:06, 323.90s/it]\u001b[A\n",
            " 17%|█▋        | 35/210 [1:02:01<11:27:17, 235.64s/it]\u001b[A\n",
            " 17%|█▋        | 36/210 [1:02:04<8:01:23, 166.00s/it] \u001b[A\n",
            " 18%|█▊        | 37/210 [1:02:10<5:39:45, 117.83s/it]\u001b[A\n",
            " 18%|█▊        | 38/210 [1:04:34<6:00:32, 125.77s/it]\u001b[A\n",
            " 19%|█▊        | 39/210 [1:05:16<4:47:11, 100.77s/it]\u001b[A\n",
            " 19%|█▉        | 40/210 [1:05:37<3:37:20, 76.71s/it] \u001b[A\n",
            " 20%|█▉        | 41/210 [1:05:52<2:44:01, 58.23s/it]\u001b[A\n",
            " 20%|██        | 42/210 [1:06:15<2:13:44, 47.77s/it]\u001b[A\n",
            " 20%|██        | 43/210 [1:06:33<1:47:54, 38.77s/it]\u001b[A\n",
            " 21%|██        | 44/210 [1:06:39<1:19:58, 28.91s/it]\u001b[A\n",
            " 21%|██▏       | 45/210 [1:07:18<1:27:45, 31.91s/it]\u001b[A\n",
            " 22%|██▏       | 46/210 [1:07:32<1:12:43, 26.60s/it]\u001b[A\n",
            " 22%|██▏       | 47/210 [1:07:45<1:00:41, 22.34s/it]\u001b[A\n",
            " 23%|██▎       | 48/210 [1:09:48<2:22:18, 52.71s/it]\u001b[A\n",
            " 23%|██▎       | 49/210 [1:11:50<3:16:41, 73.30s/it]\u001b[A\n",
            "  1%|          | 2/210 [1:14:10<128:33:47, 2225.13s/it]\n",
            "\n",
            " 24%|██▍       | 51/210 [1:12:27<1:59:52, 45.23s/it]\u001b[A\n",
            " 25%|██▍       | 52/210 [1:12:50<1:41:38, 38.60s/it]\u001b[A\n",
            " 25%|██▌       | 53/210 [1:13:22<1:35:32, 36.51s/it]\u001b[A\n",
            " 26%|██▌       | 54/210 [1:28:13<12:41:41, 292.96s/it]\u001b[A\n",
            " 26%|██▌       | 55/210 [1:28:55<9:22:33, 217.77s/it] \u001b[A\n",
            " 27%|██▋       | 56/210 [1:29:07<6:40:27, 156.02s/it]\u001b[A\n",
            " 27%|██▋       | 57/210 [1:29:18<4:46:24, 112.32s/it]\u001b[A\n",
            " 28%|██▊       | 58/210 [1:32:45<5:56:42, 140.80s/it]\u001b[A\n",
            " 28%|██▊       | 59/210 [1:35:44<6:23:04, 152.22s/it]\u001b[A\n",
            " 29%|██▊       | 60/210 [1:35:50<4:31:31, 108.61s/it]\u001b[A\n",
            " 29%|██▉       | 61/210 [1:35:55<3:12:21, 77.46s/it] \u001b[A\n",
            " 30%|██▉       | 62/210 [1:36:03<2:19:40, 56.62s/it]\u001b[A\n",
            " 30%|███       | 63/210 [1:36:13<1:44:23, 42.61s/it]\u001b[A\n",
            " 30%|███       | 64/210 [1:40:57<4:39:35, 114.90s/it]\u001b[A\n",
            " 31%|███       | 65/210 [1:41:08<3:22:38, 83.85s/it] \u001b[A\n",
            " 31%|███▏      | 66/210 [1:41:11<2:22:58, 59.57s/it]\u001b[A\n",
            " 32%|███▏      | 67/210 [1:41:14<1:41:28, 42.58s/it]\u001b[A\n",
            " 32%|███▏      | 68/210 [1:41:39<1:27:58, 37.17s/it]\u001b[A\n",
            " 33%|███▎      | 69/210 [1:41:46<1:06:24, 28.26s/it]\u001b[A\n",
            " 33%|███▎      | 70/210 [1:42:07<1:01:04, 26.18s/it]\u001b[A\n",
            " 34%|███▍      | 71/210 [1:42:16<48:20, 20.86s/it]  \u001b[A\n",
            " 34%|███▍      | 72/210 [1:42:32<44:29, 19.35s/it]\u001b[A\n",
            " 35%|███▍      | 73/210 [1:42:46<40:25, 17.70s/it]\u001b[A\n",
            " 35%|███▌      | 74/210 [1:42:53<32:51, 14.50s/it]\u001b[A\n",
            " 36%|███▌      | 75/210 [1:43:18<39:46, 17.68s/it]\u001b[A\n",
            " 36%|███▌      | 76/210 [1:43:21<30:02, 13.45s/it]\u001b[A\n",
            " 37%|███▋      | 77/210 [1:43:25<23:16, 10.50s/it]\u001b[A\n",
            " 37%|███▋      | 78/210 [1:47:11<2:45:43, 75.33s/it]\u001b[A\n",
            " 38%|███▊      | 79/210 [1:48:38<2:51:39, 78.62s/it]\u001b[A\n",
            " 38%|███▊      | 80/210 [1:49:00<2:13:56, 61.82s/it]\u001b[A\n",
            " 39%|███▊      | 81/210 [1:49:07<1:37:08, 45.19s/it]\u001b[A\n",
            " 39%|███▉      | 82/210 [1:49:22<1:17:33, 36.36s/it]\u001b[A\n",
            " 40%|███▉      | 83/210 [1:49:47<1:09:31, 32.84s/it]\u001b[A\n",
            " 40%|████      | 84/210 [1:58:41<6:24:32, 183.12s/it]\u001b[A\n",
            " 40%|████      | 85/210 [1:59:08<4:44:13, 136.43s/it]\u001b[A\n",
            " 41%|████      | 86/210 [1:59:13<3:20:15, 96.90s/it] \u001b[A\n",
            " 41%|████▏     | 87/210 [1:59:18<2:22:01, 69.28s/it]\u001b[A\n",
            " 42%|████▏     | 88/210 [2:02:53<3:49:52, 113.06s/it]\u001b[A\n",
            " 42%|████▏     | 89/210 [2:04:23<3:33:59, 106.11s/it]\u001b[A\n",
            " 43%|████▎     | 90/210 [2:04:45<2:41:45, 80.88s/it] \u001b[A\n",
            " 43%|████▎     | 91/210 [2:04:53<1:57:10, 59.08s/it]\u001b[A\n",
            " 44%|████▍     | 92/210 [2:05:08<1:29:57, 45.74s/it]\u001b[A\n",
            " 44%|████▍     | 93/210 [2:05:28<1:14:21, 38.13s/it]\u001b[A\n",
            " 45%|████▍     | 94/210 [2:06:26<1:25:23, 44.17s/it]\u001b[A\n",
            " 45%|████▌     | 95/210 [2:06:53<1:14:35, 38.92s/it]\u001b[A\n",
            " 46%|████▌     | 96/210 [2:07:00<55:26, 29.18s/it]  \u001b[A\n",
            " 46%|████▌     | 97/210 [2:07:04<40:59, 21.76s/it]\u001b[A\n",
            " 47%|████▋     | 98/210 [2:10:57<2:38:53, 85.12s/it]\u001b[A\n",
            " 47%|████▋     | 99/210 [2:12:24<2:38:18, 85.57s/it]\u001b[A\n",
            " 48%|████▊     | 100/210 [2:12:43<2:00:46, 65.87s/it]\u001b[A\n",
            " 48%|████▊     | 101/210 [2:12:50<1:27:24, 48.11s/it]\u001b[A\n",
            " 49%|████▊     | 102/210 [2:13:03<1:07:35, 37.55s/it]\u001b[A\n",
            " 49%|████▉     | 103/210 [2:13:22<57:10, 32.06s/it]  \u001b[A\n",
            " 50%|████▉     | 104/210 [2:21:19<4:52:28, 165.55s/it]\u001b[A\n",
            " 50%|█████     | 105/210 [2:21:43<3:35:23, 123.08s/it]\u001b[A\n",
            " 50%|█████     | 106/210 [2:21:48<2:31:59, 87.69s/it] \u001b[A\n",
            " 51%|█████     | 107/210 [2:21:52<1:47:10, 62.43s/it]\u001b[A\n",
            " 51%|█████▏    | 108/210 [2:25:41<3:11:13, 112.49s/it]\u001b[A\n",
            " 52%|█████▏    | 109/210 [2:27:04<2:54:30, 103.67s/it]\u001b[A\n",
            " 52%|█████▏    | 110/210 [2:27:31<2:14:11, 80.51s/it] \u001b[A\n",
            " 53%|█████▎    | 111/210 [2:27:44<1:39:37, 60.38s/it]\u001b[A\n",
            " 53%|█████▎    | 112/210 [2:28:04<1:18:51, 48.28s/it]\u001b[A\n",
            " 54%|█████▍    | 113/210 [2:28:25<1:04:45, 40.06s/it]\u001b[A\n",
            " 54%|█████▍    | 114/210 [2:29:36<1:18:55, 49.32s/it]\u001b[A\n",
            " 55%|█████▍    | 115/210 [2:30:06<1:08:42, 43.39s/it]\u001b[A\n",
            " 55%|█████▌    | 116/210 [2:30:18<53:32, 34.18s/it]  \u001b[A\n",
            " 56%|█████▌    | 117/210 [2:30:31<43:02, 27.77s/it]\u001b[A\n",
            " 56%|█████▌    | 118/210 [2:32:21<1:20:24, 52.44s/it]\u001b[A\n",
            " 57%|█████▋    | 119/210 [2:33:29<1:26:41, 57.16s/it]\u001b[A\n",
            " 57%|█████▋    | 120/210 [2:33:54<1:11:16, 47.51s/it]\u001b[A\n",
            " 58%|█████▊    | 121/210 [2:34:05<54:15, 36.58s/it]  \u001b[A\n",
            " 58%|█████▊    | 122/210 [2:34:24<45:41, 31.15s/it]\u001b[A\n",
            " 59%|█████▊    | 123/210 [2:34:51<43:32, 30.02s/it]\u001b[A\n",
            " 59%|█████▉    | 124/210 [2:39:16<2:23:51, 100.37s/it]\u001b[A\n",
            " 60%|█████▉    | 125/210 [2:39:45<1:52:02, 79.09s/it] \u001b[A\n",
            " 60%|██████    | 126/210 [2:39:53<1:20:41, 57.64s/it]\u001b[A\n",
            " 60%|██████    | 127/210 [2:40:03<59:52, 43.29s/it]  \u001b[A\n",
            " 61%|██████    | 128/210 [2:43:52<2:15:18, 99.01s/it]\u001b[A\n",
            " 61%|██████▏   | 129/210 [2:45:23<2:10:40, 96.79s/it]\u001b[A\n",
            " 62%|██████▏   | 130/210 [2:45:34<1:34:28, 70.86s/it]\u001b[A\n",
            " 62%|██████▏   | 131/210 [2:45:37<1:06:39, 50.62s/it]\u001b[A\n",
            " 63%|██████▎   | 132/210 [2:45:46<49:32, 38.11s/it]  \u001b[A\n",
            " 63%|██████▎   | 133/210 [2:45:54<37:21, 29.11s/it]\u001b[A\n",
            " 64%|██████▍   | 134/210 [2:49:23<1:45:19, 83.15s/it]\u001b[A\n",
            " 64%|██████▍   | 135/210 [2:49:33<1:16:25, 61.14s/it]\u001b[A\n",
            " 65%|██████▍   | 136/210 [2:49:36<53:50, 43.66s/it]  \u001b[A\n",
            " 65%|██████▌   | 137/210 [2:49:39<38:17, 31.47s/it]\u001b[A\n",
            " 66%|██████▌   | 138/210 [2:50:06<36:00, 30.01s/it]\u001b[A\n",
            " 66%|██████▌   | 139/210 [2:50:38<36:28, 30.82s/it]\u001b[A\n",
            " 67%|██████▋   | 140/210 [2:51:08<35:32, 30.47s/it]\u001b[A\n",
            " 67%|██████▋   | 141/210 [2:51:37<34:38, 30.12s/it]\u001b[A\n",
            " 68%|██████▊   | 142/210 [2:52:26<40:21, 35.60s/it]\u001b[A\n",
            " 68%|██████▊   | 143/210 [2:52:29<28:55, 25.91s/it]\u001b[A\n",
            " 69%|██████▊   | 144/210 [2:52:59<29:46, 27.07s/it]\u001b[A\n",
            " 69%|██████▉   | 145/210 [2:53:49<36:53, 34.05s/it]\u001b[A\n",
            " 70%|██████▉   | 146/210 [2:54:17<34:22, 32.22s/it]\u001b[A\n",
            " 70%|███████   | 147/210 [2:54:43<31:49, 30.31s/it]\u001b[A\n",
            " 70%|███████   | 148/210 [2:55:51<43:08, 41.76s/it]\u001b[A\n",
            " 71%|███████   | 149/210 [2:57:20<56:50, 55.90s/it]\u001b[A\n",
            " 71%|███████▏  | 150/210 [2:57:50<48:00, 48.02s/it]\u001b[A\n",
            " 72%|███████▏  | 151/210 [2:58:20<41:59, 42.71s/it]\u001b[A\n",
            " 72%|███████▏  | 152/210 [2:59:10<43:14, 44.73s/it]\u001b[A\n",
            " 73%|███████▎  | 153/210 [2:59:46<40:07, 42.24s/it]\u001b[A\n",
            " 73%|███████▎  | 154/210 [3:11:34<3:45:47, 241.91s/it]\u001b[A\n",
            " 74%|███████▍  | 155/210 [3:12:19<2:47:46, 183.03s/it]\u001b[A\n",
            " 74%|███████▍  | 156/210 [3:12:46<2:02:29, 136.11s/it]\u001b[A\n",
            " 75%|███████▍  | 157/210 [3:13:15<1:31:45, 103.88s/it]\u001b[A\n",
            " 75%|███████▌  | 158/210 [3:14:18<1:19:32, 91.78s/it] \u001b[A\n",
            " 76%|███████▌  | 159/210 [3:15:44<1:16:26, 89.94s/it]\u001b[A\n",
            " 76%|███████▌  | 160/210 [3:16:12<59:32, 71.44s/it]  \u001b[A\n",
            " 77%|███████▋  | 161/210 [3:16:42<48:03, 58.84s/it]\u001b[A\n",
            " 77%|███████▋  | 162/210 [3:17:35<45:40, 57.09s/it]\u001b[A\n",
            " 78%|███████▊  | 163/210 [3:18:06<38:41, 49.39s/it]\u001b[A\n",
            " 78%|███████▊  | 164/210 [3:18:35<33:13, 43.33s/it]\u001b[A\n",
            " 79%|███████▊  | 165/210 [3:19:25<33:49, 45.11s/it]\u001b[A\n",
            " 79%|███████▉  | 166/210 [3:19:50<28:51, 39.35s/it]\u001b[A\n",
            " 80%|███████▉  | 167/210 [3:20:16<25:08, 35.08s/it]\u001b[A\n",
            " 80%|████████  | 168/210 [3:21:24<31:28, 44.96s/it]\u001b[A\n",
            " 80%|████████  | 169/210 [3:22:54<40:05, 58.67s/it]\u001b[A\n",
            " 81%|████████  | 170/210 [3:23:25<33:30, 50.26s/it]\u001b[A\n",
            " 81%|████████▏ | 171/210 [3:23:53<28:22, 43.65s/it]\u001b[A\n",
            " 82%|████████▏ | 172/210 [3:24:37<27:47, 43.88s/it]\u001b[A\n",
            " 82%|████████▏ | 173/210 [3:25:08<24:37, 39.94s/it]\u001b[A\n",
            " 83%|████████▎ | 174/210 [3:36:27<2:18:59, 231.64s/it]\u001b[A\n",
            " 83%|████████▎ | 175/210 [3:37:05<1:41:10, 173.43s/it]\u001b[A\n",
            " 84%|████████▍ | 176/210 [3:37:13<1:10:08, 123.78s/it]\u001b[A\n",
            " 84%|████████▍ | 177/210 [3:37:38<51:47, 94.17s/it]   \u001b[A\n",
            " 85%|████████▍ | 178/210 [3:37:47<36:37, 68.66s/it]\u001b[A\n",
            " 85%|████████▌ | 179/210 [3:38:56<35:32, 68.79s/it]\u001b[A\n",
            " 86%|████████▌ | 180/210 [3:39:16<27:02, 54.09s/it]\u001b[A\n",
            " 86%|████████▌ | 181/210 [3:39:35<21:08, 43.73s/it]\u001b[A\n",
            " 87%|████████▋ | 182/210 [3:39:53<16:41, 35.76s/it]\u001b[A\n",
            " 87%|████████▋ | 183/210 [3:40:28<16:06, 35.81s/it]\u001b[A\n",
            " 88%|████████▊ | 184/210 [3:40:29<10:54, 25.17s/it]\u001b[A\n",
            " 88%|████████▊ | 185/210 [3:41:17<13:20, 32.02s/it]\u001b[A\n",
            " 89%|████████▊ | 186/210 [3:41:46<12:27, 31.16s/it]\u001b[A\n",
            " 89%|████████▉ | 187/210 [3:42:14<11:32, 30.09s/it]\u001b[A\n",
            " 90%|████████▉ | 188/210 [3:43:01<12:57, 35.35s/it]\u001b[A\n",
            " 90%|█████████ | 189/210 [3:44:06<15:31, 44.34s/it]\u001b[A\n",
            " 90%|█████████ | 190/210 [3:44:37<13:23, 40.19s/it]\u001b[A\n",
            " 91%|█████████ | 191/210 [3:45:07<11:47, 37.22s/it]\u001b[A\n",
            " 91%|█████████▏| 192/210 [3:45:59<12:25, 41.44s/it]\u001b[A\n",
            " 92%|█████████▏| 193/210 [3:46:33<11:08, 39.32s/it]\u001b[A\n",
            " 92%|█████████▏| 194/210 [3:55:18<49:18, 184.91s/it]\u001b[A\n",
            " 93%|█████████▎| 195/210 [3:56:07<36:05, 144.39s/it]\u001b[A\n",
            " 93%|█████████▎| 196/210 [3:56:35<25:30, 109.33s/it]\u001b[A\n",
            " 94%|█████████▍| 197/210 [3:57:03<18:23, 84.88s/it] \u001b[A\n",
            " 94%|█████████▍| 198/210 [3:58:08<15:47, 78.95s/it]\u001b[A\n",
            " 95%|█████████▍| 199/210 [3:59:45<15:30, 84.55s/it]\u001b[A\n",
            " 95%|█████████▌| 200/210 [4:00:13<11:13, 67.33s/it]\u001b[A\n",
            " 96%|█████████▌| 201/210 [4:00:40<08:17, 55.31s/it]\u001b[A\n",
            " 96%|█████████▌| 202/210 [4:01:14<06:30, 48.81s/it]\u001b[A\n",
            " 97%|█████████▋| 203/210 [4:01:43<05:01, 43.12s/it]\u001b[A\n",
            " 97%|█████████▋| 204/210 [4:07:13<12:53, 129.00s/it]\u001b[A\n",
            " 98%|█████████▊| 205/210 [4:07:45<08:20, 100.09s/it]\u001b[A\n",
            " 98%|█████████▊| 206/210 [4:07:59<04:57, 74.28s/it] \u001b[A\n",
            " 99%|█████████▊| 207/210 [4:08:18<02:52, 57.61s/it]\u001b[A\n",
            " 99%|█████████▉| 208/210 [4:08:37<01:31, 45.83s/it]\u001b[A\n",
            "100%|█████████▉| 209/210 [4:08:55<00:37, 37.48s/it]\u001b[A\n",
            "100%|██████████| 210/210 [4:09:33<00:00, 71.30s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('exp1.pkl', 'wb') as f:\n",
        "    pickle.dump(exp, f)\n",
        "files.download('exp1.pkl')"
      ],
      "metadata": {
        "id": "nnYJCXfWsMjY",
        "outputId": "44d5e13a-5de2-43b2-df02-0ddc0b1bf4a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "id": "nnYJCXfWsMjY",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_52ed133b-8ea6-4592-8954-6738e485fa48\", \"exp1.pkl\", 3432959)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, after trainign 180 models with different parameters, we can see that the `GradientBoostingClassifier` had an outstanding performance in comparison to the other models. We'll take the results of a 73.46% mean F1-score and 1.7% std in the K-Fold validation as our baseline for future comparisons."
      ],
      "metadata": {
        "id": "0uZwZBTf8Qv-"
      },
      "id": "0uZwZBTf8Qv-"
    },
    {
      "cell_type": "code",
      "source": [
        "results.sort_values(by=['F1 Mean', \"F1 Std\"], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "DwWKw3k6UVqB",
        "outputId": "710f3cf9-7e88-4e4d-d1bd-e843072cf81f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        }
      },
      "id": "DwWKw3k6UVqB",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                          Model Vectorizer            Sampler  Accuracy Mean  \\\n",
              "94   GradientBoostingClassifier      Count             ADASYN       0.699298   \n",
              "104  GradientBoostingClassifier      Count  RandomOversampler       0.694333   \n",
              "74   GradientBoostingClassifier      Count              SMOTE       0.693347   \n",
              "84   GradientBoostingClassifier      Count   Borderline-SMOTE       0.688407   \n",
              "54   GradientBoostingClassifier      TFIDF        SMOTE-Tomek       0.677515   \n",
              "124  GradientBoostingClassifier      Count        SMOTE-Tomek       0.680500   \n",
              "34   GradientBoostingClassifier      TFIDF  RandomOversampler       0.677486   \n",
              "64   GradientBoostingClassifier      TFIDF             NoSamp       0.679496   \n",
              "24   GradientBoostingClassifier      TFIDF             ADASYN       0.673555   \n",
              "134  GradientBoostingClassifier      Count             NoSamp       0.680491   \n",
              "\n",
              "     Accuracy Std  Recall Mean  Recall Std  Precision Mean  Precision Std  \\\n",
              "94       0.009880     0.699298    0.009880        0.693292       0.009541   \n",
              "104      0.019053     0.694333    0.019053        0.705052       0.017707   \n",
              "74       0.023312     0.693347    0.023312        0.698465       0.015648   \n",
              "84       0.021736     0.688407    0.021736        0.684475       0.023663   \n",
              "54       0.032724     0.677515    0.032724        0.708091       0.018617   \n",
              "124      0.015156     0.680500    0.015156        0.681979       0.012754   \n",
              "34       0.034528     0.677486    0.034528        0.686205       0.026626   \n",
              "64       0.027137     0.679496    0.027137        0.691487       0.023965   \n",
              "24       0.022593     0.673555    0.022593        0.697297       0.016998   \n",
              "134      0.018784     0.680491    0.018784        0.689769       0.025222   \n",
              "\n",
              "      F1 Mean    F1 Std  \n",
              "94   0.691044  0.009947  \n",
              "104  0.689394  0.018435  \n",
              "74   0.687498  0.021960  \n",
              "84   0.680387  0.022421  \n",
              "54   0.677958  0.028411  \n",
              "124  0.675060  0.014813  \n",
              "34   0.674007  0.031581  \n",
              "64   0.671905  0.024160  \n",
              "24   0.671654  0.016708  \n",
              "134  0.669836  0.018454  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ac503c95-3ed6-4689-be36-0a6c56fe7a5d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>Sampler</th>\n",
              "      <th>Accuracy Mean</th>\n",
              "      <th>Accuracy Std</th>\n",
              "      <th>Recall Mean</th>\n",
              "      <th>Recall Std</th>\n",
              "      <th>Precision Mean</th>\n",
              "      <th>Precision Std</th>\n",
              "      <th>F1 Mean</th>\n",
              "      <th>F1 Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>94</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>ADASYN</td>\n",
              "      <td>0.699298</td>\n",
              "      <td>0.009880</td>\n",
              "      <td>0.699298</td>\n",
              "      <td>0.009880</td>\n",
              "      <td>0.693292</td>\n",
              "      <td>0.009541</td>\n",
              "      <td>0.691044</td>\n",
              "      <td>0.009947</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>RandomOversampler</td>\n",
              "      <td>0.694333</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.694333</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.705052</td>\n",
              "      <td>0.017707</td>\n",
              "      <td>0.689394</td>\n",
              "      <td>0.018435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>74</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>SMOTE</td>\n",
              "      <td>0.693347</td>\n",
              "      <td>0.023312</td>\n",
              "      <td>0.693347</td>\n",
              "      <td>0.023312</td>\n",
              "      <td>0.698465</td>\n",
              "      <td>0.015648</td>\n",
              "      <td>0.687498</td>\n",
              "      <td>0.021960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>Borderline-SMOTE</td>\n",
              "      <td>0.688407</td>\n",
              "      <td>0.021736</td>\n",
              "      <td>0.688407</td>\n",
              "      <td>0.021736</td>\n",
              "      <td>0.684475</td>\n",
              "      <td>0.023663</td>\n",
              "      <td>0.680387</td>\n",
              "      <td>0.022421</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>54</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>SMOTE-Tomek</td>\n",
              "      <td>0.677515</td>\n",
              "      <td>0.032724</td>\n",
              "      <td>0.677515</td>\n",
              "      <td>0.032724</td>\n",
              "      <td>0.708091</td>\n",
              "      <td>0.018617</td>\n",
              "      <td>0.677958</td>\n",
              "      <td>0.028411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>124</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>SMOTE-Tomek</td>\n",
              "      <td>0.680500</td>\n",
              "      <td>0.015156</td>\n",
              "      <td>0.680500</td>\n",
              "      <td>0.015156</td>\n",
              "      <td>0.681979</td>\n",
              "      <td>0.012754</td>\n",
              "      <td>0.675060</td>\n",
              "      <td>0.014813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>RandomOversampler</td>\n",
              "      <td>0.677486</td>\n",
              "      <td>0.034528</td>\n",
              "      <td>0.677486</td>\n",
              "      <td>0.034528</td>\n",
              "      <td>0.686205</td>\n",
              "      <td>0.026626</td>\n",
              "      <td>0.674007</td>\n",
              "      <td>0.031581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>NoSamp</td>\n",
              "      <td>0.679496</td>\n",
              "      <td>0.027137</td>\n",
              "      <td>0.679496</td>\n",
              "      <td>0.027137</td>\n",
              "      <td>0.691487</td>\n",
              "      <td>0.023965</td>\n",
              "      <td>0.671905</td>\n",
              "      <td>0.024160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>TFIDF</td>\n",
              "      <td>ADASYN</td>\n",
              "      <td>0.673555</td>\n",
              "      <td>0.022593</td>\n",
              "      <td>0.673555</td>\n",
              "      <td>0.022593</td>\n",
              "      <td>0.697297</td>\n",
              "      <td>0.016998</td>\n",
              "      <td>0.671654</td>\n",
              "      <td>0.016708</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>134</th>\n",
              "      <td>GradientBoostingClassifier</td>\n",
              "      <td>Count</td>\n",
              "      <td>NoSamp</td>\n",
              "      <td>0.680491</td>\n",
              "      <td>0.018784</td>\n",
              "      <td>0.680491</td>\n",
              "      <td>0.018784</td>\n",
              "      <td>0.689769</td>\n",
              "      <td>0.025222</td>\n",
              "      <td>0.669836</td>\n",
              "      <td>0.018454</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ac503c95-3ed6-4689-be36-0a6c56fe7a5d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-ac503c95-3ed6-4689-be36-0a6c56fe7a5d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-ac503c95-3ed6-4689-be36-0a6c56fe7a5d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-83c6387d-03ec-4642-96c1-1ce836d9f79e\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-83c6387d-03ec-4642-96c1-1ce836d9f79e')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-83c6387d-03ec-4642-96c1-1ce836d9f79e button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"results\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"GradientBoostingClassifier\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vectorizer\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"TFIDF\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sampler\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"ADASYN\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008718023804336845,\n        \"min\": 0.6735550894990978,\n        \"max\": 0.6992976637565234,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6735550894990978\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007533888121982147,\n        \"min\": 0.00987965098702527,\n        \"max\": 0.03452770497976163,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.022592547630823866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008718023804336845,\n        \"min\": 0.6735550894990978,\n        \"max\": 0.6992976637565234,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6735550894990978\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007533888121982147,\n        \"min\": 0.00987965098702527,\n        \"max\": 0.03452770497976163,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.022592547630823866\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.008632778629395467,\n        \"min\": 0.6819785350143458,\n        \"max\": 0.7080914142615653,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6972972589833883\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0056642362176959945,\n        \"min\": 0.009540814487384995,\n        \"max\": 0.026625564094362193,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.0169983066383649\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Mean\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.007866143527520535,\n        \"min\": 0.669836055763601,\n        \"max\": 0.6910439143921288,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.6716540110754181\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Std\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006418536526410559,\n        \"min\": 0.009947033990819459,\n        \"max\": 0.03158140601409704,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.0167084519863505\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_results = pd.DataFrame(exp.test_ensemble()).T\n",
        "ensemble_results.columns = [\"Model\",\"Vectorizer\",\"Sampler\",\"Accuracy Mean\",\"Accuracy Std\",\"Recall Mean\",\"Recall Std\",\"Precision Mean\",\"Precision Std\",\"F1 Mean\",\"F1 Std\"]"
      ],
      "metadata": {
        "id": "h5ZsapdFUWGY",
        "outputId": "8688f73b-b5d4-4a98-c4ea-b14419098958",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "h5ZsapdFUWGY",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing folds: 100%|██████████| 5/5 [4:26:26<00:00, 3197.34s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "None the less, using the 180 models to build an ensemble model did not improve the results of the F1 score, since it only achieved a 69.53% mean F1-score with 2.11% of std."
      ],
      "metadata": {
        "id": "cPRerlPW8fhe"
      },
      "id": "cPRerlPW8fhe"
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_results"
      ],
      "metadata": {
        "id": "NBkQcsuUUY1h",
        "outputId": "dae3ec40-f2d4-4337-c2ad-4caf83205eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "id": "NBkQcsuUUY1h",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      Model Vectorizer   Sampler Accuracy Mean Accuracy Std Recall Mean  \\\n",
              "0  Ensemble   Multiple  Multiple      0.675574     0.019847    0.675574   \n",
              "\n",
              "  Recall Std Precision Mean Precision Std   F1 Mean    F1 Std  \n",
              "0   0.019847       0.676288      0.019961  0.667454  0.018378  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7fafffc7-06d8-41b3-8821-e3290335194f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Vectorizer</th>\n",
              "      <th>Sampler</th>\n",
              "      <th>Accuracy Mean</th>\n",
              "      <th>Accuracy Std</th>\n",
              "      <th>Recall Mean</th>\n",
              "      <th>Recall Std</th>\n",
              "      <th>Precision Mean</th>\n",
              "      <th>Precision Std</th>\n",
              "      <th>F1 Mean</th>\n",
              "      <th>F1 Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ensemble</td>\n",
              "      <td>Multiple</td>\n",
              "      <td>Multiple</td>\n",
              "      <td>0.675574</td>\n",
              "      <td>0.019847</td>\n",
              "      <td>0.675574</td>\n",
              "      <td>0.019847</td>\n",
              "      <td>0.676288</td>\n",
              "      <td>0.019961</td>\n",
              "      <td>0.667454</td>\n",
              "      <td>0.018378</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7fafffc7-06d8-41b3-8821-e3290335194f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7fafffc7-06d8-41b3-8821-e3290335194f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7fafffc7-06d8-41b3-8821-e3290335194f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_f1ec16aa-4d03-407f-ba57-3970aef99e36\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('ensemble_results')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_f1ec16aa-4d03-407f-ba57-3970aef99e36 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('ensemble_results');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "ensemble_results",
              "summary": "{\n  \"name\": \"ensemble_results\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"Model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Ensemble\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Vectorizer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Multiple\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Sampler\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Multiple\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Mean\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.6755743061990928,\n        \"max\": 0.6755743061990928,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6755743061990928\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Accuracy Std\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.019846856091082086,\n        \"max\": 0.019846856091082086,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.019846856091082086\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Mean\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.6755743061990928,\n        \"max\": 0.6755743061990928,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6755743061990928\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall Std\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.019846856091082086,\n        \"max\": 0.019846856091082086,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.019846856091082086\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Mean\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.676288026479922,\n        \"max\": 0.676288026479922,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.676288026479922\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision Std\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.019961168147456174,\n        \"max\": 0.019961168147456174,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.019961168147456174\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Mean\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.6674541593719706,\n        \"max\": 0.6674541593719706,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6674541593719706\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Std\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": 0.0183776098684052,\n        \"max\": 0.0183776098684052,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0183776098684052\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Representation A: Keep only the events that occur less than 85% of the time\n",
        "We took the paper provided by SCNB as inspiration to filter out some inherent noise that could affect the results of the previous experiment."
      ],
      "metadata": {
        "id": "buth96_X8LJe"
      },
      "id": "buth96_X8LJe"
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = Representations(df).representation_a(df)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()"
      ],
      "metadata": {
        "id": "_aqzrdn9ukm_"
      },
      "id": "_aqzrdn9ukm_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the following table we can see that filtering out the noise negatively impacted the model, since this time we achieved a 3 percentag points lower mean F1-score than our base line, i.e. 70.65% with an almost 3x greater standar deviation."
      ],
      "metadata": {
        "id": "stKf5SUS-B0N"
      },
      "id": "stKf5SUS-B0N"
    },
    {
      "cell_type": "code",
      "source": [
        "results.sort_values(by=['F1 Mean', \"F1 Std\"], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "vJRyX1Ef2S00"
      },
      "id": "vJRyX1Ef2S00",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_results = pd.DataFrame(exp.test_ensemble()).T\n",
        "ensemble_results.columns = [\"Model\",\"Vectorizer\",\"Sampler\",\"Accuracy Mean\",\"Accuracy Std\",\"Recall Mean\",\"Recall Std\",\"Precision Mean\",\"Precision Std\",\"F1 Mean\",\"F1 Std\"]"
      ],
      "metadata": {
        "id": "iiBWV2LYwC0k"
      },
      "id": "iiBWV2LYwC0k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we built an ensemble model to test if we could achieve a better mean F1-score. Unlike the 1st experiment, we achieced a mea F1-score 2 percentage points greater than the ensemble of the 1st experiment. Nonetheless, it was not enought to outperform the baseline model and also it had a slightly greater standard deviation."
      ],
      "metadata": {
        "id": "gLJiO0G7-iJr"
      },
      "id": "gLJiO0G7-iJr"
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_results"
      ],
      "metadata": {
        "id": "ulsxTrri2UsF"
      },
      "id": "ulsxTrri2UsF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Representation B: Split events into those before and after the incident"
      ],
      "metadata": {
        "id": "MuLCNXm59Wcd"
      },
      "id": "MuLCNXm59Wcd"
    },
    {
      "cell_type": "code",
      "source": [
        "df_before, df_after = Representations(df).representation_b(df)\n",
        "X, y = pd.concat([df_before.events_sequence, df_after.events_sequence]), pd.concat([df_before['class'], df_after['class']])\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()"
      ],
      "metadata": {
        "id": "razuBz2VOucS"
      },
      "id": "razuBz2VOucS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.sort_values(by=['F1 Mean', \"F1 Std\"], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "eVEC5TGedBmj"
      },
      "id": "eVEC5TGedBmj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_results = pd.DataFrame(exp.test_ensemble()).T\n",
        "ensemble_results.columns = [\"Model\",\"Vectorizer\",\"Sampler\",\"Accuracy Mean\",\"Accuracy Std\",\"Recall Mean\",\"Recall Std\",\"Precision Mean\",\"Precision Std\",\"F1 Mean\",\"F1 Std\"]"
      ],
      "metadata": {
        "id": "dVbpIzWPHctr"
      },
      "id": "dVbpIzWPHctr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_results"
      ],
      "metadata": {
        "id": "AkbQGcBtHfDi"
      },
      "id": "AkbQGcBtHfDi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Representation C: Generates fixed length overlappng and non-overlaping sequences out of the provided sequences"
      ],
      "metadata": {
        "id": "aseee5o49YQk"
      },
      "id": "aseee5o49YQk"
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = Representations(df).representation_c(df, sequence_length=100)\n",
        "exp = Experiment(X, y)\n",
        "results = exp.training()"
      ],
      "metadata": {
        "id": "A7frHCFPeHhB"
      },
      "id": "A7frHCFPeHhB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.sort_values(by=['F1 Mean', \"F1 Std\"], ascending=False).head(10)"
      ],
      "metadata": {
        "id": "ebD6mRosHlvl"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ebD6mRosHlvl"
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_results = pd.DataFrame(exp.test_ensemble()).T\n",
        "ensemble_results.columns = [\"Model\",\"Vectorizer\",\"Sampler\",\"Accuracy Mean\",\"Accuracy Std\",\"Recall Mean\",\"Recall Std\",\"Precision Mean\",\"Precision Std\",\"F1 Mean\",\"F1 Std\"]"
      ],
      "metadata": {
        "id": "rpIj_0FDHlvm"
      },
      "execution_count": null,
      "outputs": [],
      "id": "rpIj_0FDHlvm"
    },
    {
      "cell_type": "code",
      "source": [
        "ensemble_results"
      ],
      "metadata": {
        "id": "fXlUulj9Hlvn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "fXlUulj9Hlvn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "7Vk_xyb-G2Pm"
      },
      "id": "7Vk_xyb-G2Pm"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5NRhYv80u-4_"
      },
      "id": "5NRhYv80u-4_",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}